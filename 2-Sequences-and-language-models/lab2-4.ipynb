{
 "cells": [
  {
   "cell_type": "code",
   "id": "12f95d0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:20.892111Z",
     "iopub.status.busy": "2024-12-16T09:27:20.891493Z",
     "iopub.status.idle": "2024-12-16T09:27:22.354424Z",
     "shell.execute_reply": "2024-12-16T09:27:22.353441Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.042146Z",
     "start_time": "2024-12-18T21:36:55.147215Z"
    }
   },
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs236299-2024-winter/lab2-4.git .tmp\n",
    " mv .tmp/tests ./\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 184
  },
  {
   "cell_type": "code",
   "id": "1a23c3c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.072709Z",
     "start_time": "2024-12-18T21:36:56.069970Z"
    }
   },
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ],
   "outputs": [],
   "execution_count": 185
  },
  {
   "cell_type": "raw",
   "id": "2e9fe264",
   "metadata": {},
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336e8c2d",
   "metadata": {
    "id": "336e8c2d",
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# Course 236299\n",
    "\n",
    "## Lab 2-4 – Language models with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d842b",
   "metadata": {
    "id": "852d842b"
   },
   "source": [
    "In previous labs, you have explored two different language model architectures: the _n_-gram model and Recurrent Neural Network (RNN) model. Both of these models utilize context to predict the most likely next word in a sequence, but they do so in distinct ways.\n",
    "\n",
    "The _n_-gram model bases its predictions solely on the previous $n$ words. This method, though simple, is limited by its fixed context window. On the other hand, the RNN language model uses a hidden state that carries representations from previous time steps, theoretically enabling the model to capture long-range dependencies in text. However, in practice, RNNs often struggle with very long sequences due to issues like vanishing gradients, making them less effective for modeling extended texts.\n",
    "\n",
    "In the upcoming lab, you will implement and explore the _attention mechanism_, a powerful technique for encoding and utilizing context from previous time steps. This will be your first step toward understanding the Transformer model, one of the most important language model innovations in recent years.\n",
    "\n",
    "New bits of PyTorch used in the _solution set_ for this lab, and which you may therefore find useful:\n",
    "\n",
    "* `torch.bmm`\n",
    "* `torch.cumsum`\n",
    "* `torch.masked_fill`\n",
    "* `torch.repeat`\n",
    "* `torch.softmax`\n",
    "* `torch.transpose`\n",
    "* `torch.triu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752a3b61",
   "metadata": {
    "id": "752a3b61"
   },
   "source": [
    "# Preparation – Loading packages and data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c564d5",
   "metadata": {
    "id": "14c564d5",
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a90ded8",
   "metadata": {
    "deletable": false,
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:22.358251Z",
     "iopub.status.busy": "2024-12-16T09:27:22.357658Z",
     "iopub.status.idle": "2024-12-16T09:27:27.936169Z",
     "shell.execute_reply": "2024-12-16T09:27:27.935177Z"
    },
    "id": "4a90ded8",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.077499Z",
     "start_time": "2024-12-18T21:36:56.075281Z"
    }
   },
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import wget\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from math import inf\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 186
  },
  {
   "cell_type": "code",
   "id": "5a773bea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:27.940041Z",
     "iopub.status.busy": "2024-12-16T09:27:27.939684Z",
     "iopub.status.idle": "2024-12-16T09:27:27.947421Z",
     "shell.execute_reply": "2024-12-16T09:27:27.946557Z"
    },
    "id": "5a773bea",
    "outputId": "e399669c-5ed5-4b24-bd68-6156f3e30356",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.095017Z",
     "start_time": "2024-12-18T21:36:56.091504Z"
    }
   },
   "source": [
    "# Set random seeds\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# GPU check, sets runtime type to \"GPU\" where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 187
  },
  {
   "cell_type": "markdown",
   "id": "11807d3e",
   "metadata": {
    "id": "11807d3e"
   },
   "source": [
    "We use the same data as in lab 2-3 – the *Federalist* papers."
   ]
  },
  {
   "cell_type": "code",
   "id": "09876dbd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 216
    },
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:27.950908Z",
     "iopub.status.busy": "2024-12-16T09:27:27.950318Z",
     "iopub.status.idle": "2024-12-16T09:27:33.617856Z",
     "shell.execute_reply": "2024-12-16T09:27:33.616804Z"
    },
    "id": "09876dbd",
    "outputId": "ed717aa8-b094-4b17-fa4b-12e2a4889f37",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.155152Z",
     "start_time": "2024-12-18T21:36:56.108468Z"
    }
   },
   "source": [
    "# Prepare to download needed data\n",
    "def download_if_needed(source, dest, filename):\n",
    "    if os.path.exists(f\"./{dest}{filename}\"):\n",
    "        print (f\"Skipping {filename}\")\n",
    "    else: \n",
    "        print (f\"Downloading {filename} from {source}\")\n",
    "        wget.download(source + filename, out=dest)\n",
    "\n",
    "source_path = \"https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "# Download files, including pretrained language models\n",
    "for filename in [\"federalist_data_raw2.json\",\n",
    "                 \"tokenizer.pt\",\n",
    "                 # language models:\n",
    "                 # Hamilton          Madison\n",
    "                 \"u_attn_lm_h.pt\",   \"u_attn_lm_m.pt\", # uniform attn\n",
    "                 \"attn_lm_h.pt\",     \"attn_lm_m.pt\"    # attention\n",
    "                ]:\n",
    "    download_if_needed(source_path, data_path, filename)\n",
    "\n",
    "# Read in the raw data\n",
    "dataset = json.load(open(data_path + \"federalist_data_raw2.json\"))\n",
    "\n",
    "# Read in the pretrained tokenizer\n",
    "hf_tokenizer = torch.load(data_path + \"tokenizer.pt\")\n",
    "hf_tokenizer.split_special_tokens = False"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping federalist_data_raw2.json\n",
      "Skipping tokenizer.pt\n",
      "Skipping u_attn_lm_h.pt\n",
      "Skipping u_attn_lm_m.pt\n",
      "Skipping attn_lm_h.pt\n",
      "Skipping attn_lm_m.pt\n"
     ]
    }
   ],
   "execution_count": 188
  },
  {
   "cell_type": "markdown",
   "id": "8ecc150b",
   "metadata": {
    "id": "8ecc150b"
   },
   "source": [
    "Once again we will split the dataset into training, validation, and test sets. Since we have provided pretrained models, you won't be using the training set in this lab. For the validation set, we have separate ones for papers authored by Hamilton (`validation_hamilton`) and papers authored by Madison (`validation_madison`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "d0c4b5d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:33.622011Z",
     "iopub.status.busy": "2024-12-16T09:27:33.621260Z",
     "iopub.status.idle": "2024-12-16T09:27:33.632184Z",
     "shell.execute_reply": "2024-12-16T09:27:33.631146Z"
    },
    "id": "d0c4b5d3",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.179292Z",
     "start_time": "2024-12-18T21:36:56.169618Z"
    }
   },
   "source": [
    "# Split training, validation, and test sets\n",
    "TRAIN_RATIO = 0.9\n",
    "# Extract the papers of unknown authorship\n",
    "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n",
    "                      dataset))\n",
    "# Change gold labels in-place\n",
    "for ex in testing:\n",
    "  ex['authors'] = 'Madison'\n",
    "\n",
    "# Extract the papers by Madison\n",
    "dataset_madison = list(filter(lambda ex: ex['authors']=='Madison', dataset))\n",
    "random.seed(SEED)\n",
    "random.shuffle(dataset_madison)\n",
    "training_size_madison = int(math.floor(TRAIN_RATIO * len(dataset_madison)))\n",
    "validation_madison = dataset_madison[training_size_madison:]\n",
    "\n",
    "# Extract the papers by Hamilton\n",
    "dataset_hamilton = list(filter(lambda ex: ex['authors']=='Hamilton', dataset))\n",
    "random.seed(SEED)\n",
    "random.shuffle(dataset_hamilton)\n",
    "training_size_hamilton = int(math.floor(TRAIN_RATIO * len(dataset_hamilton)))\n",
    "validation_hamilton = dataset_hamilton[training_size_hamilton:]\n",
    "\n",
    "# We only consider the first 200 tokens of each document for speed\n",
    "def truncate(s, k=200):\n",
    "  for document in s:\n",
    "    document['tokens'] = document['tokens'][:k]\n",
    "truncate(validation_madison)\n",
    "truncate(validation_hamilton)\n",
    "truncate(testing)\n",
    "\n",
    "print (f\"Madison validation size:  {len(validation_madison)} documents\\n\"\n",
    "       f\"Hamilton validation size: {len(validation_hamilton)} documents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madison validation size:  3 documents\n",
      "Hamilton validation size: 6 documents\n"
     ]
    }
   ],
   "execution_count": 189
  },
  {
   "cell_type": "markdown",
   "id": "fyqWa563R0_p",
   "metadata": {
    "id": "fyqWa563R0_p"
   },
   "source": [
    "Before diving into the *attention mechanism*, we’d like to offer some intuitions about its underlying idea. You’ll begin by exploring simple models that will help you understand the basic concepts behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5-2wH8XHdZyu",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5-2wH8XHdZyu"
   },
   "source": [
    "# A uniform attention model\n",
    "\n",
    "Recall the forward step in RNN models, starting with converting words to an embedding:\n",
    "\n",
    "$$\n",
    "\\vect{x}_t = \\vect{U}{\\vect{w}_t} \\\\\n",
    "$$\n",
    "\n",
    "followed by the RNN layer proper (note that the non-linearity is ommited):\n",
    "\n",
    "$$\n",
    "\\vect{h}_t = \\vect{W}\\vect{h}_{t-1} + \\vect{V}\\vect{x}_i\\\\\n",
    "$$\n",
    "\n",
    "and finally, a linear layer and softmax to convert to a distribution over the vocabulary.\n",
    "\n",
    "$$\n",
    "\\vect{y}_t = \\textrm{softmax}(\\vect{X}\\vect{h}_t)\n",
    "$$\n",
    "\n",
    "Here, \n",
    "\n",
    "* $\\vect{w}_t$ is the word (one-hot or vocabulary index) at time step $t$,\n",
    "* $\\vect{x}_t$ is the corresponding embedding of size $D$,\n",
    "* $\\vect{h}_t$ is the hidden value encapsulating the history of size $H$, and\n",
    "* $\\vect{y}_t$ is the output.\n",
    "\n",
    "The parameters of the model are the matrices $\\vect{U}, \\vect{V}, \\vect{W}, \\vect{X}, \\vect{Q}, \\vect{K}, \\vect{V}$.\n",
    "\n",
    "Notice how $\\vect{h}_t$ has access to representations from previous time steps only through $\\vect{h}_{t-1}$. In addition, calculating $\\vect{h}_t$ requires waiting for the computation of $\\vect{h}_{t-1}$; for this reason, RNNs are inherently serial in their computations. You will now test a new approach where the context at time step $t$ directly incorporates word embeddings from all previous time steps.\n",
    "\n",
    "You will first explore a simple and naive model called *The Uniform Attention Model*, so-called because at each time step the model assigns equal attention to all previous words when calculating the distribution for the next word.\n",
    "\n",
    "At each time step $t$, $\\vect{h}_t$ is calculated simply based on the sum of all the previous word embeddings $\\vect{x}_1,\\vect{x}_2, \\ldots \\vect{x}_{t}$ as above except that\n",
    "\n",
    "$$\n",
    "\\vect{h}_t = \\sum_{i=1}^{t}{\\vect{x}_i}\n",
    "$$\n",
    "\n",
    "Now, complete the implementation of the `forward` function. (As usual, the final softmax is handled by the loss function, so you needn't handle that.)\n",
    "\n",
    "> **Hint:** You might find [`torch.cumsum`](https://pytorch.org/docs/stable/generated/torch.cumsum.html) helpful.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: uattnlm_forward\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "96e5424b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:33.636134Z",
     "iopub.status.busy": "2024-12-16T09:27:33.635332Z",
     "iopub.status.idle": "2024-12-16T09:27:33.643148Z",
     "shell.execute_reply": "2024-12-16T09:27:33.642289Z"
    },
    "id": "96e5424b",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.208643Z",
     "start_time": "2024-12-18T21:36:56.204394Z"
    }
   },
   "source": [
    "class UATTNLM(torch.nn.Module):\n",
    "  def __init__(self, hf_tokenizer, embedding_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.hf_tokenizer = hf_tokenizer\n",
    "    self.pad_index = hf_tokenizer.pad_token_id\n",
    "    self.hidden_size = hidden_size\n",
    "    vocab_size = len(hf_tokenizer)\n",
    "\n",
    "    # In this uniform attention model, the hidden size and the \n",
    "    # embedding size must be the same, since the former is \n",
    "    # just a sum over the latter\n",
    "    assert hidden_size == embedding_size\n",
    "    \n",
    "    # Create modules\n",
    "    self.embed = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "    self.hidden2output = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, context_words):\n",
    "    \"\"\"Computes the distribution over the next word given \n",
    "       context `history_words`.\n",
    "       \n",
    "    Arguments:\n",
    "      context_words: a list of word strings, could be an \n",
    "                     empty list when generating the first word.\n",
    "    Returns:\n",
    "      logits, should be a tensor of size (bsz, seq_len, vocab_size)\n",
    "      \"\"\"\n",
    "    self.eval()\n",
    "    context = self.hf_tokenizer(context_words, is_split_into_words=True, return_tensors='pt')['input_ids']\\\n",
    "              .long()\\\n",
    "              .to(device) # bsz, context_len\n",
    "    context_len = context.size(1)\n",
    "\n",
    "    # For generating the first word, we feed in a special beginning-of-sentence symbol <pad>,\n",
    "    # which is also what we use for padding. In future labs we'll be using <bos>, but as long\n",
    "    # as training and evaluation use the same beginning-of-sentence symbol, it doesn't matter\n",
    "    # which particular symbol we use.\n",
    "    if context_len == 0:\n",
    "      context = context.new(1, 1).fill_(self.pad_index)\n",
    "      context_len = context.size(1)\n",
    "\n",
    "    hidden = None\n",
    "    # TODO: calculate output and set logits\n",
    "    # Logits should be a tensor of size (1, seq_len, vocab_size)\n",
    "    # The structure of the network is\n",
    "    #   embeddings -> output -> hidden2output -> logits\n",
    "    embeddings = self.embed(context)\n",
    "    hidden = torch.cumsum(embeddings, dim=1)\n",
    "    logits = self.hidden2output(hidden)\n",
    "    return logits"
   ],
   "outputs": [],
   "execution_count": 190
  },
  {
   "cell_type": "markdown",
   "id": "d9dd682f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: uattnlm_forward\n",
    "-->\n",
    "**Question:** For uniform attention to work as above, $H$ and $D$, the hidden size and the embedding size, must be the same. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06524a6a",
   "metadata": {},
   "source": "In this model, the hidden layer at each time step $t$, $\\vect{h}_t$ is calculated simply based on the sum of all the previous word embeddings $\\vect{x}_1,\\vect{x}_2, \\ldots \\vect{x}_{t}$. The sum of vectors must be the same shape as the vetors themselves."
  },
  {
   "cell_type": "markdown",
   "id": "-i07SK4dWskG",
   "metadata": {
    "id": "-i07SK4dWskG"
   },
   "source": [
    "Now, let's load the models for Hamilton and Madison. The model `u_attn_lm_madison` was trained on documents authored by Madison, while `u_attn_lm_hamilton` was trained on documents authored by Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "id": "8YBju6RcWwrF",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:33.646965Z",
     "iopub.status.busy": "2024-12-16T09:27:33.646144Z",
     "iopub.status.idle": "2024-12-16T09:27:33.673953Z",
     "shell.execute_reply": "2024-12-16T09:27:33.672982Z"
    },
    "id": "8YBju6RcWwrF",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.232240Z",
     "start_time": "2024-12-18T21:36:56.212133Z"
    }
   },
   "source": [
    "# Create and load uniform attention LM for Madison\n",
    "u_attn_lm_madison = UATTNLM(hf_tokenizer,\n",
    "                            embedding_size=128,\n",
    "                            hidden_size=128,\n",
    "                           ).to(device)\n",
    "u_attn_lm_madison.load_state_dict(\n",
    "    torch.load(data_path + 'u_attn_lm_m.pt',\n",
    "               map_location=device))\n",
    "\n",
    "# Create and load uniform attention LM for Hamilton\n",
    "u_attn_lm_hamilton = UATTNLM(hf_tokenizer,\n",
    "                             embedding_size=128,\n",
    "                             hidden_size=128,\n",
    "                            ).to(device)\n",
    "u_attn_lm_hamilton.load_state_dict(\n",
    "    torch.load(data_path + 'u_attn_lm_h.pt',\n",
    "               map_location=device))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 191
  },
  {
   "cell_type": "markdown",
   "id": "Hqr1eocPVZ3u",
   "metadata": {
    "id": "Hqr1eocPVZ3u"
   },
   "source": [
    "Let's try to sample from the model. Don’t expect a fluent text yet—more improvements are needed in the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "hjmKfuRTVqR-",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:33.677749Z",
     "iopub.status.busy": "2024-12-16T09:27:33.677482Z",
     "iopub.status.idle": "2024-12-16T09:27:33.684693Z",
     "shell.execute_reply": "2024-12-16T09:27:33.683751Z"
    },
    "id": "hjmKfuRTVqR-",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:36:56.263792Z",
     "start_time": "2024-12-18T21:36:56.258666Z"
    }
   },
   "source": [
    "def sample(model, context):\n",
    "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
    "    logits = model(context)[:,-1] # calls internally to model.forward(context)\n",
    "\n",
    "     # Normalize to get probabilities\n",
    "    probs = torch.softmax(logits, -1).view(-1) # vocab_size\n",
    "\n",
    "    # Match probabilities with actual word types\n",
    "    distribution = {}\n",
    "    for i, prob in enumerate(probs):\n",
    "      word = model.hf_tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
    "      distribution[word] = prob.item()\n",
    "\n",
    "    prob_remaining = random.random()\n",
    "    for token, prob in sorted(distribution.items()):\n",
    "        if prob_remaining < prob:\n",
    "            return token\n",
    "        else:\n",
    "            prob_remaining -= prob\n",
    "    raise ValueError\n",
    "\n",
    "def sample_sequence(model, start_context, count=100):\n",
    "    \"\"\"Returns a sequence of tokens of length `count` sampled successively\n",
    "       from the `model` starting with the `start_context`\n",
    "    \"\"\"\n",
    "    random.seed(SEED) # for reproducibility\n",
    "    context = list(start_context)\n",
    "    result = list(start_context)\n",
    "    for i in range(0, count):\n",
    "        next = sample(model, tuple(context))\n",
    "        result.append(next)\n",
    "        context = context + [next]\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 192
  },
  {
   "cell_type": "code",
   "id": "9f920e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:33.688657Z",
     "iopub.status.busy": "2024-12-16T09:27:33.687947Z",
     "iopub.status.idle": "2024-12-16T09:27:40.775243Z",
     "shell.execute_reply": "2024-12-16T09:27:40.774203Z"
    },
    "id": "-ImaCgeNVthR",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.517071Z",
     "start_time": "2024-12-18T21:36:56.268501Z"
    }
   },
   "source": [
    "print(' '.join(sample_sequence(u_attn_lm_madison,\n",
    "                               ('constitution', 'proposed', 'by', 'the'))), \"\\n\")\n",
    "print(' '.join(sample_sequence(u_attn_lm_hamilton,\n",
    "                               ('constitution', 'proposed', 'by', 'the'))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitution proposed by the virtuous hope . those unwise notorious population an revenue convention , the had of on and be [UNK] , of which . the subject the ? the government one the of is as to of the , the , [UNK] of of , the all the [UNK] they the to [UNK] federal , are the [UNK] [UNK] to [UNK] the , of in of the of the of the the of the the convention . [UNK] of . the the of the states people and states the , the [UNK] , states of states [UNK] , [UNK] , , , \n",
      "\n",
      "constitution proposed by the violated here [UNK] thing union misrepresentations peculiarly america restraint consist [UNK] secure expenses observed now but command arise , into when affairs man in objection a naturally debts mislead our in disciplining being us if language , itself , a in extent , of [UNK] more [UNK] understood the we . a , a the [UNK] , when [UNK] to , and a [UNK] the and the by the the of on the and [UNK] [UNK] of [UNK] of the of the of of [UNK] the the [UNK] the [UNK] [UNK] of of the [UNK] , [UNK] , [UNK] ,\n"
     ]
    }
   ],
   "execution_count": 193
  },
  {
   "cell_type": "code",
   "id": "af101903",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.638897Z",
     "start_time": "2024-12-18T21:37:00.534497Z"
    }
   },
   "source": [
    "grader.check(\"uattnlm_sample\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 194
  },
  {
   "cell_type": "markdown",
   "id": "PayCOKueY1wk",
   "metadata": {
    "id": "PayCOKueY1wk"
   },
   "source": [
    "In the previous model, we made a simple assumption – that all previous words are equally important. Of course, this simple assumption is wrong in general, as different time steps can have varying levels of importance and dependencies.\n",
    "\n",
    "Next, you will build a model that differentiates between time steps.\n",
    "\n",
    "Are you ready? You’re about to learn about one of the most important mechanisms invented in recent years."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8h4V5WcgM7Nd",
   "metadata": {
    "id": "8h4V5WcgM7Nd"
   },
   "source": [
    "# The attention mechanism\n",
    "\n",
    "The previous model was quite basic and suffered from an obvious problem:\n",
    "\n",
    "At each time step, we assumed the dependencies on the previous time steps, the context, were static. Ideally, we want the model to determine dynamically at each stage which parts of the context are more relevant and which are less, that is, which parts of the context to _attend to_.\n",
    "\n",
    "You'll now be introduced to the _attention mechanism_ (proposed in [this seminal paper](https://arxiv.org/abs/1409.0473)), a powerful architecture that allows models to selectively focus on different parts of the input sequence. This mechanism has become a cornerstone in modern natural-language processing, significantly enhancing a model's ability to model language distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6721fe",
   "metadata": {
    "id": "Igm9_uxSSEuF"
   },
   "source": [
    "Attention works by _querying_ a (dynamically sized) sequence of _keys_ associated with _values_. Each key-value pair is associated with the context elements of a particular token.\n",
    "\n",
    "As usual, each query, key, and value is represented as a vector. Attention proceeds as follows: The query is compared to each of the keys, providing a score that specifies how much each key should be attended to. The attention can then be summarized by taking an average of the _values_ weighted by the attention score of the corresponding _keys_. This _context vector_ can then be used as another input to other processes you will learn about in the next lab.\n",
    "\n",
    "More formally, let's suppose we have a query vector $\\vect{q}$ of size $D$, a set of $S$ keys $\\vect{k}_i$ each of size $D$ and a set of $S$ values $\\vect{v}_i$ each of size $D$, where $D$ is the embedding size. What we want to do through the attention mechanism is to use the query to attend to the keys, and summarize those values associated with the \"relevant\" keys into a fixed-size context vector $\\vect{c}_i$ also of size $D$. Note that this is different from directly compressing the key-value pairs into a fixed-size vector, since depending on the query, we might end up with different context vectors.\n",
    "\n",
    "> In general, not all of these need to be of the same size $D$, but we restrict to that case here for simplicity.\n",
    "\n",
    "We'll turn to implementing this attention mechanism shortly, but first, for concreteness, we show how the mechanism can be used to form a language model. Like the uniform attention mechanism above, we start with an embedding of the vocabulary.\n",
    "\n",
    "$$\n",
    "\\vect{x}_t = \\vect{U}{\\vect{w}_t}\n",
    "$$\n",
    "\n",
    "Next, we construct a query, key, and value for each position with linear transforms for each:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\vect{q}_t &= \\vect{Q}{\\vect{w}_t} \\\\\n",
    "\\vect{k}_t &= \\vect{K}{\\vect{w}_t} \\\\\n",
    "\\vect{v}_t &= \\vect{V}{\\vect{w}_t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "followed by the attention layer proper:\n",
    "\n",
    "$$\n",
    "\\vect{h}_t = \\operatorname{attn}(\\vect{q}_t, \\vect{k}_{1:t}, \\vect{v}_{1:t})\n",
    "$$\n",
    "\n",
    "and finally, a linear layer and softmax to convert to a distribution over the vocabulary.\n",
    "\n",
    "$$\n",
    "\\vect{y}_t = \\textrm{softmax}(\\vect{X}\\vect{h}_t)\n",
    "$$\n",
    "\n",
    "> We write $\\vect{k}_{1:t}$ for the sequence of elements $\\langle\\vect{k}_1, \\vect{k}_2, \\ldots, \\vect{k}_t\\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Igm9_uxSSEuF",
   "metadata": {
    "id": "Igm9_uxSSEuF"
   },
   "source": [
    "Now back to how the attention calculation works.\n",
    "\n",
    "To determine the score for a given query and key, it is standard to use a measure of similarity between the query and key. You've seen such similarity measures before, in labs 1-1 and 1-2. A good choice is simply the normalized dot product between query and key. We'll thus take the attention score for query $\\vect{q}$ and key $\\vect{k}_i$ to be\n",
    "\n",
    "$$\n",
    "s_i = \\frac{\\vect{q} \\cdot \\vect{k}_i}{\\sqrt{D}}\n",
    "$$\n",
    "\n",
    "and the attention vector to be the softmax of the score\n",
    "\n",
    "$$\n",
    "\\vect{a} = \\operatorname{softmax}(\\vect{s})\n",
    "$$\n",
    "\n",
    "where $\\cdot$ denotes the dot product (inner product). (There are multiple ways of parameterizing the attention function, but the form we present here is the most popular one.).\n",
    "\n",
    "The softmax guarantees that the attention scores $\\vect{a}$ lie on a *simplex* (meaning $a_i\\ge 0$ and $\\sum_i a_i=1$), so $\\vect{a}$ can be viewed as a probability distribution over the key-value pairs. This also lends it some interpretability: the closer $a_i$ is to 1, the more \"relevant\" a key $k_i$ (and hence its value $v_i$) is to the given query.\n",
    "\n",
    "> Why divide through by the square root of the hidden size (the length of $\\vect{q}$ and of $\\vect{k}$)? This bit of neural network magic is one of many in the black art of machine learning. Without this scaling, the dot product values can get quite large, leading to numerical problems in perfroming the softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EBkXXYiJSQRG",
   "metadata": {
    "id": "EBkXXYiJSQRG"
   },
   "source": [
    "Finally, to compute the result of the attention query – the \"context vector\" $\\vect{c}$ – we take the weighted sum of values using the corresponding attention scores as weights:\n",
    "\n",
    "$$\n",
    "\\vect{c} = \\sum_{i=1}^S a_i \\vect{v}_i\n",
    "$$\n",
    "\n",
    "The closer $a_i$ is to 1, the higher the weight $\\vect{v}_i$ receives, and the more the context vector \"attends to\" the value $\\vect{v}_i$ at position $i$.\n",
    "\n",
    "> In usage, the result of attention, the context vector for position $t$, plays the role of $\\vect{h}_t$, as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v0_8E4XbSaEH",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "v0_8E4XbSaEH"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_a_i_1\n",
    "-->\n",
    "\n",
    "**Question:** Suppose there exists $i$ for which $a_i$ is 1. What will the value of $\\vect{c}$ be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e177b10",
   "metadata": {},
   "source": "The value of $\\vect{c}$ will be the value of $\\vect{v}_i$"
  },
  {
   "cell_type": "markdown",
   "id": "3X-PiLhXStKh",
   "metadata": {
    "id": "3X-PiLhXStKh"
   },
   "source": [
    "In practice, instead of computing the context vector once for each query, we want to combine together computations for different queries (for different positions $t$ in the input) to allow for parallel processing on GPUs. This will become especially useful for the transformer implementation in the next lab. We use a matrix $Q\\in\\mathbb{R}^{{T} \\times D}$ to store $T$ queries, a matrix $K\\in\\mathbb{R}^{S \\times D}$ to store $S$ keys, and a matrix $V\\in\\mathbb{R}^{S\\times D}$ to store the corresponding values. Then we can write down how we compute the attention scores $A\\in\\mathbb{R}^{T \\times S}$ in a matrix form:\n",
    "\n",
    "$$\n",
    "A = \\operatorname{softmax} (Q K^{\\top}, \\text{dim}=-1)\n",
    "$$\n",
    "\n",
    "and to get the context matrix $C \\in \\mathbb{R}^{T \\times D}$, we have\n",
    "\n",
    "$$\n",
    "C = A V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PvBFFJ7NSz7z",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PvBFFJ7NSz7z"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_a_ij_meaning\n",
    "-->\n",
    "**Question:** What is the shape of $A$? What does $A_{ij}$ represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20295f19",
   "metadata": {},
   "source": "The shape of $A$ is $T\\times S$. $A_{ij}$ represents the attention score of the jth key-value pair in the ith query"
  },
  {
   "cell_type": "markdown",
   "id": "5QE-91s0TIkS",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5QE-91s0TIkS"
   },
   "source": [
    "Your first job is to implement this calculation by finishing the attention function below, which takes the $Q$, $K$, and $V$ matrices and returns the $A$ and $C$ matrices. Note that for these matrices, there is one additional dimension for the batching, so instead of $Q\\in \\mathbb{R}^{T \\times D}$, $K,V\\in \\mathbb{R}^{S \\times D}$, $A\\in \\mathbb{R}^{T \\times S}$, $C\\in \\mathbb{R}^{T \\times D}$, we have $Q\\in\\mathbb{R}^{B \\times T \\times D}$, $K,V\\in \\mathbb{R}^{B \\times S\\times D}$, $A\\in \\mathbb{R}^{B\\times T \\times S}$, $C\\in \\mathbb{R}^{B \\times T \\times D}$, where $B$ is the batch size.  In addition, the function below also takes an argument `mask` of size $\\mathbb{R}^{B\\times T \\times S}$ to mark where attentions are disallowed. This is useful not only in disallowing attending to padding symbols, but also to efficiently train the model.\n",
    "\n",
    "> **Hint:** You might find [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) helpful for batched matrix multiplications. You might need to transpose and reshape tensors to be able to use this function.\n",
    "\n",
    "> **Hint:** As mentioned in the beginning of the lab, you might also find [`torch.transpose`](https://pytorch.org/docs/stable/generated/torch.transpose.html), [`torch.reshape`](https://pytorch.org/docs/stable/generated/torch.reshape.html), [`torch.masked_fill`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_fill), and [`torch.softmax`](https://pytorch.org/docs/stable/_modules/torch/nn/functional.html#softmax) useful.\n",
    "\n",
    "> **Hint:** A simple trick for masking an attention score is to set it to negative infinity before normalization.\n",
    "\n",
    "> **Hint:** Debugging the `attention` function can be tricky. Try defining a very small example of batched $\\vect{q}$, $\\vect{k}$, and $\\vect{v}$ values for which you can manually carry out the attention claculation and test the code on those, working your way up to more complex examples.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: attention\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "-SFxCQN1WRGF",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:41.643273Z",
     "iopub.status.busy": "2024-12-16T09:27:41.642895Z",
     "iopub.status.idle": "2024-12-16T09:27:41.650461Z",
     "shell.execute_reply": "2024-12-16T09:27:41.649545Z"
    },
    "id": "-SFxCQN1WRGF",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.658771Z",
     "start_time": "2024-12-18T21:37:00.653460Z"
    }
   },
   "source": [
    "#TODO - finish implementing this function.\n",
    "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
    "  \"\"\"\n",
    "  Performs the attention operation and returns the attention matrix\n",
    "  `batched_A` and the context matrix `batched_C` using queries\n",
    "  `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
    "\n",
    "  Arguments:\n",
    "      batched_Q: (bsz, q_len, D)\n",
    "      batched_K: (bsz, k_len, D)\n",
    "      batched_V: (bsz, k_len, D)\n",
    "      mask: (bsz, q_len, k_len). An optional boolean mask *disallowing*\n",
    "            attentions where the mask value is *`False`*.\n",
    "  Returns:\n",
    "      batched_A: the normalized attention scores (bsz, q_len, k_len)\n",
    "      batched_C: a tensor of size (bsz, q_len, D).\n",
    "  \"\"\"\n",
    " # Check sizes\n",
    "  D = batched_Q.size(-1)\n",
    "  bsz = batched_Q.size(0)\n",
    "  q_len = batched_Q.size(1)\n",
    "  k_len = batched_K.size(1)\n",
    "    \n",
    "  assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
    "  assert batched_K.size(0) == bsz and batched_V.size(0) == bsz\n",
    "  assert batched_V.size(1) == k_len\n",
    "  if mask is not None:\n",
    "    assert mask.size() == torch.Size([bsz, q_len, k_len]), f\"{mask.size()} {torch.Size([bsz, q_len, k_len])}\"\n",
    "\n",
    "  ...\n",
    "  res = (1 / math.sqrt(D)) * torch.bmm(batched_Q, batched_K.transpose(1, 2))\n",
    "  if mask is not None:\n",
    "    res = res - torch.where(mask, math.inf, 0)\n",
    "  batched_A = torch.softmax(res, dim=-1)\n",
    "  batched_C = torch.bmm(batched_A, batched_V)\n",
    "  # Verify that things sum up to one properly.\n",
    "  assert torch.all(torch.isclose(batched_A.sum(-1),\n",
    "                                 torch.ones(bsz, q_len).to(device)))\n",
    "  return batched_A, batched_C"
   ],
   "outputs": [],
   "execution_count": 195
  },
  {
   "cell_type": "markdown",
   "id": "KCPnNGIvos92",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "KCPnNGIvos92"
   },
   "source": [
    "## Causal attention mask\n",
    "\n",
    "To efficiently train the model, we want to batch the attention operations together such that they can be fully parallelized along the sequence length dimension. (The non-attention operations are position-wise so they are trivally parallelizable.) Each word at position $t$ attends to itself and the $t-1$ previous words $1, \\ldots, {t-1}$, which means each word has access to a different set of key-value pairs. It might seem then that we can't compute attention for all of the positions at the same time. Is it possible to batch them together?\n",
    "\n",
    "The solution is to use *attention masks*. For every position $t$, we give it _all_ key-value pairs at positions $1, \\ldots, T$, and we disallow attending to future words ${t+1}, {t+2},\\ldots, T$ through an attention mask. (Recall that the `attention` function takes a `mask` argument.) We usually call this attention mask a _causal attention mask_, as it prevents the leakage of information from the future into the present. Since every $t$ has the same set of (key, value) pairs, we can batch them and compute the context vectors using a single call to the function `attention`.\n",
    "\n",
    "What should such a mask be? Implement the `causal_mask` function below to generate this mask.\n",
    "\n",
    "> Hint: you might find [`torch.triu`](https://pytorch.org/docs/stable/generated/torch.triu.html) useful.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: causal_attention_mask\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "OL1RydQmox2k",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:41.653363Z",
     "iopub.status.busy": "2024-12-16T09:27:41.653133Z",
     "iopub.status.idle": "2024-12-16T09:27:41.657314Z",
     "shell.execute_reply": "2024-12-16T09:27:41.656515Z"
    },
    "id": "OL1RydQmox2k",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.674649Z",
     "start_time": "2024-12-18T21:37:00.672491Z"
    }
   },
   "source": [
    "#TODO - implement this function, which returns a causal attention mask\n",
    "def causal_mask(T):\n",
    "  \"\"\"\n",
    "  Generate a causal mask.\n",
    "  Arguments:\n",
    "      T: the length of target sequence\n",
    "  Returns:\n",
    "      mask: a T x T tensor, where `mask[i, j]` should be `True`\n",
    "      if position i can attend to position j, and `False` if i cannot\n",
    "      attend to j\n",
    "  \"\"\"\n",
    "  mask = torch.triu(torch.ones(T, T, dtype=torch.bool), diagonal=1)\n",
    "  return mask.to(device)"
   ],
   "outputs": [],
   "execution_count": 196
  },
  {
   "cell_type": "code",
   "id": "0b185599",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.693480Z",
     "start_time": "2024-12-18T21:37:00.689155Z"
    }
   },
   "source": [
    "grader.check(\"causal_attention_mask\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 197
  },
  {
   "cell_type": "markdown",
   "id": "9O4mPlx9o4rj",
   "metadata": {
    "id": "9O4mPlx9o4rj"
   },
   "source": [
    "We can visualize the attention mask and manually check if it's what we expected."
   ]
  },
  {
   "cell_type": "code",
   "id": "HdiIRBhpo8W3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:41.678851Z",
     "iopub.status.busy": "2024-12-16T09:27:41.678250Z",
     "iopub.status.idle": "2024-12-16T09:27:42.060770Z",
     "shell.execute_reply": "2024-12-16T09:27:42.059789Z"
    },
    "id": "HdiIRBhpo8W3",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.853887Z",
     "start_time": "2024-12-18T21:37:00.707632Z"
    }
   },
   "source": [
    "\n",
    "T = 7\n",
    "mask = causal_mask(T)\n",
    "\n",
    "# Create a colormap where True is one color and False is another\n",
    "cmap = mcolors.ListedColormap(['gray', 'maroon'])\n",
    "bounds = [-0.5, 0.5, 1.5]\n",
    "norm = mcolors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "# Plot the matrix\n",
    "plt.imshow(mask, cmap=cmap, norm=norm)\n",
    "\n",
    "# Add colorbar with labels\n",
    "cbar = plt.colorbar(ticks=[0, 1])\n",
    "cbar.ax.set_yticklabels(['False', 'True'])  # Customize your labels here\n",
    "\n",
    "# Label the axes\n",
    "plt.ylabel('Position $i$')\n",
    "plt.xlabel('...can attend to $j$')\n",
    "plt.title('Mask');\n",
    "\n",
    "# Uncomment the line below if the plot does not show up\n",
    "# Make sure to comment that before submitting to gradescope\n",
    "# since there would be some autograder issues with `plt.show()`\n",
    "#plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 198
  },
  {
   "cell_type": "markdown",
   "id": "CaffA386o_Ns",
   "metadata": {
    "id": "CaffA386o_Ns"
   },
   "source": [
    "By using causal masks, all positions (at the same layer) can be computed at once (if the lower layer has been computed). The parallelizability of the attention mechanism is the key to its success since it allows for training  on vast amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4N6VRc9ipqY_",
   "metadata": {
    "id": "4N6VRc9ipqY_"
   },
   "source": [
    "Now we are ready to complete the attention model.\n",
    "> Hint: The causal mask is a 2-D matrix, but we want to add a batch dimension, and expand it to be of the desired size. For this purpose, you can use [`torch.repeat`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.repeat) or [torch.expand](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d-yLr3oB3y",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "53d-yLr3oB3y"
   },
   "source": [
    "Complete the `forward` function.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: attnlm_forward\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "2UJgtw-fIdv-",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:42.064584Z",
     "iopub.status.busy": "2024-12-16T09:27:42.063947Z",
     "iopub.status.idle": "2024-12-16T09:27:42.073292Z",
     "shell.execute_reply": "2024-12-16T09:27:42.072517Z"
    },
    "id": "2UJgtw-fIdv-",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.874973Z",
     "start_time": "2024-12-18T21:37:00.869540Z"
    }
   },
   "source": [
    "class ATTNLM(torch.nn.Module):\n",
    "  def __init__(self, hf_tokenizer, embedding_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.hf_tokenizer = hf_tokenizer\n",
    "    self.pad_index = hf_tokenizer.pad_token_id\n",
    "    self.hidden_size = hidden_size\n",
    "    vocab_size = len(hf_tokenizer)\n",
    "\n",
    "    # Create modules\n",
    "    self.embed = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "    self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
    "    self.hidden2output = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, context_words):\n",
    "    \"\"\"Computes the distribution over the next word given context `history_words`.\n",
    "    Arguments:\n",
    "      context_words: batch a list of word strings, could be an empty list when generating\n",
    "                     the first word.\n",
    "    Returns:\n",
    "      logits, should be a tensor of size (batch , seq_length , vocab_size)\n",
    "      \"\"\"\n",
    "    self.eval()\n",
    "    context = self.hf_tokenizer(context_words, is_split_into_words=True, return_tensors='pt')['input_ids']\\\n",
    "              .long()\\\n",
    "              .to(device) # bsz, context_len\n",
    "    context_len = context.size(1)\n",
    "    context_bsz = context.size(0)\n",
    "\n",
    "    # For generating the first word, we feed in a special beginning-of-sentence symbol <pad>,\n",
    "    # which is also what we use for padding. In future labs we'll be using <bos>, but as long\n",
    "    # as training and evaluation use the same beginning-of-sentence symbol, it doesn't matter\n",
    "    # which particular symbol we use.\n",
    "    if context_len == 0:\n",
    "      context = context.new(1, 1).fill_(self.pad_index)\n",
    "      context_len = context.size(1)\n",
    "\n",
    "    hidden = None\n",
    "    # TODO: finish feedforward and set logits\n",
    "    # Logits should be a tensor of size (bsz, seq_length, vocab_size)\n",
    "    # The structure of the network is\n",
    "    #   embeddings -> output -> hidden2output -> logits\n",
    "    embeddings = self.embed(context)\n",
    "    q = self.q_proj(embeddings)\n",
    "    k = self.k_proj(embeddings)\n",
    "    v = self.v_proj(embeddings)\n",
    "    mask = causal_mask(context_len)\n",
    "    mask = mask.expand(context_bsz, -1, -1)\n",
    "    output = attention(q, k, v, mask)[1]\n",
    "    logits = self.hidden2output(output)\n",
    "    return logits\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 199
  },
  {
   "cell_type": "markdown",
   "id": "47b9f35f",
   "metadata": {
    "id": "47b9f35f"
   },
   "source": [
    "Now, let's load the pretrained models for Hamilton and Madison. The model `attn_lm_madison` was trained on documents authored by Madison, whereas `attn_lm_hamilton` was trained on documents authored by Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "id": "df2eba8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:42.076717Z",
     "iopub.status.busy": "2024-12-16T09:27:42.076218Z",
     "iopub.status.idle": "2024-12-16T09:27:42.101426Z",
     "shell.execute_reply": "2024-12-16T09:27:42.100485Z"
    },
    "id": "df2eba8a",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:00.912223Z",
     "start_time": "2024-12-18T21:37:00.890606Z"
    }
   },
   "source": [
    "# Create and load attention LM for Madison\n",
    "attn_lm_madison = ATTNLM(hf_tokenizer,\n",
    "                         embedding_size=128,\n",
    "                         hidden_size=128,\n",
    "                        ).to(device)\n",
    "attn_lm_madison.load_state_dict(torch.load(data_path + 'attn_lm_m.pt', map_location=device))\n",
    "\n",
    "# Create and load attention LM for Hamilton\n",
    "attn_lm_hamilton = ATTNLM(hf_tokenizer,\n",
    "                          embedding_size=128,\n",
    "                          hidden_size=128,\n",
    "                         ).to(device)\n",
    "attn_lm_hamilton.load_state_dict(torch.load(data_path + 'attn_lm_h.pt', map_location=device))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 200
  },
  {
   "cell_type": "code",
   "id": "ad6993eb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:01.050511Z",
     "start_time": "2024-12-18T21:37:00.930305Z"
    }
   },
   "source": [
    "grader.check(\"attnlm_sample\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 201
  },
  {
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:01.175401Z",
     "start_time": "2024-12-18T21:37:01.065460Z"
    }
   },
   "cell_type": "code",
   "source": "grader.check(\"attnlm_sample\")",
   "id": "cfc58730cf717cff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 202
  },
  {
   "cell_type": "markdown",
   "id": "c521cf4d",
   "metadata": {
    "id": "c521cf4d"
   },
   "source": [
    "## Sampling from the attention model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffc5a5f",
   "metadata": {
    "id": "6ffc5a5f"
   },
   "source": [
    "Let's try to sample from the models."
   ]
  },
  {
   "cell_type": "code",
   "id": "3d6d45f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:42.979180Z",
     "iopub.status.busy": "2024-12-16T09:27:42.978410Z",
     "iopub.status.idle": "2024-12-16T09:27:50.171576Z",
     "shell.execute_reply": "2024-12-16T09:27:50.170440Z"
    },
    "id": "3d6d45f7",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:05.544642Z",
     "start_time": "2024-12-18T21:37:01.191692Z"
    }
   },
   "source": [
    "print(' '.join(sample_sequence(attn_lm_madison, ('constitution', 'proposed', 'by', 'the'))), \"\\n\")\n",
    "print(' '.join(sample_sequence(attn_lm_hamilton, ('constitution', 'proposed', 'by', 'the'))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitution proposed by the united executive , we will far should as the effect , the government must find and an apportionment . not very [UNK] internal its number ? should be instead of state [UNK] alleged to half is , men , and in permit a mistake a measure necessarily were so universal act by a domestic security . be unwise an see [UNK] by order forms to in the legislative properly states by the senate . are and have appointed itself then hand to send on our power of but to be blended their present republics and [UNK] by a easily . \n",
      "\n",
      "constitution proposed by the united states , whatever we may held , the either , their first mode of an alteration , , in us , it is may are left as more necessary of making armies will become the [UNK] in [UNK] . the habitual [UNK] laid 1 not adversaries was so unnecessary . that [UNK] and the constitution , will be secured , if it has raised in the influence that to happen they sometimes choose a constant for alliance intrusted to i safely occasions not desirable that the complexion upon no community therefore or see he amount i affect it .\n"
     ]
    }
   ],
   "execution_count": 203
  },
  {
   "cell_type": "markdown",
   "id": "9f555302",
   "metadata": {
    "id": "9f555302"
   },
   "source": [
    "## Evaluating text according to the attention model"
   ]
  },
  {
   "cell_type": "code",
   "id": "6558ece3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:50.174983Z",
     "iopub.status.busy": "2024-12-16T09:27:50.174682Z",
     "iopub.status.idle": "2024-12-16T09:27:50.178792Z",
     "shell.execute_reply": "2024-12-16T09:27:50.177952Z"
    },
    "id": "6558ece3",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:05.559333Z",
     "start_time": "2024-12-18T21:37:05.557533Z"
    }
   },
   "source": [
    "document_madison = validation_madison[0]['tokens']\n",
    "document_hamilton = validation_hamilton[0]['tokens']"
   ],
   "outputs": [],
   "execution_count": 204
  },
  {
   "cell_type": "markdown",
   "id": "SyvPqJqWYk9o",
   "metadata": {
    "id": "SyvPqJqWYk9o"
   },
   "source": [
    "Just like in Lab 2-3, we want to evaluate the models using perplexity."
   ]
  },
  {
   "cell_type": "code",
   "id": "PyGFBoxiY70l",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:50.182139Z",
     "iopub.status.busy": "2024-12-16T09:27:50.181481Z",
     "iopub.status.idle": "2024-12-16T09:27:50.187586Z",
     "shell.execute_reply": "2024-12-16T09:27:50.186528Z"
    },
    "id": "PyGFBoxiY70l",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:05.574449Z",
     "start_time": "2024-12-18T21:37:05.571386Z"
    }
   },
   "source": [
    "def neglogprob(tokens, model):\n",
    "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
    "       according to a `model`\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    for i in range(len(tokens)):\n",
    "      context = tokens[:i]\n",
    "      token = tokens[i]\n",
    "      distribution = model(context)\n",
    "      logits = model(context)[:,-1]\n",
    "      probs = torch.softmax(logits, -1).view(-1) # vocab_size\n",
    "      distribution = {}\n",
    "      for i, prob in enumerate(probs):\n",
    "        word = hf_tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
    "        distribution[word] = prob.item()\n",
    "      prob = distribution[token] if token in distribution else distribution['[UNK]']\n",
    "      score += -math.log2(prob)\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 205
  },
  {
   "cell_type": "code",
   "id": "RGTuZU1qZUeL",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:50.190669Z",
     "iopub.status.busy": "2024-12-16T09:27:50.190215Z",
     "iopub.status.idle": "2024-12-16T09:27:50.194201Z",
     "shell.execute_reply": "2024-12-16T09:27:50.193389Z"
    },
    "id": "RGTuZU1qZUeL",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:05.588756Z",
     "start_time": "2024-12-18T21:37:05.586707Z"
    }
   },
   "source": [
    "def perplexity(tokens, model):\n",
    "    \"\"\"Returns the perplexity of a sequence of `tokens` according to a `model`\n",
    "    \"\"\"\n",
    "    return 2 ** (neglogprob(tokens, model) / (len(tokens)))"
   ],
   "outputs": [],
   "execution_count": 206
  },
  {
   "cell_type": "markdown",
   "id": "648e681e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "648e681e"
   },
   "source": [
    "Calculate the perplexity of each model on each document.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: rnn_ppl\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "2bd0ead7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:27:50.197073Z",
     "iopub.status.busy": "2024-12-16T09:27:50.196848Z",
     "iopub.status.idle": "2024-12-16T09:28:20.689455Z",
     "shell.execute_reply": "2024-12-16T09:28:20.688663Z"
    },
    "id": "2bd0ead7",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:26.443054Z",
     "start_time": "2024-12-18T21:37:05.600633Z"
    }
   },
   "source": [
    "# TODO\n",
    "attn_ppl_madison_model_madison_document = perplexity(document_madison, attn_lm_madison)\n",
    "attn_ppl_hamilton_model_madison_document = perplexity(document_madison, attn_lm_hamilton)\n",
    "attn_ppl_madison_model_hamilton_document = perplexity(document_hamilton, attn_lm_madison)\n",
    "attn_ppl_hamilton_model_hamilton_document = perplexity(document_hamilton, attn_lm_hamilton)"
   ],
   "outputs": [],
   "execution_count": 207
  },
  {
   "cell_type": "markdown",
   "id": "b03f10c1",
   "metadata": {
    "id": "b03f10c1"
   },
   "source": [
    "Now, let's compare those perplexity values. Do the results make sense to you?\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "d406c1c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T09:28:20.693139Z",
     "iopub.status.busy": "2024-12-16T09:28:20.692566Z",
     "iopub.status.idle": "2024-12-16T09:28:20.697229Z",
     "shell.execute_reply": "2024-12-16T09:28:20.696336Z"
    },
    "id": "d406c1c2",
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:26.459524Z",
     "start_time": "2024-12-18T21:37:26.457385Z"
    }
   },
   "source": [
    "print (f\"Author      Madison Model        Hamilton Model\\n\"\n",
    "       f\"Madison        {attn_ppl_madison_model_madison_document:5.1f}                {attn_ppl_hamilton_model_madison_document:5.1f}\\n\"\n",
    "       f\"Hamilton       {attn_ppl_madison_model_hamilton_document:5.1f}                {attn_ppl_hamilton_model_hamilton_document:5.1f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author      Madison Model        Hamilton Model\n",
      "Madison        168.1                232.2\n",
      "Hamilton       268.4                154.0\n"
     ]
    }
   ],
   "execution_count": 208
  },
  {
   "cell_type": "code",
   "id": "ea0a6212",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:26.481756Z",
     "start_time": "2024-12-18T21:37:26.475401Z"
    }
   },
   "source": [
    "grader.check(\"perplexities\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 209
  },
  {
   "cell_type": "markdown",
   "id": "y38sJMmtdUiV",
   "metadata": {
    "id": "y38sJMmtdUiV"
   },
   "source": [
    "Although attention is a useful concept, it alone isn't enough to build a strong model. In the next lab, you will learn about the *Transformer language model*, which is built on the attention mechanism along with several other important components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c59b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "396c59b2"
   },
   "source": [
    "# Lab debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following:\n",
    "\n",
    "* Was the lab too long or too short?\n",
    "* Were the readings appropriate for the lab?\n",
    "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
    "* Are there additions or changes you think would make the lab better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ea0bc0",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487c6064",
   "metadata": {
    "id": "487c6064"
   },
   "source": [
    "# End of Lab 2-4 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff35686",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ccd05fb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "1ae149fc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-18T21:37:26.713324Z",
     "start_time": "2024-12-18T21:37:26.496199Z"
    }
   },
   "source": [
    "grader.check_all()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attnlm_sample:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "causal_attention_mask:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "perplexities:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "uattnlm_sample:\n",
       "\n",
       "    All tests passed!\n",
       "    \n"
      ],
      "text/html": [
       "<p><strong>attnlm_sample:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>causal_attention_mask:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>perplexities:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>uattnlm_sample:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 210
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "Course 236299 Lab 2-6: Language modeling with attention",
  "vscode": {
   "interpreter": {
    "hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
