{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8e5d1336",
      "metadata": {
        "deletable": false,
        "editable": false,
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:32.525892Z",
          "iopub.status.busy": "2024-12-07T09:30:32.525572Z",
          "iopub.status.idle": "2024-12-07T09:30:33.894259Z",
          "shell.execute_reply": "2024-12-07T09:30:33.893266Z"
        },
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e5d1336",
        "outputId": "83ceb429-c896-40d5-86ff-01d6e0336a24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "\n",
        "       Prints the result to stdout and returns the exit status.\n",
        "       Provides a printed warning on non-zero exit status unless `warn`\n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2024-winter/lab2-5.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5c58249e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5c58249e"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "12d3dfd4",
      "metadata": {
        "id": "12d3dfd4"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3893e06-dedf-452c-a716-b92da4ac1f43",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "c3893e06-dedf-452c-a716-b92da4ac1f43"
      },
      "source": [
        "# Course 236299\n",
        "\n",
        "## Lab 2-5 – A single-layered Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a63fa93-7426-4768-865c-45b9e3f79f0d",
      "metadata": {
        "id": "7a63fa93-7426-4768-865c-45b9e3f79f0d"
      },
      "source": [
        "In the previous lab, you explored the **attention mechanism**, a crucial building block that enables models to dynamically focus on different parts of the input sequence. This marked a significant shift from traditional sequence models like n-grams and RNNs, which struggled with fixed context windows or vanishing gradients. By incorporating attention, we started to unlock the power of flexible, long-range context modeling.\n",
        "\n",
        "However, attention alone is not enough to fully capture the complexity of modern language models. In this lab, we will take the next step toward understanding the **Transformer** architecture by introducing several key components that, when combined with attention, form the backbone of this state-of-the-art model:\n",
        "\n",
        "1. **Positional Encoding:** Unlike RNNs, which inherently process input in sequence, the attention mechanism is inherently order-agnostic. (In fact, it's this property of attention that allows parallel implementation.) To give the model a sense of word order, we will add positional embeddings to the token embeddings, allowing the Transformer to account for sequence order during processing.\n",
        "2. **Multi-Layer Perceptrons (MLPs):** After attention has been applied, the output is passed through feed-forward neural networks. These MLPs, applied independently to each position, help the model process more complex representations of the input.\n",
        "3. **Normalization:** To improve training stability, we will introduce layer normalization, which ensures that the distribution of inputs to each layer remains balanced.\n",
        "4. **Residual Connections:** Finally, we will add residual connections, a technique that allows information to bypass certain layers. This enables the model to retain important information from earlier layers and helps mitigate the risk of gradient degradation during backpropagation.\n",
        "   \n",
        "By the end of this lab, you will have made use of all these components, which together form the foundation of the Transformer architecture. The Transformer model, through its parallelizable and scalable nature, has revolutionized fields like natural-language modeling, making it a central topic in modern deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2477bd27-f3c3-4f6c-8999-322796c01ba1",
      "metadata": {
        "id": "2477bd27-f3c3-4f6c-8999-322796c01ba1"
      },
      "source": [
        "# Preparation – Loading packages and data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609888f6-3265-41d5-a387-a986e917faae",
      "metadata": {
        "id": "609888f6-3265-41d5-a387-a986e917faae"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3632a011-c647-4f73-aa10-1ca54d420e0a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:33.897992Z",
          "iopub.status.busy": "2024-12-07T09:30:33.897535Z",
          "iopub.status.idle": "2024-12-07T09:30:38.788729Z",
          "shell.execute_reply": "2024-12-07T09:30:38.787924Z"
        },
        "id": "3632a011-c647-4f73-aa10-1ca54d420e0a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "import random\n",
        "import wget\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from math import inf\n",
        "from tokenizers import Tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8efc7244-8650-4658-b03c-065ae93a629e",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:38.792425Z",
          "iopub.status.busy": "2024-12-07T09:30:38.791831Z",
          "iopub.status.idle": "2024-12-07T09:30:38.799040Z",
          "shell.execute_reply": "2024-12-07T09:30:38.798215Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8efc7244-8650-4658-b03c-065ae93a629e",
        "outputId": "a4494008-b65b-48b6-cf7c-76b6931b374d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "# Set random seeds\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "# GPU check, sets runtime type to \"GPU\" where available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b4d90ac-6617-4d1c-8a20-a1893b561ee7",
      "metadata": {
        "id": "4b4d90ac-6617-4d1c-8a20-a1893b561ee7"
      },
      "source": [
        "Again, we use the *Federalist* papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6f1f34f0-9955-4f74-a881-4e7449380413",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:38.802252Z",
          "iopub.status.busy": "2024-12-07T09:30:38.801737Z",
          "iopub.status.idle": "2024-12-07T09:30:41.409614Z",
          "shell.execute_reply": "2024-12-07T09:30:41.408632Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f1f34f0-9955-4f74-a881-4e7449380413",
        "outputId": "1dd26998-5d67-4a9f-b9e0-274b06b54b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/federalist_data_raw2.json\n",
            "Downloading federalist_data_raw2.json from https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\n",
            "./data/tokenizer.pt\n",
            "Downloading tokenizer.pt from https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\n",
            "./data/transformer_lm_h.pt\n",
            "Downloading transformer_lm_h.pt from https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\n",
            "./data/transformer_lm_m.pt\n",
            "Downloading transformer_lm_m.pt from https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-a7ef7a718c83>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  hf_tokenizer = torch.load(data_path + \"tokenizer.pt\")\n"
          ]
        }
      ],
      "source": [
        "# Prepare to download needed data\n",
        "def download_if_needed(source, dest, filename):\n",
        "    print(f\"./{dest}{filename}\")\n",
        "    if os.path.exists(f\"./{dest}{filename}\"):\n",
        "        print (f\"Skipping {filename}\")\n",
        "    else:\n",
        "        print (f\"Downloading {filename} from {source}\")\n",
        "        wget.download(source + filename, out=dest)\n",
        "\n",
        "source_path = \"https://github.com/nlp-236299/data/raw/refs/heads/master/Federalist/\"\n",
        "data_path = \"data/\"\n",
        "\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "# Download files, including pretrained language models\n",
        "for filename in [\"federalist_data_raw2.json\",\n",
        "                 \"tokenizer.pt\",\n",
        "                 # language models:\n",
        "                 # Hamilton          Madison\n",
        "                 \"transformer_lm_h.pt\",   \"transformer_lm_m.pt\", # transformer\n",
        "                ]:\n",
        "    download_if_needed(source_path, data_path, filename)\n",
        "\n",
        "# Read in the raw data\n",
        "dataset = json.load(open(data_path + \"federalist_data_raw2.json\"))\n",
        "\n",
        "# Read in the pretrained tokenizer\n",
        "hf_tokenizer = torch.load(data_path + \"tokenizer.pt\")\n",
        "hf_tokenizer.split_special_tokens = False\n",
        "hf_tokenizer.pad_token_id = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffaa933b-3d25-4548-a490-41148a0a718d",
      "metadata": {
        "id": "ffaa933b-3d25-4548-a490-41148a0a718d"
      },
      "source": [
        "Once again we will split the dataset into training, validation, and test sets, but we won't be using the training set in this lab.\n",
        "\n",
        "Again, for the validation set, we have separate ones for papers authored by Hamilton (`validation_hamilton`) and papers authored by Madison (`validation_madison`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "61626a5a-7d4b-4cf1-b4d3-796cfe416427",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.413354Z",
          "iopub.status.busy": "2024-12-07T09:30:41.412702Z",
          "iopub.status.idle": "2024-12-07T09:30:41.420913Z",
          "shell.execute_reply": "2024-12-07T09:30:41.420005Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61626a5a-7d4b-4cf1-b4d3-796cfe416427",
        "outputId": "7bb0f57c-967d-41ec-ae2b-c8d499fd793e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madison validation size:  3 documents\n",
            "Hamilton validation size: 6 documents\n"
          ]
        }
      ],
      "source": [
        "# Split training, validation, and test sets\n",
        "TRAIN_RATIO = 0.9\n",
        "# Extract the papers of unknown authorship\n",
        "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n",
        "                      dataset))\n",
        "# Change gold labels in-place\n",
        "for ex in testing:\n",
        "    ex['authors'] = 'Madison'\n",
        "\n",
        "# Extract the papers by Madison\n",
        "dataset_madison = list(filter(lambda ex: ex['authors']=='Madison', dataset))\n",
        "random.seed(SEED)\n",
        "random.shuffle(dataset_madison)\n",
        "training_size_madison = int(math.floor(TRAIN_RATIO * len(dataset_madison)))\n",
        "validation_madison = dataset_madison[training_size_madison:]\n",
        "\n",
        "# Extract the papers by Hamilton\n",
        "dataset_hamilton = list(filter(lambda ex: ex['authors']=='Hamilton', dataset))\n",
        "random.seed(SEED)\n",
        "random.shuffle(dataset_hamilton)\n",
        "training_size_hamilton = int(math.floor(TRAIN_RATIO * len(dataset_hamilton)))\n",
        "validation_hamilton = dataset_hamilton[training_size_hamilton:]\n",
        "\n",
        "# We only consider the first 200 tokens of each document for speed\n",
        "def truncate(s, k=200):\n",
        "    for document in s:\n",
        "        document['tokens'] = document['tokens'][:k]\n",
        "truncate(validation_madison)\n",
        "truncate(validation_hamilton)\n",
        "truncate(testing)\n",
        "\n",
        "print (f\"Madison validation size:  {len(validation_madison)} documents\\n\"\n",
        "       f\"Hamilton validation size: {len(validation_hamilton)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19038fe6",
      "metadata": {
        "id": "19038fe6"
      },
      "source": [
        "# Issues of order in attention\n",
        "\n",
        "In the last lab, you implemented _attention_, and used it as a basis for a language model. The attention model improves on the RNN by allowing the output to be sensitive to the prior context in a much more expansive way. Whereas the RNN encapsulates all information about the prior context in a single fixed-size history vector, attention allows prior context to be calculated with access to the entire prior context as a weighted average of values associated with each prior token. In equations, the RNN sets\n",
        "\n",
        "$$\n",
        "\\vect{h}_t = \\vect{W}\\vect{h}_{t-1} + \\vect{V}\\vect{x}_t\\\\\n",
        "$$\n",
        "\n",
        "(depending only on $\\vect{h}_{t-1}$ to provide contextual information), whereas the attention model sets\n",
        "\n",
        "$$\n",
        "\\vect{h}_t = \\operatorname{attn}(\\vect{q}_t, \\vect{k}_{1:t}, \\vect{v}_{1:t})\n",
        "$$\n",
        "\n",
        "(which depends on the $\\vect{k}$ and $\\vect{v}$ values for _all_ of the preceding time steps)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25f7b585",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "25f7b585"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "---\n",
        "\n",
        "**Question:** Given this difference, what is the complexity of the output computations of RNN and attention as a function of the length $T$ of its input $x_{1:t}$? Which has worse complexity? You can give your answer using big-O notation.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_complexity\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df9be85",
      "metadata": {
        "id": "8df9be85"
      },
      "source": [
        "The transformer model has a worse complexity compared with the RNN model.\n",
        "\n",
        "For the RNN model:\n",
        "In each iteration we calculate a hidden and output vectors based on a single current token and a *single* prev hidden layer (which represents all prev tokens). This means that for a single token we make $O(1)$ calculations, so for overall t tokens we get $O(t)$ calculations.\n",
        "\n",
        "For the transformer model:\n",
        "The attention we calculate for each single token is based on all other tokens (ignoring the masking for simplicity, this doesnt affect the final answer, we made sure with the TA in class :). This means that for a single token we make $O(t)$ calculations. Overall for t tokens we get $O(t*t)=O(t^2)$ calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274d2388",
      "metadata": {
        "id": "274d2388"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "## Attention is order-independent\n",
        "\n",
        "Notice that the attention calculation is insensitive to permutations of context tokens. Imagine we permute some of the tokens in the context of an attention calculation. The attention calculation computes a weighted average of the $\\vect{v}_i$ with the weights based on the corresponding $\\vect{k}_i$. If we permute the context tokens, we permute the $\\vect{v}_i, \\vect{k}_i$ pairs, but the weighted average is not affected.\n",
        "\n",
        "This position independence has the nice property that the attention values at each time step ($\\vect{h}_t$) can be calculated independently, and therefore in parallel, for a tremendous efficiency advantage over RNNs, which can calculate $\\vect{h}_t$ only after $\\vect{h}_{t-1}$ has been calculated, that is, sequentially.\n",
        "\n",
        "On the other hand, this position independence has the unfortunate property that the attention values are _independent of word order_. Of course, natural language is highly dependent on word order. (Recall Charles Anderson Dana's observation about \"man bites dog\" and \"dog bites man\".) This is the first problem with attention that the Transformer model mitigates using _positional embeddings_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45aef09",
      "metadata": {
        "id": "b45aef09"
      },
      "source": [
        "## Positional embeddings to the rescue\n",
        "\n",
        "The order-independence of attention is typically resolved by encoding at each step not only an embedding for the token but also an embedding for the absolute position of that token. There have been many proposals for this _positional embedding_. We use a fixed embedding based on trigonometric functions as proposed in the seminal Transformers paper \"[Attention is all you need](https://arxiv.org/pdf/1706.03762)\".\n",
        "The positional embedding for time step $t$ is a vector of $D$ elements, each of which is defined to be the value of a sinusoidal function (either sine or cosine) at a phase determined by the element's index. Formally, it is something like\n",
        "\n",
        "$$ \\operatorname{pe}(t, i) = \\left\\{\n",
        "     \\begin{array}{ll}\n",
        "       \\sin(t / 10^{4i/D}) & \\mbox{if $i$ is even} \\\\\n",
        "       \\cos(t / 10^{4(i-1)/D}) & \\mbox{if $i$ is odd}\n",
        "     \\end{array}\n",
        "     \\right.\n",
        "$$\n",
        "\n",
        "where $t$ is the time step and $i$ is the index within the embedding vector.\n",
        "\n",
        "> The magic numbers 10 and 4 and the division by $D$ are empirically determined.\n",
        "\n",
        "The positional embedding may be concatenated with the input embedding or (as we do here) simply added."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "247ef2a2",
      "metadata": {
        "id": "247ef2a2"
      },
      "source": [
        "## Visualizing positional embeddings\n",
        "\n",
        "The intuition behind this sinusoidal positional embedding is that the values of the sinusoidal waves sampled at the different wavelengths provide information about where we are in the sequence of time steps. The visualization below depicts the positional embeddings for different positions as rows in a heat map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bd44b2d9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.424189Z",
          "iopub.status.busy": "2024-12-07T09:30:41.423843Z",
          "iopub.status.idle": "2024-12-07T09:30:41.844973Z",
          "shell.execute_reply": "2024-12-07T09:30:41.844033Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "bd44b2d9",
        "outputId": "11e01f9f-e753-471d-9095-a5f9e715de80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_positional_embeddings(seq_length, D):\n",
        "    \"\"\"\n",
        "    Returns samples from a positional embedding\n",
        "\n",
        "    Arguments:\n",
        "        seq_length: number of time steps in the sequence\n",
        "        D: size of the embedding dimension\n",
        "    Returns:\n",
        "        positional_embeddings: An array of seq_length elements,\n",
        "        each an array of D elements from the embedding\n",
        "    \"\"\"\n",
        "    positions = np.arange(seq_length)[:, np.newaxis]  # shape (seq_length, 1)\n",
        "    dimensions = np.arange(D)[np.newaxis, :]  # shape (1, d_model)\n",
        "\n",
        "    # Compute angle rates using the formula from the \"Attention Is All You Need\" paper\n",
        "    angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / np.float32(D))\n",
        "\n",
        "    # Generate angle radians\n",
        "    angle_rads = positions * angle_rates\n",
        "\n",
        "    # Apply sine to even indices and cosine to odd indices\n",
        "    positional_embeddings = np.zeros_like(angle_rads)\n",
        "    positional_embeddings[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    positional_embeddings[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    return positional_embeddings\n",
        "\n",
        "# Parameters\n",
        "seq_length = 20  # Length of the sequence\n",
        "D = 10  # Dimensionality of the model (must be even)\n",
        "\n",
        "# Get positional embeddings\n",
        "pos_embeddings = get_positional_embeddings(seq_length, D)\n",
        "\n",
        "# Plot the positional embeddings\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Sinusoidal Positional Embeddings\")\n",
        "plt.imshow(pos_embeddings, aspect=\"auto\", cmap=\"gray\")\n",
        "plt.colorbar(label=\"embedding value\")\n",
        "plt.xlabel(\"embedding dimension ($i$)\")\n",
        "plt.ylabel(\"position in sequence ($t$)\")\n",
        "plt.yticks(range(0,20,2))\n",
        "plt.xticks(range(0,10))\n",
        "None\n",
        "\n",
        "# Uncomment the line below if the plot does not show up.\n",
        "# Make sure to comment it before submitting to gradescope\n",
        "# since there would be some autograder issues with plt.show().\n",
        "\n",
        "#plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51dc6fa0",
      "metadata": {
        "id": "51dc6fa0"
      },
      "source": [
        "## Implementing positional embeddings\n",
        "\n",
        "We provide an implementation of this sinusoidal positional embedding, which also handles combining the positional embeddings with the token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d88c6738",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.848196Z",
          "iopub.status.busy": "2024-12-07T09:30:41.847956Z",
          "iopub.status.idle": "2024-12-07T09:30:41.854794Z",
          "shell.execute_reply": "2024-12-07T09:30:41.853933Z"
        },
        "id": "d88c6738"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(nn.Module):\n",
        "    \"\"\" \"Embeds a word both by its word id and by its position in the sentence.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_size, max_len=1024):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
        "        pe = torch.zeros(max_len, embedding_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, embedding_size, 2)\n",
        "            * -(math.log(10.0 ** 4.0) / embedding_size)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # 1, max_len, embedding_size\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        x = self.embed(batch) * math.sqrt(self.embedding_size)  # type embedding\n",
        "        # Add positional encoding to type embedding\n",
        "        x = x + self.pe[:, : x.size(1)].detach()\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c178d802-4d22-461a-9aaa-beb8f146e63d",
      "metadata": {
        "id": "c178d802-4d22-461a-9aaa-beb8f146e63d"
      },
      "source": [
        "# More required components\n",
        "\n",
        "We provide the implementation for attention and the causal mask, from the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ad76e8f7-47c0-4441-85d0-90100da52feb",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.857941Z",
          "iopub.status.busy": "2024-12-07T09:30:41.857396Z",
          "iopub.status.idle": "2024-12-07T09:30:41.864593Z",
          "shell.execute_reply": "2024-12-07T09:30:41.863717Z"
        },
        "id": "ad76e8f7-47c0-4441-85d0-90100da52feb"
      },
      "outputs": [],
      "source": [
        "def attention(batched_Q, batched_K, batched_V, mask=None):\n",
        "    \"\"\"\n",
        "    Performs the attention operation and returns the attention matrix\n",
        "    `batched_A` and the context matrix `batched_C` using queries\n",
        "    `batched_Q`, keys `batched_K`, and values `batched_V`.\n",
        "\n",
        "    Arguments:\n",
        "        batched_Q: (bsz, q_len, D)\n",
        "        batched_K: (bsz, k_len, D)\n",
        "        batched_V: (bsz, k_len, D)\n",
        "        mask: (bsz, q_len, k_len). An optional boolean mask *disallowing*\n",
        "              attentions where the mask value is *`False`*.\n",
        "    Returns:\n",
        "        batched_A: the normalized attention scores (bsz, q_len, k_len)\n",
        "        batched_C: a tensor of size (bsz, q_len, D).\n",
        "    \"\"\"\n",
        "    # Check sizes\n",
        "    D = batched_Q.size(-1)\n",
        "    bsz = batched_Q.size(0)\n",
        "    q_len = batched_Q.size(1)\n",
        "    k_len = batched_K.size(1)\n",
        "\n",
        "    assert batched_K.size(-1) == D and batched_V.size(-1) == D\n",
        "    assert batched_K.size(0) == bsz and batched_V.size(0) == bsz\n",
        "    assert batched_V.size(1) == k_len\n",
        "    if mask is not None:\n",
        "        assert mask.size() == torch.Size([bsz, q_len, k_len])\n",
        "\n",
        "    q = batched_Q                            # bsz, q_len, hidden\n",
        "    k = batched_K.transpose(1, 2)            # bsz, hidden, k_len\n",
        "    # Compute unnormalized attention scores\n",
        "    scores = torch.bmm(q, k) / math.sqrt(D)  # bsz, q_len, k_len\n",
        "    # Mask attention scores to -inf where mask is False\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == False, -inf)\n",
        "    batched_A = torch.softmax(scores, dim=-1)   # bsz, q_len, k_len\n",
        "    batched_C = torch.bmm(batched_A, batched_V) # bsz, q_len, D\n",
        "    # Verify that things sum up to one properly.\n",
        "    assert torch.all(\n",
        "        torch.isclose(batched_A.sum(-1), torch.ones(bsz, q_len).to(device))\n",
        "    )\n",
        "    return batched_A, batched_C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e3c9884b-629f-4439-a706-17b7f72788e7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.867328Z",
          "iopub.status.busy": "2024-12-07T09:30:41.867117Z",
          "iopub.status.idle": "2024-12-07T09:30:41.871130Z",
          "shell.execute_reply": "2024-12-07T09:30:41.870220Z"
        },
        "id": "e3c9884b-629f-4439-a706-17b7f72788e7"
      },
      "outputs": [],
      "source": [
        "def causal_mask(T):\n",
        "    \"\"\"\n",
        "    Generates a causal mask.\n",
        "    Arguments:\n",
        "        T: the length of target sequence\n",
        "    Returns:\n",
        "        mask: a T x T tensor, where `mask[i, j]` should be `True`\n",
        "        if y_i can attend to y_j, and `False` if y_i cannot\n",
        "        attend to y_j\n",
        "    \"\"\"\n",
        "    mask = torch.triu(torch.ones(T, T), diagonal=1) == 0\n",
        "    return mask.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6136144",
      "metadata": {
        "id": "e6136144"
      },
      "source": [
        "# Implementing the Transformer\n",
        "\n",
        "Now let's implement the Transformer.\n",
        "\n",
        "Putting together the various pieces discussed above, the Transformer operates as follows. Starting with the input sequence $\\vect{w}_1^T$, we first apply the positional embedding $\\textrm{pe}$\n",
        "\n",
        "$$\n",
        "\\vect{x}_t = \\textrm{pe}(\\vect{w}_t)\n",
        "$$\n",
        "\n",
        "from which we construct the query, key, and value for each input position, each as a separate linear layer\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\vect{q}_t &= \\textrm{linear}(\\vect{x}_t) \\\\\n",
        "\\vect{k}_t &= \\textrm{linear}(\\vect{x}_t) \\\\\n",
        "\\vect{v}_t &= \\textrm{linear}(\\vect{x}_t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "followed by the attention layer proper\n",
        "\n",
        "$$\n",
        "\\vect{h}_t = \\operatorname{attn}(\\vect{q}_t, \\vect{k}_1^t, \\vect{v}_1^t) \\rlap{\\qquad.}\n",
        "$$\n",
        "\n",
        "The first residual connection augments the attention output with the embedding that served as the source for the attention. We make sure to normalize the result.\n",
        "\n",
        "$$\n",
        "\\vect{h'}_t = \\textrm{norm}(\\vect{h}_t + \\vect{x}_t)\n",
        "$$\n",
        "\n",
        "We use this normalized vector as the input to an MLP.\n",
        "\n",
        "$$\n",
        "\\vect{m}_t = \\textrm{mlp}(\\vect{h'}_t)\n",
        "$$\n",
        "\n",
        "Again, we add in residuals of the input to the MLP and normalize.\n",
        "\n",
        "$$\n",
        "\\vect{m'}_t = \\textrm{norm}(\\vect{m}_t + \\vect{h'}_t)\n",
        "$$\n",
        "\n",
        "Finally, we generate the output with a final linear layer and softmax (absorbed into the loss function as usual)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7b181e-0dab-4395-b9b4-10e7eb5ba0d8",
      "metadata": {
        "id": "1f7b181e-0dab-4395-b9b4-10e7eb5ba0d8"
      },
      "source": [
        "Your job is to implement this pipeline as a forward function, accounting for the layer norms, MLP, and residual connections. We provide an appropriate set of modules to plug together to form the Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "27afdf2a-4b5e-48db-aa7a-893dffa563d3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.874137Z",
          "iopub.status.busy": "2024-12-07T09:30:41.873928Z",
          "iopub.status.idle": "2024-12-07T09:30:41.883588Z",
          "shell.execute_reply": "2024-12-07T09:30:41.882858Z"
        },
        "id": "27afdf2a-4b5e-48db-aa7a-893dffa563d3"
      },
      "outputs": [],
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, hf_tokenizer, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.hf_tokenizer = hf_tokenizer\n",
        "        self.pad_index = hf_tokenizer.pad_token_id\n",
        "        self.hidden_size = hidden_size\n",
        "        vocab_size = len(hf_tokenizer)\n",
        "\n",
        "        # Create modules\n",
        "        self.embed = PositionalEmbedding(vocab_size, hidden_size)\n",
        "        self.q_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.k_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.v_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "        self.hidden2output = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, context_words):\n",
        "        \"\"\"Computes the distribution over the next word given\n",
        "        context `context_words`.\n",
        "\n",
        "        Arguments:\n",
        "          context_words: batch of a list of word strings; could\n",
        "                         be an empty list when generating\n",
        "                         the first word.\n",
        "        Returns:\n",
        "          logits: a tensor of size (batch, seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        self.eval()\n",
        "        context = (\n",
        "            self.hf_tokenizer(\n",
        "                context_words, is_split_into_words=True, return_tensors=\"pt\"\n",
        "            )[\"input_ids\"]\n",
        "            .long()\n",
        "            .to(device)\n",
        "        )  # bsz, context_len\n",
        "        context_len = context.size(1)\n",
        "        context_bsz = context.size(0)\n",
        "\n",
        "        # For generating the first word, we feed in a special\n",
        "        # beginning-of-sentence symbol <pad>, which is also\n",
        "        # what we use for padding. In future labs we'll be\n",
        "        # using <bos>, but as long as training and evaluation\n",
        "        # use the same beginning-of-sentence symbol, it doesn't\n",
        "        # matter which particular symbol we use.\n",
        "        if context_len == 0:\n",
        "            context = context.new(1, 1).fill_(self.pad_index)\n",
        "            context_len = context.size(1)\n",
        "\n",
        "        # hidden = None\n",
        "\n",
        "        embeddings = self.embed(context)\n",
        "\n",
        "        q = self.q_proj(embeddings)\n",
        "        k = self.k_proj(embeddings)\n",
        "        v = self.v_proj(embeddings)\n",
        "        mask = causal_mask(context_len).unsqueeze(0).expand(context_bsz, -1, -1)\n",
        "        attention_output = attention(q, k, v, mask)[1]\n",
        "\n",
        "        # TODO: finish feedforward and set logits\n",
        "        # Logits should be a tensor of size (bsz, seq_length, vocab_size)\n",
        "        # The structure of the network is\n",
        "        #   embeddings -> attention -> residual connection\n",
        "        #              -> normalization (1) -> mlp\n",
        "        #              -> residual connection -> normalization (2)\n",
        "        #              -> hidden2output -> logits\n",
        "        first_normed = self.norm1(attention_output + embeddings)\n",
        "        mlp_output = self.mlp(first_normed)\n",
        "        second_normed = self.norm2(mlp_output + first_normed)\n",
        "        logits = self.hidden2output(second_normed)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7830105-faac-4aee-8dff-e6649fc6522d",
      "metadata": {
        "id": "a7830105-faac-4aee-8dff-e6649fc6522d"
      },
      "source": [
        "Now, let's load the pretrained models for Hamilton and Madison. The model `transformers_lm_madison` was trained on documents authored by Madison, whereas `transformers_lm_hamilton` was trained on documents authored by Hamilton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "adb38046-4859-40c8-858b-035fe98f3276",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.887578Z",
          "iopub.status.busy": "2024-12-07T09:30:41.887251Z",
          "iopub.status.idle": "2024-12-07T09:30:41.921170Z",
          "shell.execute_reply": "2024-12-07T09:30:41.920269Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adb38046-4859-40c8-858b-035fe98f3276",
        "outputId": "02593541-c98c-4555-c79b-5e184136a40a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4e0b657c0dc4>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(data_path + \"transformer_lm_m.pt\", map_location=device)\n",
            "<ipython-input-12-4e0b657c0dc4>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(data_path + \"transformer_lm_h.pt\", map_location=device)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Create and load Transformer LM for Madison\n",
        "transformer_lm_madison = Transformer(\n",
        "    hf_tokenizer,\n",
        "    embedding_size=128,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "transformer_lm_madison.load_state_dict(\n",
        "    torch.load(data_path + \"transformer_lm_m.pt\", map_location=device)\n",
        ")\n",
        "\n",
        "# Create and load Transformer LM for Hamilton\n",
        "transformer_lm_hamilton = Transformer(\n",
        "    hf_tokenizer,\n",
        "    embedding_size=128,\n",
        "    hidden_size=128,\n",
        ").to(device)\n",
        "transformer_lm_hamilton.load_state_dict(\n",
        "    torch.load(data_path + \"transformer_lm_h.pt\", map_location=device)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40833259-eee6-4931-b702-3a19dcd6f3c6",
      "metadata": {
        "id": "40833259-eee6-4931-b702-3a19dcd6f3c6"
      },
      "source": [
        "## Sampling from the transformer model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9faf426-11d2-4443-befb-3a648563d20c",
      "metadata": {
        "id": "d9faf426-11d2-4443-befb-3a648563d20c"
      },
      "source": [
        "Let's try to sample from the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "c5d28d61-85b7-4563-9742-0274286596d4",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.924423Z",
          "iopub.status.busy": "2024-12-07T09:30:41.923876Z",
          "iopub.status.idle": "2024-12-07T09:30:41.930499Z",
          "shell.execute_reply": "2024-12-07T09:30:41.929723Z"
        },
        "id": "c5d28d61-85b7-4563-9742-0274286596d4"
      },
      "outputs": [],
      "source": [
        "def sample(model, context):\n",
        "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
        "    logits = model(context)[:,-1] # calls internally to model.forward(context)\n",
        "\n",
        "     # Normalize to get probabilities\n",
        "    probs = torch.softmax(logits, -1).view(-1) # vocab_size\n",
        "\n",
        "    # Match probabilities with actual word types\n",
        "    distribution = {}\n",
        "    for i, prob in enumerate(probs):\n",
        "      word = model.hf_tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
        "      distribution[word] = prob.item()\n",
        "\n",
        "    prob_remaining = random.random()\n",
        "    for token, prob in sorted(distribution.items()):\n",
        "        if prob_remaining < prob:\n",
        "            return token\n",
        "        else:\n",
        "            prob_remaining -= prob\n",
        "    raise ValueError\n",
        "\n",
        "def sample_sequence(model, start_context, count=100):\n",
        "    \"\"\"Returns a sequence of tokens of length `count` sampled successively\n",
        "       from the `model` starting with the `start_context`\n",
        "    \"\"\"\n",
        "    random.seed(SEED) # for reproducibility\n",
        "    context = list(start_context)\n",
        "    result = list(start_context)\n",
        "    for i in range(0, count):\n",
        "        next = sample(model, tuple(context))\n",
        "        result.append(next)\n",
        "        context = context + [next]\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8b1f3b6f-dfc5-4489-8afd-59256e5c313c",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:41.933510Z",
          "iopub.status.busy": "2024-12-07T09:30:41.932984Z",
          "iopub.status.idle": "2024-12-07T09:30:48.834562Z",
          "shell.execute_reply": "2024-12-07T09:30:48.833579Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8b1f3b6f-dfc5-4489-8afd-59256e5c313c",
        "outputId": "09f2c9fc-f546-4132-d99e-d1eb7a867f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constitution proposed by the various it , we will derives portions , the electors , the general myself question , are a [UNK] in we . no lies in a national character hand has been feeble and unequal government formation , let , and interests . a national and members . were the union , consequently , by the constitutional authority with argument on a great britain ask of security parties have recollected there distinguish of the federal consideration , in all the security from the people liberty . the same and to co-operate as they shall rise may alarming exertions , by a \n",
            "\n",
            "constitution proposed by the union any , which will happily strikes a remedy , a right in so improved and and [UNK] , it would appear of redress of a more disunited of the laws are complicated up importance is , it , and injustice class , or [UNK] most an take the united states , all events the constitution , will be so . it is certainly the members of republican supported the legislature of the federal clause . it are made theme is that the point as the several . this expose [UNK] the public safety consideration , in a court ,\n"
          ]
        }
      ],
      "source": [
        "print(' '.join(sample_sequence(transformer_lm_madison, ('constitution', 'proposed', 'by', 'the'))), \"\\n\")\n",
        "print(' '.join(sample_sequence(transformer_lm_hamilton, ('constitution', 'proposed', 'by', 'the'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0a5c3948",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "0a5c3948",
        "outputId": "f3293c14-58f3-481a-b0d1-c3a1a3731d8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "grader.check(\"transformer_sample\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf6e99bb-5eb2-4e34-a6ee-ab77596b7367",
      "metadata": {
        "id": "bf6e99bb-5eb2-4e34-a6ee-ab77596b7367"
      },
      "source": [
        "## Evaluating text according to the transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "b2381e84-55e1-4d80-abde-cfb3cfa81aa9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:49.671835Z",
          "iopub.status.busy": "2024-12-07T09:30:49.671608Z",
          "iopub.status.idle": "2024-12-07T09:30:49.675441Z",
          "shell.execute_reply": "2024-12-07T09:30:49.674541Z"
        },
        "id": "b2381e84-55e1-4d80-abde-cfb3cfa81aa9"
      },
      "outputs": [],
      "source": [
        "document_madison = validation_madison[0]['tokens']\n",
        "document_hamilton = validation_hamilton[0]['tokens']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796633df-c509-4c9b-a3ca-279616f21050",
      "metadata": {
        "id": "796633df-c509-4c9b-a3ca-279616f21050"
      },
      "source": [
        "Just like in Labs 2-3 and 2-4, we want to evaluate the models using perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ea8be815-fb51-4c38-ae77-6f1dd02fdc4b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:49.678439Z",
          "iopub.status.busy": "2024-12-07T09:30:49.678227Z",
          "iopub.status.idle": "2024-12-07T09:30:49.683522Z",
          "shell.execute_reply": "2024-12-07T09:30:49.682629Z"
        },
        "id": "ea8be815-fb51-4c38-ae77-6f1dd02fdc4b"
      },
      "outputs": [],
      "source": [
        "def neglogprob(tokens, model):\n",
        "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
        "    according to a `model`\n",
        "    \"\"\"\n",
        "    score = 0.0\n",
        "    for i in range(len(tokens)):\n",
        "        context = tokens[:i]\n",
        "        token = tokens[i]\n",
        "        distribution = model(context)\n",
        "        logits = model(context)[:, -1]\n",
        "        probs = torch.softmax(logits, -1).view(-1)  # vocab_size\n",
        "        distribution = {}\n",
        "        for i, prob in enumerate(probs):\n",
        "            word = hf_tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
        "            distribution[word] = prob.item()\n",
        "        prob = distribution[token] if token in distribution \\\n",
        "               else distribution[\"[UNK]\"]\n",
        "        score += -math.log2(prob)\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e74e5673-f8de-40a8-b946-6aaa720a8919",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:49.686184Z",
          "iopub.status.busy": "2024-12-07T09:30:49.685970Z",
          "iopub.status.idle": "2024-12-07T09:30:49.689832Z",
          "shell.execute_reply": "2024-12-07T09:30:49.689036Z"
        },
        "id": "e74e5673-f8de-40a8-b946-6aaa720a8919"
      },
      "outputs": [],
      "source": [
        "def perplexity(tokens, model):\n",
        "    \"\"\"Returns the perplexity of a sequence of `tokens` according to a `model`\n",
        "    \"\"\"\n",
        "    return 2 ** (neglogprob(tokens, model) / (len(tokens)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1248c16d-496f-4921-a093-bcc5c0c9279f",
      "metadata": {
        "id": "1248c16d-496f-4921-a093-bcc5c0c9279f"
      },
      "source": [
        "Calculate the perplexity of each model on each document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4a33af8a-2673-452f-a9d3-f47cad645379",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:30:49.692764Z",
          "iopub.status.busy": "2024-12-07T09:30:49.692557Z",
          "iopub.status.idle": "2024-12-07T09:31:19.360694Z",
          "shell.execute_reply": "2024-12-07T09:31:19.359906Z"
        },
        "id": "4a33af8a-2673-452f-a9d3-f47cad645379"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "transformer_ppl_madison_model_madison_document = perplexity(document_madison, transformer_lm_madison)\n",
        "transformer_ppl_hamilton_model_madison_document = perplexity(document_madison, transformer_lm_hamilton)\n",
        "transformer_ppl_madison_model_hamilton_document = perplexity(document_hamilton, transformer_lm_madison)\n",
        "transformer_ppl_hamilton_model_hamilton_document = perplexity(document_hamilton, transformer_lm_hamilton)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85f40914-5c6f-483f-85ba-b64bd96127a4",
      "metadata": {
        "id": "85f40914-5c6f-483f-85ba-b64bd96127a4"
      },
      "source": [
        "Now, let's compare those perplexity values. Do the results make sense to you? Is it better than the attention-only model in lab2-4?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ab3f4b7b-5744-4097-99df-cde9744eda11",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-07T09:31:19.364066Z",
          "iopub.status.busy": "2024-12-07T09:31:19.363836Z",
          "iopub.status.idle": "2024-12-07T09:31:19.368497Z",
          "shell.execute_reply": "2024-12-07T09:31:19.367620Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab3f4b7b-5744-4097-99df-cde9744eda11",
        "outputId": "001383a2-2d87-4092-c923-fff4559a61b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author     Madison Model Hamilton Model \n",
            "Madison        129.3          136.6     \n",
            "Hamilton       137.1          100.3     \n"
          ]
        }
      ],
      "source": [
        "print (f\"{'Author'         : <10}\"\n",
        "       f\"{'Madison Model'  : ^15}\"\n",
        "       f\"{'Hamilton Model' : ^15}\\n\"\n",
        "       f\"{'Madison' : <10}\"\n",
        "       f\"{transformer_ppl_madison_model_madison_document   : ^15.1f}\"\n",
        "       f\"{transformer_ppl_hamilton_model_madison_document  : ^15.1f}\\n\"\n",
        "       f\"{'Hamilton' : <10}\"\n",
        "       f\"{transformer_ppl_madison_model_hamilton_document  : ^15.1f}\"\n",
        "       f\"{transformer_ppl_hamilton_model_hamilton_document : ^15.1f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e203eff1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "e203eff1",
        "outputId": "5ac84d89-c5cb-4416-8a2e-fc146d9909a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "grader.check(\"perplexities\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32fcca80-3f42-4598-8ae6-2bd6982fabce",
      "metadata": {
        "id": "32fcca80-3f42-4598-8ae6-2bd6982fabce"
      },
      "source": [
        "Our transformer model performs slightly better than the attention-only model, but its overall performance is still suboptimal.\n",
        "Why might this be? Several factors could explain the limitations:\n",
        "\n",
        "1. **Single-layer architecture:** This model uses only a single transformer layer. In practice, transformers show their true potential when stacked into multiple layers, allowing for richer representations and better learning capacity.\n",
        "2. **Limited data:** The dataset used for training is quite small, leading to rapid overfitting. This means that there was a large gap between the training perplexity and validation perplexity. In such cases, providing more data can often improve generalization and reduce overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd65d124-67ae-4d73-aebc-1502bf46dd62",
      "metadata": {
        "id": "dd65d124-67ae-4d73-aebc-1502bf46dd62"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following:\n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab?\n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5df732f",
      "metadata": {
        "id": "b5df732f"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93b28118-9225-4fd2-9c20-e8b9866726d3",
      "metadata": {
        "id": "93b28118-9225-4fd2-9c20-e8b9866726d3"
      },
      "source": [
        "# End of Lab 2-5 {-}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67527f9b",
      "metadata": {
        "id": "67527f9b"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2443681d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2443681d"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "af91925d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "id": "af91925d",
        "outputId": "a70c414a-ae82-4e09-aaa7-0cbcec7af4bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "perplexities:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "transformer_sample:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>perplexities:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>transformer_sample:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "Course 236299 Lab 2-7: Language modeling with transformers",
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}