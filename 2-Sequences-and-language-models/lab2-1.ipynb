{
 "cells": [
  {
   "cell_type": "code",
   "id": "7092034f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:12.292533Z",
     "start_time": "2024-12-12T18:25:11.277545Z"
    }
   },
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "     \n",
    "       Prints the result to stdout and returns the exit status. \n",
    "       Provides a printed warning on non-zero exit status unless `warn` \n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs236299-2024-winter/lab2-1.git .tmp\n",
    " mv .tmp/tests ./\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "ed098715",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:12.756774Z",
     "start_time": "2024-12-12T18:25:12.302392Z"
    }
   },
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "raw",
   "id": "bfe68042",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fff316",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103889a6",
   "metadata": {
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# Course 236299\n",
    "## Lab 2-1 – Language modeling with $n$-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ee225",
   "metadata": {},
   "source": [
    "We turn from tasks that _classify_ texts – mapping texts into a finite set of classes – to tasks that _model_ texts by providing a full probability distribution over texts (or providing a similar scoring metric). Such _language models_ attempt to answer the question \"How likely is a token sequence to be generated as an instance of the language?\".\n",
    "\n",
    "In this lab, you'll construct and apply a particularly simple kind of language model, the $n$-gram language model.\n",
    "\n",
    "After this lab, you should be able to\n",
    "\n",
    "* Construct an $n$-gram language model based on counts of the $n$-grams in a training text.\n",
    "* Apply an $n$-gram model to compute a text's probability and its perplexity.\n",
    "* Sample a text from the probability distribution defined by an $n$-gram language model.\n",
    "* Smooth an $n$-gram model to eliminate zero-probability $n$-grams using one of a small set of methods: add-$\\delta$, interpolation, and backoff smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb00246",
   "metadata": {},
   "source": [
    "We'll start with $n$-gram language models. Given a token sequence $\\vect{w} = w_1, w_2, \\ldots, w_N$, its probability $\\Prob(w_1, w_2, \\ldots, w_N)$ can be calculated using the chain rule of probability:\n",
    "\n",
    "$$\\Prob(A, B \\given \\theta)= \\Prob(A \\given \\theta) \\cdot \\Prob(B \\given A, \\theta) $$\n",
    "\n",
    "Thus, \n",
    "\n",
    "\\begin{align}\n",
    "\\Prob(w_1, w_2, \\ldots, w_N) & = \\Prob(w_1) \\cdot \\Prob(w_2, \\ldots, w_N \\given w_1) \\nonumber \\\\\n",
    "& = \\Prob(w_1) \\cdot \\Prob(w_2 \\given w_1) \\cdot \\Prob(w_3, \\ldots, w_N \\given w_1, w_2) \\nonumber \\\\\n",
    "& \\cdots  \\nonumber \\\\\n",
    "& = \\prod_{i=1}^N \\Prob (w_i \\given w_1, \\cdots, w_{i-1}) \\nonumber \\\\\n",
    "& \\approx \\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "In the last step, we replace the probability $\\Prob (w_i \\given w_1, \\cdots, w_{i-1})$, which conditions $w_i$ on _all_ of the preceding tokens, with $\\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})$, which conditions $w_i$ only on the $n-1$ preceding tokens. We call the $n-1$ preceding tokens ($w_{i-n+1}, \\cdots, w_{i-1}$) the _context_ and $w_i$ the _target_. Taken together, these $n$ tokens form an $n$-gram, hence the term _$n$-gram model_.\n",
    "\n",
    "In this lab you'll work with $n$-gram models: generating them, sampling from them, and scoring held-out texts according to them. We'll find some problems with $n$-gram models as language models:\n",
    "\n",
    "1. They are profligate with memory.\n",
    "2. They are sensitive to very limited context.\n",
    "3. They don't generalize well across similar words.\n",
    "\n",
    "In the next lab, we'll explore neural models to address these failings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242f945a",
   "metadata": {},
   "source": [
    "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
    "\n",
    "* [`itertools.product`](https://docs.python.org/3.8/library/itertools.html#itertools.product)\n",
    "* [`list`](https://docs.python.org/3/library/functions.html#func-list)\n",
    "* [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bc5160",
   "metadata": {},
   "source": [
    "# Preparation – Loading packages and data"
   ]
  },
  {
   "cell_type": "code",
   "id": "72d9c79c",
   "metadata": {
    "deletable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:14.459889Z",
     "start_time": "2024-12-12T18:25:12.790904Z"
    }
   },
   "source": [
    "import itertools\n",
    "import math\n",
    "import random\n",
    "import re\n",
    "import wget\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt', quiet=True) # download tokenizer module\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sys import getsizeof\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 1234\n",
    "random.seed(SEED)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "27a412a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.231223Z",
     "start_time": "2024-12-12T18:25:14.468384Z"
    }
   },
   "source": [
    "# Some utilities to manipulate the corpus\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Strips #comments and empty lines from a string\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()              # trim whitespace\n",
    "        line = re.sub('#.*$', '', line)  # trim comments\n",
    "        if line != '':                   # drop blank lines\n",
    "            result.append(line)\n",
    "    return result\n",
    "\n",
    "def nltk_normpunc_tokenize(str):\n",
    "    return nltk.tokenize.word_tokenize(str.lower())\n",
    "\n",
    "def geah_tokenize(lines):\n",
    "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        # tokenize\n",
    "        tokens = nltk_normpunc_tokenize(line)\n",
    "        # revert the speaker ID token\n",
    "        if tokens[0] == \"sam\":\n",
    "            tokens[0] = \"SAM:\"\n",
    "        elif tokens[0] == \"guy\":\n",
    "            tokens[0] = \"GUY:\"\n",
    "        else:\n",
    "            raise ValueError(\"format problem - bad speaker ID\")\n",
    "        # add a start of sentence token\n",
    "        result += [\"<s>\"] + tokens\n",
    "    return result\n",
    "                    \n",
    "def postprocess(tokens):\n",
    "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
    "    return ' '.join(tokens)\\\n",
    "              .replace(\"<s> \", \"\\n\")\n",
    "\n",
    "# Read the GEaH data and preprocess into training and test streams of tokens\n",
    "geah_filename = (\"https://github.com/nlp-236299/data/raw/master/Seuss/\"\n",
    "                 \"seuss - 1960 - green eggs and ham.txt\")\n",
    "os.makedirs('data', exist_ok=True)\n",
    "wget.download(geah_filename, out=\"data/\")\n",
    "\n",
    "def split(list, portions, offset):\n",
    "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n",
    "    \n",
    "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n",
    "    The small part is the piece at index `offset`; the large part is the remainder.\n",
    "    \"\"\"\n",
    "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n",
    "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n",
    "\n",
    "with open(\"data/seuss - 1960 - green eggs and ham.txt\", 'r') as fin:\n",
    "    lines = preprocess(fin.read())\n",
    "    train_lines, test_lines = split(lines, 12, 0)\n",
    "    train_tokens = geah_tokenize(train_lines)\n",
    "    test_tokens = geah_tokenize(test_lines)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "33d0cdc9",
   "metadata": {},
   "source": [
    "We've already loaded in the text of _Green Eggs and Ham_ for you and split it (about 90%/10%) into two token sequences, `train_tokens` and `test_tokens`. Here's a preview:"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea89e38d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.246097Z",
     "start_time": "2024-12-12T18:25:15.242556Z"
    }
   },
   "source": [
    "print(train_tokens[:50])\n",
    "print(postprocess(train_tokens[:50]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'SAM:', 'sam', 'i', 'am', '.', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'that', 'sam-i-am', '!', '<s>', 'SAM:', 'do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '?', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am']\n",
      "\n",
      "SAM: i am sam . \n",
      "SAM: sam i am . \n",
      "GUY: that sam-i-am ! \n",
      "GUY: that sam-i-am ! \n",
      "GUY: i do not like that sam-i-am ! \n",
      "SAM: do you like green eggs and ham ? \n",
      "GUY: i do not like them , sam-i-am\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "8eef7561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.262223Z",
     "start_time": "2024-12-12T18:25:15.259531Z"
    }
   },
   "source": [
    "print(test_tokens[:50])\n",
    "print(postprocess(test_tokens[:50]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'box', '.', '<s>', 'SAM:', 'eat', 'them', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', 'with', 'a', 'mouse', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'car', '!', '<s>', 'SAM:', 'in']\n",
      "\n",
      "SAM: i am sam . \n",
      "GUY: i do not like green eggs and ham . \n",
      "GUY: not in a box . \n",
      "SAM: eat them ! \n",
      "GUY: i do not like them with a mouse . \n",
      "GUY: not in a car ! \n",
      "SAM: in\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "508aa32b",
   "metadata": {},
   "source": [
    "We extract the vocabulary from the training text."
   ]
  },
  {
   "cell_type": "code",
   "id": "6ce77f61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.292178Z",
     "start_time": "2024-12-12T18:25:15.289213Z"
    }
   },
   "source": [
    "# Extract vocabulary from dataset\n",
    "vocabulary = list(set(train_tokens))\n",
    "print(vocabulary)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['would', 'if', 'see', 'i', ',', 'do', 'thank', 'are', 'try', 'rain', 'good', 'am', '...', 'boat', 'fox', 'say', 'box', 'here', 'goat', 'like', '?', 'or', 'eat', 'house', 'train', 'dark', 'sam-i-am', 'let', 'will', 'mouse', 'eggs', 'them', 'car', 'you', 'in', 'there', 'be', 'that', 'green', 'SAM:', '.', 'anywhere', 'tree', 'they', 'may', 'the', 'me', 'not', 'and', 'GUY:', 'sam', 'with', '!', '<s>', 'could', 'ham', 'a', 'on', 'so']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "c301b5ea",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Generating $n$-grams\n",
    "\n",
    "The _$n$-grams_ in a text are the contiguous subsequences of $n$ tokens. (We'll implement them as Python tuples.) In theory, any sequence of $n$ tokens is a potential $n$-gram type. Let's generate a list of all the possible $n$-gram types over a vocabulary. (Notice how the type/token distinction is useful for talking about $n$-grams, just as it is for words.)\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: all_ngrams\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "9b017aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.320594Z",
     "start_time": "2024-12-12T18:25:15.316321Z"
    }
   },
   "source": [
    "#TODO\n",
    "def all_ngrams(vocabulary, n):\n",
    "    \"\"\"Returns a list of all `n`-long *tuples* of elements of the `vocabulary`.\n",
    "    \n",
    "    For instance,  \n",
    "    \n",
    "        >>> all_ngrams([\"one\", \"two\"], 3)\n",
    "        [('one', 'one', 'one'),\n",
    "         ('one', 'one', 'two'),\n",
    "         ('one', 'two', 'one'),\n",
    "         ('one', 'two', 'two'),\n",
    "         ('two', 'one', 'one'),\n",
    "         ('two', 'one', 'two'),\n",
    "         ('two', 'two', 'one'),\n",
    "         ('two', 'two', 'two')]\n",
    "         \n",
    "    Order of returned list is not specified or guaranteed.\n",
    "    When `n` is 0, returns `[()]`.\n",
    "    \"\"\"\n",
    "    return list(itertools.product(vocabulary, repeat=n))"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "fa78cb46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.358347Z",
     "start_time": "2024-12-12T18:25:15.333265Z"
    }
   },
   "source": [
    "grader.check(\"all_ngrams\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "67b1316f",
   "metadata": {},
   "source": [
    "We can generate a list of all of the $n$-grams (tokens, not types) in a text."
   ]
  },
  {
   "cell_type": "code",
   "id": "d709f3d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.376239Z",
     "start_time": "2024-12-12T18:25:15.370275Z"
    }
   },
   "source": [
    "def ngrams(tokens, n):\n",
    "    \"\"\"Returns a list of all `n`-gram instances in a list of `tokens`, in order.\n",
    "    \n",
    "    For instance, \n",
    "    \n",
    "    >>> ngrams(nltk_normpunc_tokenize('I am Sam! Sam I am.'), 3)\n",
    "    [('i', 'am', 'sam'),\n",
    "     ('am', 'sam', '!'),\n",
    "     ('sam', '!', 'sam'),\n",
    "     ('!', 'sam', 'i'),\n",
    "     ('sam', 'i', 'am'),\n",
    "     ('i', 'am', '.')]\n",
    "    \"\"\"\n",
    "    return [tuple(tokens[i : i + n])\n",
    "            for i in range(0, len(tokens) - n + 1)]"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "76f4652c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.391742Z",
     "start_time": "2024-12-12T18:25:15.387826Z"
    }
   },
   "source": [
    "print (train_tokens[:6])\n",
    "print (ngrams(train_tokens[:6], 3))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'SAM:', 'i', 'am', 'sam', '.']\n",
      "[('<s>', 'SAM:', 'i'), ('SAM:', 'i', 'am'), ('i', 'am', 'sam'), ('am', 'sam', '.')]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "f4a427e0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Counting $n$-grams\n",
    "\n",
    "We conceptualize an $n$-gram as having two parts:\n",
    "\n",
    "* The _context_ is the first $n-1$ tokens in the $n$-gram.\n",
    "* The _target_ is the final token in the $n$-gram.\n",
    "\n",
    "An $n$-gram language model specifies a probability for each $n$-gram type. We'll implement a model as a 2-D dictionary, indexed first by context and then by target, providing the probability for the $n$-gram.\n",
    "\n",
    "We start by generating a similar data structure -- indexed first by context and then by target -- but for counts instead of probabilities. That is, it should store for each $n$-gram type the count of how many times it occurs in a given token sequence. It should contain counts for _every_ possible $n$-gram type (including those with a zero count).\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ngram_counts\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "4960c6d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.411517Z",
     "start_time": "2024-12-12T18:25:15.407703Z"
    }
   },
   "source": [
    "#TODO\n",
    "def ngram_counts(vocabulary, tokens, n):\n",
    "    \"\"\"Returns a dictionary of counts of the `n`-grams over the `vocabulary` \n",
    "    appearing in `tokens`.\n",
    "    \n",
    "    The dictionary is structured with first index by (n-1)-gram context\n",
    "    and second index by the final target token.\n",
    "    \n",
    "    For instance,\n",
    "    \n",
    "    >>> ngram_counts([\"a\",\"b\",\"c\"], nltk_normpunc_tokenize('a b c a b'), 2)\n",
    "    defaultdict(<function __main__.ngram_counts.<locals>.<lambda>()>,\n",
    "                {('a',): defaultdict(int, {'a': 0, 'b': 2, 'c': 0}),\n",
    "                 ('b',): defaultdict(int, {'a': 0, 'b': 0, 'c': 1}),\n",
    "                 ('c',): defaultdict(int, {'a': 1, 'b': 0, 'c': 0})})\n",
    "    \"\"\"\n",
    "    ngram_count = {}\n",
    "    for ngram in all_ngrams(vocabulary, n):\n",
    "            ngram_count[ngram[:n-1]] = {word: 0 for word in vocabulary}\n",
    "    for ngram in ngrams(tokens, n):\n",
    "        ngram_count[ngram[:n-1]][ngram[n-1]] += 1\n",
    "    return ngram_count"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "d1819c7f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.430435Z",
     "start_time": "2024-12-12T18:25:15.423054Z"
    }
   },
   "source": [
    "grader.check(\"ngram_counts\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "c3f4a067",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Use the `ngram_counts` function to generate count data structures for unigrams, bigrams, and trigrams for the _Green Eggs and Ham_ training text.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ngram_counts_geah\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "4d8dd794",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.983Z",
     "start_time": "2024-12-12T18:25:15.443293Z"
    }
   },
   "source": [
    "#TODO\n",
    "unigram_counts = ngram_counts(vocabulary, train_tokens, 1)\n",
    "bigram_counts = ngram_counts(vocabulary, train_tokens, 2)\n",
    "trigram_counts = ngram_counts(vocabulary, train_tokens, 3)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "e8670163",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:15.997399Z",
     "start_time": "2024-12-12T18:25:15.992005Z"
    }
   },
   "source": [
    "grader.check(\"ngram_counts_geah\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "073865c1",
   "metadata": {},
   "source": [
    "Check your work by examining the total count of unigrams, bigrams, and trigrams. Do the numbers make sense?"
   ]
  },
  {
   "cell_type": "code",
   "id": "d0d61481",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.014303Z",
     "start_time": "2024-12-12T18:25:16.008663Z"
    }
   },
   "source": [
    "# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n",
    "token_count = len(train_tokens)\n",
    "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n",
    "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n",
    "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)               \n",
    "\n",
    "# Report on the totals\n",
    "print(f\"Tokens:   {token_count:6}\\n\"\n",
    "      f\"Unigrams: {unigram_count:6}\\n\"\n",
    "      f\"Bigrams:  {bigram_count:6}\\n\"\n",
    "      f\"Trigrams: {trigram_count:6}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:     1145\n",
      "Unigrams:     59\n",
      "Bigrams:    3481\n",
      "Trigrams: 205379\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "338d7048",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Calculating $n$-gram probabilities\n",
    "\n",
    "We can convert the counts into a probability model by _normalizing_ the counts. Given an $n$-gram type $w_1, w_2, \\ldots, w_n$, instead of storing the count $\\cnt{w_1, w_2, \\ldots, w_n}$, we store an estimate of the probability \n",
    "\n",
    "\\begin{align*}\n",
    "  \\Pr(w_n \\given w_1, w_2, \\ldots, w_{n-1})\n",
    "  & \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\cnt{w_1, w_2, \\ldots, w_{n-1}}} \\\\\n",
    "  & = \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\sum_{w'} \\cnt{w_1, w_2, \\ldots, w_{n-1}, w'}}\n",
    "\\end{align*}\n",
    "\n",
    "that is, the ratio of the count of the $n$-gram and the sum of the counts of all $n$-grams with the same context. Fortunately, all of those counts are already stored in the count data structures we've already built. \n",
    "\n",
    "Write a function that takes an $n$-gram count data structure and returns an $n$-gram probability data structure. As with the counts, the probabilities should be stored indexed first by context and then by target.\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ngram_model\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "512e5d7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.030878Z",
     "start_time": "2024-12-12T18:25:16.027646Z"
    }
   },
   "source": [
    "#TODO\n",
    "from copy import deepcopy\n",
    "def ngram_model(ngram_counts):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
    "       provided `ngram-counts` dictionary\n",
    "    \"\"\"\n",
    "    model = deepcopy(ngram_counts)\n",
    "    for context, targets_dict in ngram_counts.items():\n",
    "        context_occurrences = sum(targets_dict.values())\n",
    "        for target in targets_dict.keys():\n",
    "            if context_occurrences != 0:\n",
    "                model[context][target] /= context_occurrences\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "7aa6a014",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.048732Z",
     "start_time": "2024-12-12T18:25:16.042019Z"
    }
   },
   "source": [
    "grader.check(\"ngram_model\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "8ce5ee5d",
   "metadata": {},
   "source": [
    "We can now build some $n$-gram models – unigram, bigram, and trigram – based on the counts."
   ]
  },
  {
   "cell_type": "code",
   "id": "1b2e72c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.209622Z",
     "start_time": "2024-12-12T18:25:16.059644Z"
    }
   },
   "source": [
    "unigram_model = ngram_model(unigram_counts)\n",
    "bigram_model = ngram_model(bigram_counts)\n",
    "trigram_model = ngram_model(trigram_counts)"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "de7ee4e9",
   "metadata": {},
   "source": [
    "# Space considerations\n",
    "\n",
    "For the most part, we aren't too concerned about matters of time or space efficiency, though these are crucial issues in the engineering of NLP systems. But the size of $n$-gram models merits consideration, looking especially at their size as $n$ grows. We can use Python's [`sys.getsizeof`](https://docs.python.org/3/library/sys.html#sys.getsizeof) function to get a rough sense of the size of the models we've been working with."
   ]
  },
  {
   "cell_type": "code",
   "id": "d6cac493",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.224191Z",
     "start_time": "2024-12-12T18:25:16.220713Z"
    }
   },
   "source": [
    "print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n",
    "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
    "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n",
    "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:     9792\n",
      "Unigrams:    232\n",
      "Bigrams:    2272\n",
      "Trigrams: 147552\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "3829ae8b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** What do these sizes tell you about the memory usage of $n$-gram models? With a larger vocabulary of, say, 10,000 word types, would it be practical to run, say, 5-gram models on your laptop?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_sizes\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5662bb6",
   "metadata": {},
   "source": "We can see from these sizes that the memory needed for n-gram models is exponentially ($|V|^n)$ increasing as a function of n is getting bigger. Considering these models with even larger train_tokens, we would get models that are unpractical to save and use on a standard laptop."
  },
  {
   "cell_type": "markdown",
   "id": "7593ac84",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Sampling from an $n$-gram model\n",
    "\n",
    "We have cleverly constructed the models to index by context. This allows us to sample a word given its context. For instance, in the trigram context `(\"<s>\", \"SAM:\")`, the following probability distribution captures which words can come next and with what probability:"
   ]
  },
  {
   "cell_type": "code",
   "id": "167ee88c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.240785Z",
     "start_time": "2024-12-12T18:25:16.235578Z"
    }
   },
   "source": [
    "trigram_model[(\"<s>\", \"SAM:\")]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'would': 0.2647058823529412,\n",
       " 'if': 0.0,\n",
       " 'see': 0.0,\n",
       " 'i': 0.029411764705882353,\n",
       " ',': 0.0,\n",
       " 'do': 0.029411764705882353,\n",
       " 'thank': 0.0,\n",
       " 'are': 0.0,\n",
       " 'try': 0.08823529411764706,\n",
       " 'rain': 0.0,\n",
       " 'good': 0.0,\n",
       " 'am': 0.0,\n",
       " '...': 0.0,\n",
       " 'boat': 0.0,\n",
       " 'fox': 0.0,\n",
       " 'say': 0.029411764705882353,\n",
       " 'box': 0.0,\n",
       " 'here': 0.058823529411764705,\n",
       " 'goat': 0.0,\n",
       " 'like': 0.0,\n",
       " '?': 0.0,\n",
       " 'or': 0.0,\n",
       " 'eat': 0.029411764705882353,\n",
       " 'house': 0.0,\n",
       " 'train': 0.0,\n",
       " 'dark': 0.0,\n",
       " 'sam-i-am': 0.0,\n",
       " 'let': 0.0,\n",
       " 'will': 0.0,\n",
       " 'mouse': 0.0,\n",
       " 'eggs': 0.0,\n",
       " 'them': 0.0,\n",
       " 'car': 0.0,\n",
       " 'you': 0.14705882352941177,\n",
       " 'in': 0.029411764705882353,\n",
       " 'there': 0.0,\n",
       " 'be': 0.0,\n",
       " 'that': 0.0,\n",
       " 'green': 0.0,\n",
       " 'SAM:': 0.0,\n",
       " '.': 0.0,\n",
       " 'anywhere': 0.0,\n",
       " 'tree': 0.0,\n",
       " 'they': 0.0,\n",
       " 'may': 0.0,\n",
       " 'the': 0.0,\n",
       " 'me': 0.0,\n",
       " 'not': 0.0,\n",
       " 'and': 0.029411764705882353,\n",
       " 'GUY:': 0.0,\n",
       " 'sam': 0.029411764705882353,\n",
       " 'with': 0.0,\n",
       " '!': 0.0,\n",
       " '<s>': 0.0,\n",
       " 'could': 0.08823529411764706,\n",
       " 'ham': 0.0,\n",
       " 'a': 0.11764705882352941,\n",
       " 'on': 0.0,\n",
       " 'so': 0.029411764705882353}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "a614539b",
   "metadata": {},
   "source": [
    "We can sample a single token according to this probability distribution. Here's one way to do so."
   ]
  },
  {
   "cell_type": "code",
   "id": "8ed2f5fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.260809Z",
     "start_time": "2024-12-12T18:25:16.258046Z"
    }
   },
   "source": [
    "def sample(model, context):\n",
    "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
    "    distribution = model[context]\n",
    "    prob_remaining = random.random()\n",
    "    for token, prob in sorted(distribution.items()):\n",
    "        if prob_remaining < prob:\n",
    "            return token\n",
    "        else:\n",
    "            prob_remaining -= prob\n",
    "    raise ValueError"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "f688dcc1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We can extend the sampling to a sequence of words by updating the context as we sample each word.\n",
    "\n",
    "Define a function `sample_sequence` that performs this sampling of a sequence. It's given a model and a starting context and begins by sampling the first token based on the starting context, then updates the starting context to reflect the word just sampled, repeating the process until a specified number of tokens have been sampled.\n",
    "\n",
    "> Hint: You might find function [`list`](https://docs.python.org/3/library/functions.html#func-list) helpful for converting immutable tuples to lists, and conversely [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple) helpful for converting lists to tuples.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: sample_sequence\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "2d29553d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.276219Z",
     "start_time": "2024-12-12T18:25:16.272606Z"
    }
   },
   "source": [
    "#TODO\n",
    "def sample_sequence(model, start_context, count=100):\n",
    "    \"\"\"Returns a sequence of `count` tokens sampled successively\n",
    "       from the `model` *following the `start_context`*.\n",
    "       The length of the returned list should be `count+len(start_context)`.\n",
    "    \"\"\"\n",
    "    random.seed(SEED) # for reproducibility, do not change\n",
    "    context = list(start_context)\n",
    "    for i in range(count):\n",
    "        sampled = sample(model, start_context)\n",
    "        if len(start_context) > 0:\n",
    "            start_context = tuple(list(start_context[1:]) + [sampled])\n",
    "        context.append(sampled)\n",
    "    return context"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "f98a563e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.293600Z",
     "start_time": "2024-12-12T18:25:16.289072Z"
    }
   },
   "source": [
    "grader.check(\"sample_sequence\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "0dbc0e47",
   "metadata": {},
   "source": [
    "Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "id": "2c9f1f9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.311238Z",
     "start_time": "2024-12-12T18:25:16.305608Z"
    }
   },
   "source": [
    "print(postprocess(sample_sequence(unigram_model, ())))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "would anywhere ! tree with i like . not \n",
      ", on SAM: i i \n",
      "\n",
      ". ! do would , fox could i . i GUY: ham in do SAM: ? with box eggs ! do ! \n",
      "could a , i \n",
      "ham \n",
      "with not would \n",
      "GUY: , GUY: sam-i-am \n",
      "\n",
      "would \n",
      "the , a SAM: SAM: say dark not could say them anywhere not sam-i-am GUY: . \n",
      "and . eggs thank do say in in SAM: like sam-i-am \n",
      "tree GUY: \n",
      "them not or are . a , GUY: ,\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "38220f62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.330325Z",
     "start_time": "2024-12-12T18:25:16.327245Z"
    }
   },
   "source": [
    "print(postprocess(sample_sequence(bigram_model, (\"<s>\",))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAM: sam ! \n",
      "SAM: try them , so good , will let me be ! \n",
      "GUY: and i would eat them here or there . \n",
      "GUY: i do not eat them anywhere . \n",
      "GUY: and ham . \n",
      "GUY: i do not , would you will eat them ! \n",
      "SAM: could not with a train ! \n",
      "GUY: i would not like them in the dark . \n",
      "GUY: i do not , sam-i-am . \n",
      "SAM: would you , sam-i-am . \n",
      "SAM: eat them on a train ! \n",
      "GUY: and ham !\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2c093121",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.359783Z",
     "start_time": "2024-12-12T18:25:16.356429Z"
    }
   },
   "source": [
    "print(postprocess(sample_sequence(trigram_model, (\"<s>\", \"SAM:\"))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SAM: you may , i will not eat green eggs and ham ? \n",
      "GUY: i do not like green eggs and ham . \n",
      "GUY: i do not like them anywhere ! \n",
      "SAM: say ! \n",
      "GUY: and i will eat them in a house . \n",
      "GUY: that sam-i-am ! \n",
      "GUY: not in a tree ! \n",
      "GUY: i do not like them in a tree . \n",
      "GUY: not in a box . \n",
      "GUY: not in the dark . \n",
      "GUY: not in the dark ! \n",
      "SAM: would you , in a car !\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "ee02c0d9",
   "metadata": {},
   "source": [
    "# Evaluating text according to an $n$-gram model\n",
    "\n",
    "## The probability metric\n",
    "\n",
    "The main point of a language model is to assign probabilities (or similar scores) to texts. For $n$-gram models, that's done according to Equation (1) at the start of the lab. Let's implement that. We define a function `probability` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the probability of the token sequence  according to the model. It merely multiplies all of the $n$-gram probabilities for all of the $n$-grams in the token sequence.\n",
    "\n",
    "> Throughout this lab, we ignore the scores of the first $n-1$ tokens as our $n$-gram model cannot score them due to the lack of context. In the next lab you will see how to solve this issue in practice."
   ]
  },
  {
   "cell_type": "code",
   "id": "e5778f62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.383649Z",
     "start_time": "2024-12-12T18:25:16.380224Z"
    }
   },
   "source": [
    "def probability(tokens, model, n):\n",
    "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
    "       `n`-gram `model`\n",
    "    \"\"\"\n",
    "    score = 1.0\n",
    "    context = tokens[0:n-1]\n",
    "    # Ignores the scores of the first n-1 tokens\n",
    "    for token in tokens[n-1:]:\n",
    "        prob = model[tuple(context)][token]\n",
    "        score *= prob\n",
    "        context = (context + [token])[1:]\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "1890bcbe",
   "metadata": {},
   "source": [
    "We test it on the test text that we held out from the training text."
   ]
  },
  {
   "cell_type": "code",
   "id": "e8e982ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.408849Z",
     "start_time": "2024-12-12T18:25:16.405783Z"
    }
   },
   "source": [
    "print(\"Test probability - unigram: \"\n",
    "          f\"{probability(test_tokens, unigram_model, 1):6e}\\n\"\n",
    "      \"Test probability -  bigram: \"\n",
    "          f\"{probability(test_tokens, bigram_model, 2):6e}\\n\"\n",
    "      \"Test probability - trigram: \"\n",
    "          f\"{probability(test_tokens, trigram_model, 3):6e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test probability - unigram: 6.404571e-154\n",
      "Test probability -  bigram: 9.147262e-44\n",
      "Test probability - trigram: 0.000000e+00\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "a43b55cf",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## The negative log probability metric\n",
    "\n",
    "Yikes, those probabilities are _really small_. Multiplying all those small numbers is likely to lead to underflow. \n",
    "\n",
    "To solve the underflow problem, we'll do our usual trick of using  log probabilities (in this case, _negative_ log probabilities) instead of probabilities:\n",
    "\n",
    "$$ - \\log_2 \\left(\\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\right)$$\n",
    "\n",
    "Define a function `neglogprob` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the negative log probability of the token sequence according to the model, calculating it in such a way as to avoid underflow. (You'll want to simplify the formula above before implementing it.)\n",
    "\n",
    "> Be careful when confronting zero probabilities. Taking `-math.log2(0)` raises a \"Math domain error\". Instead, you should use `math.inf` (Python's representation of infinity) as the value for the negative log of zero. This accords with our understanding that an impossible event would require infinite bits to specify.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: neglogprob\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "449fd97e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.430765Z",
     "start_time": "2024-12-12T18:25:16.425317Z"
    }
   },
   "source": [
    "#TODO\n",
    "def neglogprob(tokens, model, n):\n",
    "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
    "       according to an `n`-gram `model`\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    context = tokens[0:n-1]\n",
    "    # Ignores the scores of the first n-1 tokens\n",
    "    for token in tokens[n-1:]:\n",
    "        prob = model[tuple(context)][token]\n",
    "        if prob == 0:\n",
    "            return math.inf\n",
    "        score += -math.log2(prob)\n",
    "        context = (context + [token])[1:]\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "109597ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.442119Z",
     "start_time": "2024-12-12T18:25:16.435845Z"
    }
   },
   "source": [
    "grader.check(\"neglogprob\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "f6d7feb1",
   "metadata": {},
   "source": [
    "We compute the negative log probabilities of the test text using the different models and report on them."
   ]
  },
  {
   "cell_type": "code",
   "id": "53463d3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.459153Z",
     "start_time": "2024-12-12T18:25:16.455199Z"
    }
   },
   "source": [
    "unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n",
    "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n",
    "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n",
    "\n",
    "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n",
    "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n",
    "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test neglogprob - unigram: 508.897825\n",
      "Test neglogprob -  bigram: 142.971496\n",
      "Test neglogprob - trigram:    inf\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "51651123",
   "metadata": {},
   "source": [
    "There, those numbers seem more manageable. We can even convert the neglogprobs back into probabilities as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "id": "e299129d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.478533Z",
     "start_time": "2024-12-12T18:25:16.474871Z"
    }
   },
   "source": [
    "print(f\"Test probability - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n",
    "      f\"Test probability -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n",
    "      f\"Test probability - trigram: {2 ** (-trigram_test_nlp):6e}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test probability - unigram: 6.404571e-154\n",
      "Test probability -  bigram: 9.147262e-44\n",
      "Test probability - trigram: 0.000000e+00\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "44fdb3d2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** Why does the bigram model assign a lower neglogprob (that is, a higher probability) to the test text than the unigram model? Why does the trigram model assign a higher neglogprob (lower probability) to the test text than the other models?\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_ordering\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c3d68",
   "metadata": {},
   "source": [
    "The unigram model considers sequences of a single word, which means it has no context in its learning process. This means the model is probably under fit, as it assigns probabilities simply based on the frequency of a word in the training dataset, which isnt sufficient to \"describe\" or \"predict\" (assign probabilities) to longer, more complex unseen sentences.\n",
    "On the other hand, the bigram model does take into account a context of one word, which allows the model to assign values to sentences based on the \"relation\" between consecutive words, and not soley based on their frequency. This difference means the bi-gram model has more \"confidence\" in its predictions, as there are \"less options\" to follow a certain context.\n",
    "\n",
    "Lastly, there is the trigram model that takes into account 2 words for context. This model demonstrates over-fitting on the training set. In ngram models, overfitting is directly affected by the ratio between the size of the training set and the sequence length (n=3 in this case). When the n is relatively high compared to the training set, it might dramatically narrow down the options of different targets to follow a context. Thus, many test sentences will be assigned a 0 probability. In other words, this means that sentences in the test set that are not in the training set as well, are very difficult to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44bca2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "## The perplexity metric\n",
    "\n",
    "Another metric that is commonly used is _perplexity_. Jurafsky and Martin give a definition for perplexity as the \"inverse probability of the test set normalized by the number of words\":\n",
    "\n",
    "$$ PP(x_1, x_2, \\ldots, x_N) = \n",
    "     \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})}}\n",
    "$$\n",
    "\n",
    "Define a function `perplexity` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the perplexity of the token sequence according to the model, calculating it in such a way as to avoid underflow. (By now you're smart enough to realize that you'll want to carry out most of that calculation inside a $\\log$.)\n",
    "\n",
    "> Remember that we ignored the scores of the first n-1 tokens. What should the number of words `N` be?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: perplexity\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "5c93c7d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.511758Z",
     "start_time": "2024-12-12T18:25:16.507601Z"
    }
   },
   "source": [
    "#TODO\n",
    "def perplexity(tokens, model, n):\n",
    "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
    "       `n`-gram `model`\n",
    "    \"\"\"\n",
    "    N = len(tokens) - (n - 1)\n",
    "    if N == 0:\n",
    "        return math.inf\n",
    "    return (2 ** neglogprob(test_tokens, model, n)) ** (1 / N)"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "9d8b9361",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.552614Z",
     "start_time": "2024-12-12T18:25:16.546374Z"
    }
   },
   "source": [
    "grader.check(\"perplexity\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "0f1fe8b8",
   "metadata": {},
   "source": [
    "We can look at the perplexity of the test sample according to each of the models."
   ]
  },
  {
   "cell_type": "code",
   "id": "d78acddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.578979Z",
     "start_time": "2024-12-12T18:25:16.576137Z"
    }
   },
   "source": [
    "print(\"Test perplexity - unigram: \"\n",
    "          f\"{perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
    "      \"Test perplexity -  bigram: \"\n",
    "          f\"{perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
    "      \"Test perplexity - trigram: \"\n",
    "          f\"{perplexity(test_tokens, trigram_model, 3):.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity - unigram: 31.761\n",
      "Test perplexity -  bigram: 2.668\n",
      "Test perplexity - trigram: inf\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "markdown",
   "id": "4a2f2783",
   "metadata": {},
   "source": [
    "A perplexity value of $P$ can be interpreted as a measure of a model's average uncertainty in selecting each word, equivalent to selecting among $P$ equiprobable words on average. The bigram model gives a perplexity of less than 3, indicating that at each word in the sentence, the model is acting as if it is selecting among (slightly less than) three equiprobable words.\n",
    "\n",
    "For comparison, state of the art $n$-gram language models for more representative English text achieve perplexities of about 250."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcbe9c7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Smoothing $n$-gram language models\n",
    "\n",
    "> **This section is more open-ended in nature. Feel free to experiment.**\n",
    "\n",
    "The models we've been using have lots of zero-probability $n$-grams. Essentially any $n$-gram that doesn't appear in the training text is imputed a probability of zero, which means that any sentence that contains that $n$-gram will also be given a zero probability. Clearly this is not an accurate estimate.\n",
    "\n",
    "There are many ways to _smooth_ $n$-gram models, just as you smoothed classification models in earlier labs. The simplest is probably add-$\\delta$ smoothing. \n",
    "\n",
    "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1})\n",
    "  \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n} + \\delta}{\\cnt{w_1, w_2, \\ldots, w_{n-1}} + \\delta \\cdot |V|}\n",
    "$$\n",
    "\n",
    "Another useful method is to interpolate multiple $n$-gram models, for instance, estimating probabilities as an interpolation of trigram, bigram, and unigram models.\n",
    "\n",
    "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx\n",
    "     \\lambda_2 \\Prob(w_i \\given w_{i-2}, w_{i-1}) \n",
    "     + \\lambda_1 \\Prob(w_i \\given w_{i-1}) \n",
    "     + (1 - \\lambda_1 - \\lambda_2) \\Prob(w_i)\n",
    "$$\n",
    "\n",
    "Finally, a method called _backoff_ uses higher-order $n$-gram probabilities where available, \"backing off\" to lower order where necessary.\n",
    "\n",
    "$$\n",
    "\\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx \\begin{cases}\n",
    "    \\Prob(w_i \\given w_{i-2}, w_{i-1}) & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) > 0$}\\\\\n",
    "    \\Prob(w_i \\given w_{i-1})          & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) = 0$ and $\\Prob(w_i \\given w_{i-1}) > 0$}\\\\\n",
    "    \\Prob(w_i)                         & \\mbox{otherwise}\n",
    "  \\end{cases}\n",
    "$$\n",
    "\n",
    "Define a function `ngram_model_smoothed`, like the `ngram_model` function from above, but implementing one of these smoothing methods. Compare its perplexity on some sample text to the unsmoothed model. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_smoothed_model\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "57f732ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.786822Z",
     "start_time": "2024-12-12T18:25:16.600726Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "#TODO\n",
    "Place your definition of `ngram_model_smoothed` and whatever other testing \n",
    "of it you'd like to do in this and subsequent cells.\n",
    "\"\"\"\n",
    "def ngram_model_smoothed(ngram_counts):\n",
    "    \"\"\"Returns an n-gram probability model calculated by normalizing the\n",
    "       provided `ngram-counts` dictionary\n",
    "    \"\"\"\n",
    "    delta = 1\n",
    "    vocabulary_size = len(list(unigram_counts.values())[0])\n",
    "    model = deepcopy(ngram_counts)\n",
    "    for context, targets_dict in ngram_counts.items():\n",
    "        context_occurrences = sum(targets_dict.values())\n",
    "        for target in targets_dict.keys():\n",
    "            model[context][target] = (model[context][target] + delta) / (context_occurrences + vocabulary_size * delta)\n",
    "\n",
    "    return model\n",
    "\n",
    "unigram_model_smoothed = ngram_model_smoothed(unigram_counts)\n",
    "bigram_model_smoothed = ngram_model_smoothed(bigram_counts)\n",
    "trigram_model_smoothed = ngram_model_smoothed(trigram_counts)\n",
    "\n",
    "print(\"Test perplexity - unigram: \"\n",
    "      f\"{perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
    "      \"Test perplexity -  bigram: \"\n",
    "      f\"{perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
    "      \"Test perplexity - trigram: \"\n",
    "      f\"{perplexity(test_tokens, trigram_model, 3):.3f}\")\n",
    "print(\"Test perplexity - smoothed unigram: \"\n",
    "      f\"{perplexity(test_tokens, unigram_model_smoothed, 1):.3f}\\n\"\n",
    "      \"Test perplexity - smoothed bigram: \"\n",
    "      f\"{perplexity(test_tokens, bigram_model_smoothed, 2):.3f}\\n\"\n",
    "      \"Test perplexity - smoothed trigram: \"\n",
    "      f\"{perplexity(test_tokens, trigram_model_smoothed, 3):.3f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test perplexity - unigram: 31.761\n",
      "Test perplexity -  bigram: 2.668\n",
      "Test perplexity - trigram: inf\n",
      "Test perplexity - smoothed unigram: 31.833\n",
      "Test perplexity - smoothed bigram: 6.840\n",
      "Test perplexity - smoothed trigram: 9.977\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "29240cfe",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Lab debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following, but you're not restricted to these: \n",
    "\n",
    "* Was the lab too long or too short?\n",
    "* Were the readings appropriate for the lab? \n",
    "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
    "* Are there additions or changes you think would make the lab better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d2ba58",
   "metadata": {},
   "source": "This lab, comparing to the others was pretty long."
  },
  {
   "cell_type": "markdown",
   "id": "89d2aafc",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# End of Lab 2-1 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c806c08",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "b7c56f07",
   "metadata": {
    "deletable": false,
    "editable": false,
    "ExecuteTime": {
     "end_time": "2024-12-12T18:25:16.814508Z",
     "start_time": "2024-12-12T18:25:16.803201Z"
    }
   },
   "source": [
    "grader.check_all()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "all_ngrams:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "neglogprob:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ngram_counts:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ngram_counts_geah:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ngram_model:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "perplexity:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "sample_sequence:\n",
       "\n",
       "    All tests passed!\n",
       "    \n"
      ],
      "text/html": [
       "<p><strong>all_ngrams:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>neglogprob:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ngram_counts:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ngram_counts_geah:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ngram_model:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>perplexity:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>sample_sequence:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "Course 236299 Lab 2-1: Language modeling with n-grams"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
