{
 "cells": [
  {
   "cell_type": "code",
   "id": "0978d9c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0978d9c7",
    "outputId": "fc80dfd6-777c-4b00-dc11-225b9845f7f8",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:53.191875Z",
     "start_time": "2024-12-19T16:58:52.099541Z"
    }
   },
   "source": [
    "# Please do not change this cell because some hidden tests might depend on it.\n",
    "import os\n",
    "\n",
    "# Otter grader does not handle ! commands well, so we define and use our\n",
    "# own function to execute shell commands.\n",
    "def shell(commands, warn=True):\n",
    "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
    "\n",
    "       Prints the result to stdout and returns the exit status.\n",
    "       Provides a printed warning on non-zero exit status unless `warn`\n",
    "       flag is unset.\n",
    "    \"\"\"\n",
    "    file = os.popen(commands)\n",
    "    print (file.read().rstrip('\\n'))\n",
    "    exit_status = file.close()\n",
    "    if warn and exit_status != None:\n",
    "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
    "    return exit_status\n",
    "\n",
    "shell(\"\"\"\n",
    "ls requirements.txt >/dev/null 2>&1\n",
    "if [ ! $? = 0 ]; then\n",
    " rm -rf .tmp\n",
    " git clone https://github.com/cs236299-2024-winter/lab2-3.git .tmp\n",
    " mv .tmp/tests ./\n",
    " mv .tmp/requirements.txt ./\n",
    " rm -rf .tmp\n",
    "fi\n",
    "pip install -q -r requirements.txt\n",
    "\"\"\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "bdc4be97",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "bdc4be97",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:53.659137Z",
     "start_time": "2024-12-19T16:58:53.222907Z"
    }
   },
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook()"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "fb489159",
   "metadata": {
    "id": "fb489159",
    "tags": [
     "remove_for_latex"
    ]
   },
   "source": [
    "# Course 236299\n",
    "## Lab 2-3 – Language modeling with neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c5e9e",
   "metadata": {
    "id": "763c5e9e"
   },
   "source": [
    "In lab 2-1, you built and tested $n$-gram language models. Recall that some problems with $n$-gram language models are:\n",
    "\n",
    "1. They are profligate with memory.\n",
    "2. They are sensitive to very limited context.\n",
    "3. They don't generalize well across similar words.\n",
    "\n",
    "As promised, in this lab, you'll explore neural models to address these failings. You will:\n",
    "\n",
    "1. Build and test a neural $n$-gram language model.\n",
    "2. Build and test a neural RNN language model.\n",
    "3. Use language models for classification (*Federalist Papers* author identification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef2c06",
   "metadata": {
    "id": "7bef2c06"
   },
   "source": [
    "# Preparation – Loading packages and data {-}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ccb7646d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "id": "ccb7646d"
   },
   "source": [
    "%%latex\n",
    "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\newcommand{\\Prob}{\\Pr}\n",
    "\\newcommand{\\given}{\\,|\\,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b609f7d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "remove_for_latex"
    ],
    "id": "b609f7d6"
   },
   "source": [
    "$$\n",
    "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
    "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
    "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
    "\\renewcommand{\\Prob}{\\Pr}\n",
    "\\renewcommand{\\given}{\\,|\\,}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "id": "ba0c8608",
   "metadata": {
    "deletable": false,
    "id": "ba0c8608",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.722063Z",
     "start_time": "2024-12-19T16:58:53.695769Z"
    }
   },
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "import wget\n",
    "\n",
    "import torch\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "8e82ed8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e82ed8e",
    "outputId": "6a6df365-f5e6-4ad9-fa3c-b1e8df65f934",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.737205Z",
     "start_time": "2024-12-19T16:58:54.731548Z"
    }
   },
   "source": [
    "# Set random seeds\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# GPU check, sets runtime type to \"GPU\" where available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "508d5e36",
   "metadata": {
    "id": "508d5e36"
   },
   "source": [
    "The corpus used throughout this lab is the *Federalist* papers. We've trained two neural language models, one on papers written by Hamilton and one on papers written by Madison. We download them here."
   ]
  },
  {
   "cell_type": "code",
   "id": "0747160d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0747160d",
    "outputId": "6c12a1a2-d7e5-4064-d282-e4667206267b",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.809927Z",
     "start_time": "2024-12-19T16:58:54.753954Z"
    }
   },
   "source": [
    "# Prepare to download needed data\n",
    "def download_if_needed(source, dest, filename):\n",
    "    if not os.path.exists(f\"./{dest}{filename}\"):\n",
    "        wget.download(source + filename, out=dest)\n",
    "\n",
    "source_path = \"https://github.com/nlp-236299/\" \\\n",
    "              \"data/raw/master/Federalist/\"\n",
    "data_path = \"data/\"\n",
    "\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "# Download files, including pretrained language models\n",
    "for filename in [\"federalist_data_raw2.json\",\n",
    "                 \"tokenizer.pt\",\n",
    "                 # language models:\n",
    "                 # Hamilton        Madison\n",
    "                 \"ffnn_lm_h.pt\",   \"ffnn_lm_m.pt\", # feedforward NN\n",
    "                 \"rnn_lm_h.pt\",    \"rnn_lm_m.pt\"   # RNN\n",
    "                ]:\n",
    "    download_if_needed(source_path, data_path, filename)\n",
    "\n",
    "# Read in the raw data\n",
    "dataset = json.load(open(data_path + \"federalist_data_raw2.json\"))\n",
    "\n",
    "# Read in the pretrained tokenizer\n",
    "hf_tokenizer = torch.load(data_path + \"tokenizer.pt\")\n",
    "hf_tokenizer.split_special_tokens=False"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "cf287a3f",
   "metadata": {
    "id": "cf287a3f"
   },
   "source": [
    "First, let's split the dataset into training, validation, and test sets. Since we have provided pretrained models, you won't be using the training set in this lab. In the problem sets you will have opportunities to train models yourself.\n",
    "\n",
    "For this lab, we use a test set `testing`, which is the same as we used in lab 1-2. But for the validation set, we have separate ones for papers authored by Hamilton (`validation_hamilton`) and papers authored by Madison (`validation_madison`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "782e9eaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "782e9eaa",
    "outputId": "b5c881ff-c1bd-408e-d2f8-9d72949004ff",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.852991Z",
     "start_time": "2024-12-19T16:58:54.845920Z"
    }
   },
   "source": [
    "# Split training, validation, and test sets\n",
    "TRAIN_RATIO = 0.9\n",
    "# Extract the papers of unknown authorship\n",
    "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n",
    "                      dataset))\n",
    "# Change gold labels in-place\n",
    "for ex in testing:\n",
    "  ex['authors'] = 'Madison'\n",
    "\n",
    "# Extract the papers by Madison\n",
    "dataset_madison = list(filter(lambda ex: ex['authors']=='Madison', dataset))\n",
    "random.seed(SEED)\n",
    "random.shuffle(dataset_madison)\n",
    "training_size_madison = int(math.floor(TRAIN_RATIO * len(dataset_madison)))\n",
    "validation_madison = dataset_madison[training_size_madison:]\n",
    "\n",
    "# Extract the papers by Hamilton\n",
    "dataset_hamilton = list(filter(lambda ex: ex['authors']=='Hamilton', dataset))\n",
    "random.seed(SEED)\n",
    "random.shuffle(dataset_hamilton)\n",
    "training_size_hamilton = int(math.floor(TRAIN_RATIO * len(dataset_hamilton)))\n",
    "validation_hamilton = dataset_hamilton[training_size_hamilton:]\n",
    "\n",
    "# We only consider the first 200 tokens of each document for speed\n",
    "def truncate(document_set, k=200):\n",
    "  for document in document_set:\n",
    "    document['tokens'] = document['tokens'][:k]\n",
    "truncate(validation_madison)\n",
    "truncate(validation_hamilton)\n",
    "truncate(testing)\n",
    "\n",
    "print (f\"Madison validation size:  {len(validation_madison)} documents\\n\"\n",
    "       f\"Hamilton validation size: {len(validation_hamilton)} documents\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madison validation size:  3 documents\n",
      "Hamilton validation size: 6 documents\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "a428cccc",
   "metadata": {
    "id": "a428cccc"
   },
   "source": [
    "Note that, unlike in labs 1-2 and 1-3, here we consider _all_ word types in the data. Let's look at a couple of examples:"
   ]
  },
  {
   "cell_type": "code",
   "id": "3a4aaccd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a4aaccd",
    "outputId": "75f22722-019e-4267-8492-cb5948b3aeb7",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.890653Z",
     "start_time": "2024-12-19T16:58:54.887703Z"
    }
   },
   "source": [
    "print (f\"Example (Madison): {validation_madison[0]['tokens']}\\n\\n\"\n",
    "       f\"Example (Hamilton): {validation_hamilton[0]['tokens']}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example (Madison): ['it', 'is', 'not', 'a', 'little', 'remarkable', 'that', 'in', 'every', 'case', 'reported', 'by', 'ancient', 'history', ',', 'in', 'which', 'government', 'has', 'been', 'established', 'with', 'deliberation', 'and', 'consent', ',', 'the', 'task', 'of', 'framing', 'it', 'has', 'not', 'been', 'committed', 'to', 'an', 'assembly', 'of', 'men', ',', 'but', 'has', 'been', 'performed', 'by', 'some', 'individual', 'citizen', 'of', 'preeminent', 'wisdom', 'and', 'approved', 'integrity', '.', 'minos', ',', 'we', 'learn', ',', 'was', 'the', 'primitive', 'founder', 'of', 'the', 'government', 'of', 'crete', ',', 'as', 'zaleucus', 'was', 'of', 'that', 'of', 'the', 'locrians', '.', 'theseus', 'first', ',', 'and', 'after', 'him', 'draco', 'and', 'solon', ',', 'instituted', 'the', 'government', 'of', 'athens', '.', 'lycurgus', 'was', 'the', 'lawgiver', 'of', 'sparta', '.', 'the', 'foundation', 'of', 'the', 'original', 'government', 'of', 'rome', 'was', 'laid', 'by', 'romulus', ',', 'and', 'the', 'work', 'completed', 'by', 'two', 'of', 'his', 'elective', 'successors', ',', 'numa', 'and', 'tullius', 'hostilius', '.', 'on', 'the', 'abolition', 'of', 'royalty', 'the', 'consular', 'administration', 'was', 'substituted', 'by', 'brutus', ',', 'who', 'stepped', 'forward', 'with', 'a', 'project', 'for', 'such', 'a', 'reform', ',', 'which', ',', 'he', 'alleged', ',', 'had', 'been', 'prepared', 'by', 'tullius', 'hostilius', ',', 'and', 'to', 'which', 'his', 'address', 'obtained', 'the', 'assent', 'and', 'ratification', 'of', 'the', 'senate', 'and', 'people', '.', 'this', 'remark', 'is', 'applicable', 'to', 'confederate', 'governments', 'also', '.', 'amphictyon', ',', 'we', 'are', 'told', ',', 'was']\n",
      "\n",
      "Example (Hamilton): ['the', 'administration', 'of', 'government', ',', 'in', 'its', 'largest', 'sense', ',', 'comprehends', 'all', 'the', 'operations', 'of', 'the', 'body', 'politic', ',', 'whether', 'legislative', ',', 'executive', ',', 'or', 'judiciary', 'but', 'in', 'its', 'most', 'usual', ',', 'and', 'perhaps', 'its', 'most', 'precise', 'signification', '.', 'it', 'is', 'limited', 'to', 'executive', 'details', ',', 'and', 'falls', 'peculiarly', 'within', 'the', 'province', 'of', 'the', 'executive', 'department', '.', 'the', 'actual', 'conduct', 'of', 'foreign', 'negotiations', ',', 'the', 'preparatory', 'plans', 'of', 'finance', ',', 'the', 'application', 'and', 'disbursement', 'of', 'the', 'public', 'moneys', 'in', 'conformity', 'to', 'the', 'general', 'appropriations', 'of', 'the', 'legislature', ',', 'the', 'arrangement', 'of', 'the', 'army', 'and', 'navy', ',', 'the', 'directions', 'of', 'the', 'operations', 'of', 'war', ',', 'these', ',', 'and', 'other', 'matters', 'of', 'a', 'like', 'nature', ',', 'constitute', 'what', 'seems', 'to', 'be', 'most', 'properly', 'understood', 'by', 'the', 'administration', 'of', 'government', '.', 'the', 'persons', ',', 'therefore', ',', 'to', 'whose', 'immediate', 'management', 'these', 'different', 'matters', 'are', 'committed', ',', 'ought', 'to', 'be', 'considered', 'as', 'the', 'assistants', 'or', 'deputies', 'of', 'the', 'chief', 'magistrate', ',', 'and', 'on', 'this', 'account', ',', 'they', 'ought', 'to', 'derive', 'their', 'offices', 'from', 'his', 'appointment', ',', 'at', 'least', 'from', 'his', 'nomination', ',', 'and', 'ought', 'to', 'be', 'subject', 'to', 'his', 'superintendence', '.', 'this', 'view', 'of', 'the', 'subject', 'will', 'at', 'once', 'suggest', 'to', 'us', 'the', 'intimate']\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "f1392843",
   "metadata": {
    "id": "f1392843"
   },
   "source": [
    "# The $n$-gram feedforward network\n",
    "\n",
    "In lab 2-1, you built an $n$-gram language model using a lookup table. However, that model assigns zero probability to any $n$-gram that doesn't appear in the training text (without smoothing). In this lab, we consider a neural-network-based approach, which can address this issue.\n",
    "\n",
    "Recall that in $n$-gram language modeling, we made the assumption that the probability of a word only depends on its previous $n-1$ words:\n",
    "\n",
    "\\begin{align*}\n",
    "\\Prob(w_1, w_2, \\ldots, w_M) & = \\Prob(w_1) \\cdot \\Prob(w_2, \\ldots, w_M\\given w_1) \\\\\n",
    "& = \\Prob(w_1) \\cdot \\Prob(w_2 \\given w_1) \\cdot \\Prob(w_3 \\ldots, w_M \\given w_1, w_2) \\\\\n",
    "& \\cdots \\\\\n",
    "& = \\prod_{i=1}^M \\Prob (w_i \\given w_1, \\cdots, w_{i-1}) \\\\\n",
    "& \\approx \\prod_{i=1}^M \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1}),\n",
    "\\end{align*}\n",
    "\n",
    "and we used the empirical frequencies to estimate these conditional probabilities:\n",
    "\n",
    "$$\n",
    "\\Pr (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})= \\frac{\\cnt{w_{i-n+1}, \\cdots, w_{i-1}, w_i}}{\\sum_{x'} \\cnt{w_{i-n+1}, \\cdots, w_{i-1}, x'}}\n",
    "$$\n",
    "\n",
    "We can immediately see the problem with using a large $n$: the numerator would be 0 for any $n$-grams unseen in the training data.\n",
    "\n",
    "One way of solving this issue is to use a function that is \"smoother\". We parameterize the conditional probabilities using a neural network that computes a function $f$, which we use to estimate the probabilities.\n",
    "\n",
    "$$\n",
    "\\Pr (w_i \\given w_{i-n+1}, \\cdots, w_{i-1}) \\approx f_i(w_{i-n+1}, \\cdots, w_{i-1}),\n",
    "$$\n",
    "\n",
    "where $f$ is a function returning a vector of size $V$ (the vocabulary size). The $j$-th element of the returned vector $f_j$ stores the probability of generating the $j$-th word in the vocabulary. (We're being a little fast and loose with notation here. Strictly speaking, we should define these probability estimates in terms of word ids, that is, indices into the vocabulary.)\n",
    "\n",
    "To specify $f$, we can use a feedforward neural network. We'll represent words not by their ids or a one-hot representation; instead we map each word type in the vocabulary to a trainable vector called an _embedding_ of size `embedding_size`.\n",
    "\n",
    "Why do we represent words with such embeddings? To answer this question, let's consider two alternative representations: (1) word indices and (2) one-hot vectors (which we used in lab 1-1). (We cannot directly use the strings themselves because they are of varying lengths.) A desirable word representation system should be such that *the similarity of words can be reflected in the closeness of word representations*. Ideally, if two words have similar meaning and syntactic function, they should have similar representations, in order to alleviate the burden of learning such similarities by the rest of the model. For option (1), closeness in terms of word indices is meaningless: the 365-th word in the vocabulary is not more similar to the 366-th word than it is to other words, since the assignment of index in the vocabulary is arbitrary. For option (2), two different word types always have orthogonal vector representations, but we would hope that similar words can be placed near each other (at least we don't want to eliminate that possibility from the beginning).\n",
    "\n",
    "Therefore, we use an embedding, a vector representation for each word type in the vocabulary, which has been separately learned in a manner that has been shown to cluster similar words together. There are many such embeddings; the particular embedding we'll use is _word2vec_, a mapping from words to vectors of embedding size 128 trained under a task called \"masked language modeling\". If you are interested in more details, you should read the original [word2vec paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf). For our purposes, we can treat the embedding as just given to us.\n",
    "\n",
    "Now let's get back to the parameterization of $f(w_{i-n+1}, \\cdots, w_{i-1})$. We first map each word in $\\langle w_{i-n+1}, \\cdots, w_{i-1}\\rangle$ to its embedding $\\langle x_{i-n+1}, \\cdots, x_{i-1}\\rangle$ ($n-1$ vectors each of size `embedding_size`), and we concatenate these embeddings to a vector (of size `(n-1) * embedding_size`). Then, we apply a linear projection to project it down to size `hidden_size`, followed by a nonlinear function, and another linear projection to project to size $V$, followed by a softmax to normalize to probabilities. In this case, the nonlinear function we use is not a sigmoid. Instead, we use a Rectified Linear Unit (ReLU), which is simply a componentwise function that clips negative numbers at zero:\n",
    "\n",
    "$$ReLU(x) = \\max(0, x)$$\n",
    "\n",
    "We use $n=5$ in this lab."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d713827",
   "metadata": {
    "id": "2d713827",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.924413Z",
     "start_time": "2024-12-19T16:58:54.921685Z"
    }
   },
   "source": [
    "n = 5"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "42b50c1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "42b50c1f"
   },
   "source": [
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ffnn_forward\n",
    "-->\n",
    "\n",
    "Implement the missing part of the `forward` function below. This function takes the previous words (the entire previous history, not just the $n$-gram context) as input, and returns the probabilities of generating the next word (the target). (This design decision to take the entire history, even though the $n-1$-gram context is all that is used, is for consistency with the RNN language model that will be introduced later, which does use the full context.)\n",
    "\n",
    "The returned value should be a dictionary, with word types (vocabulary indices) as keys and their respective probabilities as values."
   ]
  },
  {
   "cell_type": "code",
   "id": "4c2cad87",
   "metadata": {
    "id": "4c2cad87",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.963788Z",
     "start_time": "2024-12-19T16:58:54.957171Z"
    }
   },
   "source": [
    "class FFNNLM(torch.nn.Module):\n",
    "  def __init__(self, n, tokenizer, embedding_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.n = n\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab = self.tokenizer.get_vocab()\n",
    "    vocab_size = len(self.vocab)\n",
    "    self.pad_index = self.tokenizer.pad_token_id\n",
    "\n",
    "    # Create modules\n",
    "    self.embed = torch.nn.Embedding(vocab_size, embedding_size)           # Embedding\n",
    "    self.sublayer1 = torch.nn.Linear((n-1) * embedding_size, hidden_size) # First layer\n",
    "    self.sublayer2 = torch.nn.ReLU()                                      # Second layer\n",
    "    self.hidden2output = torch.nn.Linear(hidden_size, vocab_size)         # Last layer\n",
    "\n",
    "  def forward(self, history_words):\n",
    "    \"\"\"Computes the distribution over the next word given context `history_words`.\n",
    "    Arguments:\n",
    "      history_words: a list of word strings; could be an empty list when generating\n",
    "                     the first word.\n",
    "    Returns:\n",
    "      the distribution over the next word given the context, stored as a dictionary,\n",
    "      with word types as keys, and probability values as values. The probability of\n",
    "      generating an unknown word is stored in `dictionary[\"[UNK]\"]`\"\"\"\n",
    "    # Switch to \"evaluation\" mode\n",
    "    self.eval()\n",
    "    # Convert strings to word ids\n",
    "    context = self.tokenizer(history_words, \\\n",
    "                             is_split_into_words=True, \\\n",
    "                             return_tensors='pt')['input_ids'] \\\n",
    "                .long() \\\n",
    "                .to(device) # bsz = 1, context_len\n",
    "    context_len = context.size(1)\n",
    "\n",
    "    # For generating the first words, we feed in a special but arbitrary beginning-of-sentence\n",
    "    # symbol. Here, we'll use `\"[PAD]\"`, which is also what we use for padding. In future\n",
    "    # labs we'll be using other tokens for this purpose, but as long as training and evaluation\n",
    "    # use the same beginning-of-sentence symbol, it doesn't matter which particular symbol we use.\n",
    "    if context_len < self.n - 1:\n",
    "      # Pad to the left if we don't have enough context words\n",
    "      padding = context.new(1, self.n - 1 - context_len).fill_(self.pad_index)\n",
    "      context = torch.cat([padding, context], 1)\n",
    "    else:\n",
    "      # TODO: Prepare proper context (just the previous n-1 words) from the full history\n",
    "      context = context[:,-self.n+1:]\n",
    "\n",
    "    embeddings = self.embed(context)           # 1, n-1, embedding_size\n",
    "    embeddings = embeddings.view(1, -1)        # 1, (n-1) * embedding_size\n",
    "    # TODO: finish feedforward and set logits from output\n",
    "    # Logits should be a tensor of size (1, vocab_size)\n",
    "    # The structure of the network is\n",
    "    #   embeddings -> sublayer1 -> sublayer2 -> hidden2output -> softmax\n",
    "    logits = self.hidden2output(self.sublayer2(self.sublayer1(embeddings)))\n",
    "\n",
    "    # Normalize to get probabilities\n",
    "    probs = torch.softmax(logits, -1).view(-1) # vocab_size\n",
    "\n",
    "    # Match probabilities with actual word types\n",
    "    distribution = {}\n",
    "    for i, prob in enumerate(probs):\n",
    "      word = self.tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
    "      distribution[word] = prob.item()\n",
    "    return distribution"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "783b2eb6",
   "metadata": {
    "id": "783b2eb6"
   },
   "source": [
    "Now, let's load the pretrained feedforward language models for Hamilton and Madison. The model `ffnn_lm_madison` was trained on documents authored by Madison, whereas `ffnn_lm_hamilton` was trained on documents authored by Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "id": "de37cc91",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de37cc91",
    "outputId": "48ecd962-c924-44d9-fd5b-7ab743830a9f",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:54.994918Z",
     "start_time": "2024-12-19T16:58:54.971590Z"
    }
   },
   "source": [
    "# Create and load feedforward LM for Madison\n",
    "ffnn_lm_madison = FFNNLM(n, hf_tokenizer,\n",
    "                         embedding_size=128,\n",
    "                         hidden_size=128\n",
    "                        ).to(device)\n",
    "ffnn_lm_madison.load_state_dict(torch.load(data_path + 'ffnn_lm_m.pt', map_location=device))\n",
    "\n",
    "# Create and load feedforward LM for Hamilton\n",
    "ffnn_lm_hamilton = FFNNLM(n, hf_tokenizer,\n",
    "                          embedding_size=128,\n",
    "                          hidden_size=128,\n",
    "                         ).to(device)\n",
    "ffnn_lm_hamilton.load_state_dict(torch.load(data_path + 'ffnn_lm_h.pt', map_location=device))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "5e8a3324",
   "metadata": {
    "id": "5e8a3324"
   },
   "source": [
    "## Sampling from an $n$-gram feedforward network\n",
    "\n",
    "Recall from lab 2-1 that we can sample a sequence of text from a model using the functions below. Again, the `sample` function here takes as an argument the full context instead of just the previous $n-1$ words, for consistency with the later RNN model."
   ]
  },
  {
   "cell_type": "code",
   "id": "98071aff",
   "metadata": {
    "id": "98071aff",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:55.039485Z",
     "start_time": "2024-12-19T16:58:55.030703Z"
    }
   },
   "source": [
    "def sample(model, context):\n",
    "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
    "    distribution = model(context) # calls internally to model.forward(context)\n",
    "    prob_remaining = random.random()\n",
    "    for token, prob in sorted(distribution.items()):\n",
    "        if prob_remaining < prob:\n",
    "            return token\n",
    "        else:\n",
    "            prob_remaining -= prob\n",
    "    raise ValueError\n",
    "\n",
    "def sample_sequence(model, start_context, count=100):\n",
    "    \"\"\"Returns a sequence of tokens of length `count` sampled successively\n",
    "       from the `model` starting with the `start_context`\n",
    "    \"\"\"\n",
    "    random.seed(SEED) # for reproducibility\n",
    "    context = list(start_context)\n",
    "    result = list(start_context)\n",
    "    for i in range(0, count):\n",
    "        next = sample(model, tuple(context))\n",
    "        result.append(next)\n",
    "        context = context + [next]\n",
    "    return result"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "b95d18c5",
   "metadata": {
    "id": "b95d18c5"
   },
   "source": [
    "Let's try to sample from our models. (Don't expect much fluency in the samples, since the dataset it is trained on is small.)"
   ]
  },
  {
   "cell_type": "code",
   "id": "43365576",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43365576",
    "outputId": "fc3451c3-b802-4494-c15b-d7d587f5b8fd",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:58:59.694562Z",
     "start_time": "2024-12-19T16:58:55.061636Z"
    }
   },
   "source": [
    "print(' '.join(sample_sequence(ffnn_lm_madison, ('constitution', 'proposed', 'by', 'the'))), \"\\n\")\n",
    "print(' '.join(sample_sequence(ffnn_lm_hamilton, ('constitution', 'proposed', 'by', 'the'))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitution proposed by the united states , which will have laid against the elevation but the federal innovations is [UNK] , and , is very [UNK] invested in the authors of that reason . little far as we have have [UNK] like , and in mankind , or as more and to the union , at a constitution passions , and very [UNK] taxes , if gives give the legislative powers falling that they is the rights of any commission , and mean them in the people of its portion of civil suffrages , and the principles of powerful acts . a enjoyed and \n",
      "\n",
      "constitution proposed by the usual has , very time never it , the convention , the general models of course , and , in treat . the legislature may are merchants . the person of nations . we have far [UNK] less , and instance , adopt properly be less [UNK] to the union , has , except to be allowed to acquire the [UNK] hands of one situation ? these is sometimes take it to serve by a [UNK] in a measures than members of the political difference have prospect for this determinate augmented states should propriety either [UNK] from a distribution but\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "8fe2f213",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "8fe2f213",
    "outputId": "b9f414dd-499d-4fef-bf95-122d4a45aadb",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.177177Z",
     "start_time": "2024-12-19T16:58:59.713511Z"
    }
   },
   "source": [
    "grader.check(\"ffnn_sample\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "945fdd2d",
   "metadata": {
    "id": "945fdd2d"
   },
   "source": [
    "## Evaluating text according to an $n$-gram feedforward network\n",
    "\n",
    "Now let's use our language model to score text. Since the $n$-gram feedforward network is able to score with zero context – internally, it pads on the left with instances of the padding token that the tokenizer provided – `ffnn_lm_hamilton.forward([])` will return the probability distribution $\\Pr(x_1)$ for the first word in a document."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd317c46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd317c46",
    "outputId": "29a07225-1d18-4d7b-ba3a-8e569d16f9fc",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.221055Z",
     "start_time": "2024-12-19T16:59:04.193511Z"
    }
   },
   "source": [
    "Pr_x1 = ffnn_lm_hamilton([])\n",
    "topk = 9\n",
    "\n",
    "# Sort by probabilities\n",
    "for i, word in enumerate(sorted(Pr_x1, key=lambda word: Pr_x1[word], reverse=True)[:topk]):\n",
    "    print (f\"top {i+1} word: {word:<8} Pr(x1): {Pr_x1[word]:.3f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 1 word: ,        Pr(x1): 0.048\n",
      "top 2 word: the      Pr(x1): 0.046\n",
      "top 3 word: of       Pr(x1): 0.031\n",
      "top 4 word: in       Pr(x1): 0.028\n",
      "top 5 word: [UNK]    Pr(x1): 0.027\n",
      "top 6 word: that     Pr(x1): 0.020\n",
      "top 7 word: which    Pr(x1): 0.019\n",
      "top 8 word: to       Pr(x1): 0.018\n",
      "top 9 word: and      Pr(x1): 0.016\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "103254de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "103254de"
   },
   "source": [
    "Define a function `neglogprob` that takes a token sequence and a language model and returns the negative log probability of the _entire_ token sequence according to the model (using log base 2). Note that you need to use the probability for the unknown word type `\"[UNK]\"` if a token does not appear in the vocabulary.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ffnn_neglogprob\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "0dc34ef0",
   "metadata": {
    "id": "0dc34ef0",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.251186Z",
     "start_time": "2024-12-19T16:59:04.247488Z"
    }
   },
   "source": [
    "# TODO\n",
    "def neglogprob(tokens, model):\n",
    "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
    "       according to a `model`\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for i in range(len(tokens)):\n",
    "      probs = model(tokens[0:i])\n",
    "      if tokens[i] in probs:\n",
    "        curr_prob = probs[tokens[i]]\n",
    "      else:\n",
    "        curr_prob = probs['[UNK]']\n",
    "      score -= math.log2(curr_prob)\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "eb790aab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "eb790aab",
    "outputId": "593e4a24-7d4d-4ca2-cbe4-c48b8eb38ece",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.317452Z",
     "start_time": "2024-12-19T16:59:04.270386Z"
    }
   },
   "source": [
    "grader.check(\"ffnn_neglogprob\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "f6009841",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6009841",
    "outputId": "ad868cb0-15c1-449d-a6c5-c1057b2ae324",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.368581Z",
     "start_time": "2024-12-19T16:59:04.333468Z"
    }
   },
   "source": [
    "round(neglogprob([\"constitution\",], ffnn_lm_madison), 2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "418e24bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "418e24bf"
   },
   "source": [
    "Define a function `perplexity` that takes a token sequence and a language model and returns the perplexity of the _entire_ token sequence according to the model.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ffnn_perplexity\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "710af108",
   "metadata": {
    "id": "710af108",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.390560Z",
     "start_time": "2024-12-19T16:59:04.387382Z"
    }
   },
   "source": [
    "# TODO\n",
    "def perplexity(tokens, model):\n",
    "    \"\"\"Returns the perplexity of a sequence of `tokens` according to a `model`\n",
    "    \"\"\"\n",
    "    N = len(tokens)\n",
    "    neglogprobability = neglogprob(tokens, model)\n",
    "    power = neglogprobability/N\n",
    "    return 2**power"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "da5df56e",
   "metadata": {
    "id": "da5df56e"
   },
   "source": [
    "What's the perplexity of each document in the validation set under the language model trained on papers authored by Madison? What about Hamilton? Let's start with one document from each author."
   ]
  },
  {
   "cell_type": "code",
   "id": "1f097a20",
   "metadata": {
    "id": "1f097a20",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:04.409470Z",
     "start_time": "2024-12-19T16:59:04.406660Z"
    }
   },
   "source": [
    "document_madison = validation_madison[0]['tokens']\n",
    "document_hamilton = validation_hamilton[0]['tokens']"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "e8d9b481",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "e8d9b481"
   },
   "source": [
    "Calculate the perplexity of each model on `document_madison` and  `document_hamilton`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ffnn_ppl\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "499ce7e1",
   "metadata": {
    "id": "499ce7e1",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:22.195383Z",
     "start_time": "2024-12-19T16:59:04.425115Z"
    }
   },
   "source": [
    "# TODO\n",
    "ppl_madison_model_madison_document = perplexity(document_madison, ffnn_lm_madison)\n",
    "ppl_hamilton_model_madison_document = perplexity(document_madison, ffnn_lm_hamilton)\n",
    "ppl_madison_model_hamilton_document = perplexity(document_hamilton, ffnn_lm_madison)\n",
    "ppl_hamilton_model_hamilton_document = perplexity(document_hamilton, ffnn_lm_hamilton)"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "c7def63c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "c7def63c",
    "outputId": "486c1a21-323e-4e86-ad57-5f7bb39d4eb9",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:22.220255Z",
     "start_time": "2024-12-19T16:59:22.214337Z"
    }
   },
   "source": [
    "grader.check(\"ffnn_ppl\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "ff470b22",
   "metadata": {
    "id": "ff470b22"
   },
   "source": [
    "Now, let's compare those perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "id": "ea3a884d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea3a884d",
    "outputId": "dc04acf8-0c79-4207-e0a2-37a5b472ae66",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:22.241827Z",
     "start_time": "2024-12-19T16:59:22.239753Z"
    }
   },
   "source": [
    "print (f\"Author    Madison Model    Hamilton Model\\n\"\n",
    "       f\"Madison      {ppl_madison_model_madison_document:5.1f}            {ppl_hamilton_model_madison_document:5.1f}\\n\"\n",
    "       f\"Hamilton     {ppl_madison_model_hamilton_document:5.1f}            {ppl_hamilton_model_hamilton_document:5.1f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author    Madison Model    Hamilton Model\n",
      "Madison      126.5            172.6\n",
      "Hamilton     140.5            107.5\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "2068227e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2068227e"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** What do you find? Why?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_ppl\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c139c3b",
   "metadata": {
    "id": "7c139c3b"
   },
   "source": [
    "There are two main findings in this table:\n",
    "1. As we expected, the values in the diagonal are smaller than the other values. This makes sense as lower perplexity values mean higher probabilities that are assigend to the given text. In the diagonal, we are using each model on docunents of the author it was trained on, which means it is more likely to assign higher probabilities to the text.\n",
    "\n",
    "2. We can see that the Hamilton model has higher \"sensitivity\" towards documents writen by Hamilton, which means that it assigns more \"extreme\" probabilities (high probabilities to its own documents, and low to others), compared to the Madison model which is more \"centered\" in its values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a7309",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "267a7309"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Now, let's revisit our motivation for parameterizing conditional probabilities using a feedforward neural network instead of through counting.\n",
    "\n",
    "**Question:** Compare the pros and cons of feedforward neural language model and the original $n$-gram language model (possibly with smoothing). Which is better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_nn_v_ngram\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c3d55",
   "metadata": {
    "id": "240c3d55"
   },
   "source": [
    "The FFNL is probably better than the original n-gram model.\n",
    "Pros of FFNL:\n",
    "1. The original n-gram model looks at every word in the contex/target as a completly unique word. This means that words that are relatively semantically close, are still considered as totally different. This means that predictions might get 0 probabilities if the next word is slightly different than the predicted one. On the other hand, an FFNL uses embeddings which allows us to consider similarities between words in a more flexible wide manner.\n",
    "2. as described in the previous lab, the original n-gram with naive implementation uses way too much memory (exponentially growing with N). The FFNL on the other hand is far more efficient in memory usage and mostly depends on the dimensions/layers we chose.\n",
    "\n",
    "pros of the n-gram model:\n",
    "1. It is easier to train as it only relies on direct estimations of probabilities based on the training data. The FFNL model however usually includes more sophisticated complex gradient calculations and repetative (multiple epochs) optimizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0c9b8",
   "metadata": {
    "id": "9dc0c9b8"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# The recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ee4102",
   "metadata": {
    "id": "95ee4102"
   },
   "source": [
    "One limitation of $n$-gram language models (both the original one and the neural one) is that they only model context up to a fixed number of words. However, natural language exhibits long-term dependencies, well beyond $n=5$. In this part of the lab, we consider an approach based on recurrent neural networks (RNN), which can consider variable amounts of context.\n",
    "\n",
    "Different from $n$-gram language modeling, RNN-based language models do not make the approximation that the probability of a word only depends on its previous $n-1$ words. That is, we use the unapproximated chain rule:\n",
    "\n",
    "$$\n",
    "\\Prob(w_1, w_2, \\ldots, w_N) = \\prod_{i=1}^N \\Prob (w_i \\given w_1, \\cdots, w_{i-1})\n",
    "$$\n",
    "\n",
    "and we again specify the conditional probabilities using a neural network:\n",
    "\n",
    "$$\n",
    "\\Pr (w_i \\given w_{\\color{red}1}, \\cdots, w_{i-1})= f({ w_{\\color{red}1}}, \\cdots, w_{i-1}),\n",
    "$$\n",
    "\n",
    "where we use an RNN to parameterize $f$. (Notice the change in the first index of the context, highlighted in red; we're using the whole history as context now, not just the last $n-1$ words.)\n",
    "\n",
    "The inputs to RNNs, like in the feedforward case, are embeddings of words, and we project the _final_ output state of the RNN to a vector of size $V$, followed by a softmax to normalize the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb02949",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "7fb02949"
   },
   "source": [
    "Implement the missing part of the `forward` function of an RNN language model below. This function takes the previous words as input, and returns the probabilities of generating the next word. The returned value should be a dictionary, with word types as keys and their respective probabilities as values.\n",
    "\n",
    "> Hint: You might find [torch.nn.RNN documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) helpful. Make sure that you understand the input and output shapes.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: rnn_forward\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fea1392",
   "metadata": {
    "id": "7fea1392",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:22.288540Z",
     "start_time": "2024-12-19T16:59:22.279239Z"
    }
   },
   "source": [
    "class RNNLM(torch.nn.Module):\n",
    "  def __init__(self, tokenizer, embedding_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.tokenizer = tokenizer\n",
    "    self.vocab = tokenizer.get_vocab()\n",
    "    vocab_size = len(self.vocab)\n",
    "    self.pad_index = self.tokenizer.pad_token_id\n",
    "\n",
    "    # Create modules\n",
    "    self.embed = torch.nn.Embedding(vocab_size, embedding_size)\n",
    "    self.rnn = torch.nn.RNN(input_size=embedding_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=1,\n",
    "                            batch_first = True)\n",
    "    self.hidden2output = torch.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "  def forward(self, history_words):\n",
    "    \"\"\"Computes the distribution over the next word given context `history_words`.\n",
    "    Arguments:\n",
    "      history_words: a list of word strings; could be an empty list when generating\n",
    "                     the first word.\n",
    "    Returns:\n",
    "      the distribution over the next word given the context, stored as a dictionary,\n",
    "      with word types as keys, and probability values as values. The probability of\n",
    "      generating an unknown word is stored in `dictionary[\"[UNK]\"]`\"\"\"\n",
    "    # Switch to \"evaluation\" mode\n",
    "    self.eval()\n",
    "    # Convert strings to word ids\n",
    "    context = self.tokenizer(history_words,\n",
    "                             is_split_into_words=True,\n",
    "                             return_tensors='pt'\n",
    "                            )['input_ids'] \\\n",
    "                  .long() \\\n",
    "                  .to(device) # 1, context_len\n",
    "    context_len = context.size(1)\n",
    "\n",
    "    # For generating the first word, we feed in a special beginning-of-sentence symbol <pad>,\n",
    "    # which is also what we use for padding. In future labs we'll be using <bos>, but as long\n",
    "    # as training and evaluation use the same beginning-of-sentence symbol, it doesn't matter\n",
    "    # which particular symbol we use.\n",
    "    if context_len == 0:\n",
    "      context = context.new(1, 1).fill_(self.pad_index)\n",
    "      context_len = context.size(1)\n",
    "\n",
    "    # Initialize hidden\n",
    "    hidden = None\n",
    "\n",
    "    # TODO: finish feedforward and set logits\n",
    "    # Logits should be a tensor of size (1, vocab_size)\n",
    "    # Note that you should project the `output` from rnn, not the `hidden`,\n",
    "    # using self.hidden2output\n",
    "    # The structure of the network is\n",
    "    #   embeddings -> the output of RNN at the last step -> hidden2output -> softmax\n",
    "    embeddings = self.embed(context)\n",
    "    output, hidden = self.rnn(embeddings, hidden)\n",
    "    logits = self.hidden2output(hidden)\n",
    "\n",
    "    # Normalize to get probabilities\n",
    "    probs = torch.softmax(logits, -1).view(-1) # vocab_size\n",
    "\n",
    "    # Match probabilities with actual word types\n",
    "    distribution = {}\n",
    "    for i, prob in enumerate(probs):\n",
    "      word = self.tokenizer.decode(i, clean_up_tokenization_spaces=True)\n",
    "      distribution[word] = prob.item()\n",
    "    return distribution"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "0ad84881",
   "metadata": {
    "id": "0ad84881"
   },
   "source": [
    "Now, let's load the pretrained RNN language models for Hamilton and Madison. The model `rnn_lm_madison` was trained on documents authored by Madison, whereas `rnn_lm_hamilton` was trained on documents authored by Hamilton."
   ]
  },
  {
   "cell_type": "code",
   "id": "e11dc692",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e11dc692",
    "outputId": "33443ace-2199-49bc-d08e-b819053cd3ec",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:22.328887Z",
     "start_time": "2024-12-19T16:59:22.308436Z"
    }
   },
   "source": [
    "# Create and load RNN LM for Madison\n",
    "rnn_lm_madison = RNNLM(hf_tokenizer,\n",
    "               embedding_size=128,\n",
    "               hidden_size=128,\n",
    "               ).to(device)\n",
    "rnn_lm_madison.load_state_dict(torch.load('data/rnn_lm_m.pt', map_location=device))\n",
    "\n",
    "# Create and load feedforward LM for Hamilton\n",
    "rnn_lm_hamilton = RNNLM(hf_tokenizer,\n",
    "               embedding_size=128,\n",
    "               hidden_size=128,\n",
    "               ).to(device)\n",
    "rnn_lm_hamilton.load_state_dict(torch.load('data/rnn_lm_h.pt', map_location=device))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "757971e1",
   "metadata": {
    "id": "757971e1"
   },
   "source": [
    "## Sampling from an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e0cc7",
   "metadata": {
    "id": "dc7e0cc7"
   },
   "source": [
    "Let's try to sample from our models. The samples might be bad since the dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "id": "aeee2408",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aeee2408",
    "outputId": "35b3a122-6b7f-43a7-9e17-9ec929ec11c6",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:27.131172Z",
     "start_time": "2024-12-19T16:59:22.348284Z"
    }
   },
   "source": [
    "print(' '.join(sample_sequence(rnn_lm_madison, ('constitution', 'proposed', 'by', 'the'))), \"\\n\")\n",
    "print(' '.join(sample_sequence(rnn_lm_hamilton, ('constitution', 'proposed', 'by', 'the'))))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constitution proposed by the united states , which will holding that a representative class , therefore , than the business could be , more weight , not more much [UNK] in his liberty and interests , are too little known , on ? being few correct [UNK] for any other branch on time towards [UNK] , a degree of citizens , will be stood , however , in the latter of the state of men relied should be [UNK] . it declares still one of the policy of man motives to assume their common cases should far the judges , by a disposition , \n",
      "\n",
      "constitution proposed by the united states , which will have no beneficial prove as [UNK] the exigencies of the confederacy , and , in various act increase in the bodies of independent jealousy in the forms of this kind has a long , and it is , that as often as well it would be certainty , can proposition , are under authorize their [UNK] . if it were marked retained in the state governments than to encroach . for relations [UNK] innovations the less proposed other government , that there are this boundary , to stated temptation he admonish for a direct [UNK]\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "id": "878ebd5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "878ebd5c",
    "outputId": "ac537eaa-9b7b-4c20-c739-aca67d5abf0d",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:32.506327Z",
     "start_time": "2024-12-19T16:59:27.160454Z"
    }
   },
   "source": [
    "grader.check(\"rnn_sample\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "5a1977b6",
   "metadata": {
    "id": "5a1977b6"
   },
   "source": [
    "## Evaluating text according to an RNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1418535",
   "metadata": {
    "id": "d1418535"
   },
   "source": [
    "Again, let's evaluate the models on a document from Hamilton and an artitle from Madison. We'll select the first Madison document and the fisrt Hamilton document in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "id": "a3b9c078",
   "metadata": {
    "id": "a3b9c078",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:32.519627Z",
     "start_time": "2024-12-19T16:59:32.517223Z"
    }
   },
   "source": [
    "document_madison = validation_madison[0]['tokens']\n",
    "document_hamilton = validation_hamilton[0]['tokens']"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "d8546b2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "d8546b2d"
   },
   "source": [
    "Calculate the perplexity of each RNN model on each document.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: rnn_ppl\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "05fe491d",
   "metadata": {
    "id": "05fe491d",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:51.894314Z",
     "start_time": "2024-12-19T16:59:32.553388Z"
    }
   },
   "source": [
    "# TODO\n",
    "rnn_ppl_madison_model_madison_document = perplexity(document_madison, rnn_lm_madison)\n",
    "rnn_ppl_hamilton_model_madison_document = perplexity(document_madison, rnn_lm_hamilton)\n",
    "rnn_ppl_madison_model_hamilton_document = perplexity(document_hamilton, rnn_lm_madison)\n",
    "rnn_ppl_hamilton_model_hamilton_document = perplexity(document_hamilton, rnn_lm_hamilton)"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "id": "61b72ac3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "61b72ac3",
    "outputId": "44645904-c6a0-493f-ceaa-07409b5d6b67",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:51.907891Z",
     "start_time": "2024-12-19T16:59:51.900173Z"
    }
   },
   "source": [
    "grader.check(\"rnn_ppl\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "d02ae79b",
   "metadata": {
    "id": "d02ae79b"
   },
   "source": [
    "Now, let's compare those perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f73d7f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f73d7f8",
    "outputId": "2c4c72e1-f782-4d22-b3c9-be8dcba9f746",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:51.932165Z",
     "start_time": "2024-12-19T16:59:51.929299Z"
    }
   },
   "source": [
    "print (f\"Author      Madison Model        Hamilton Model\\n\"\n",
    "       f\"Madison        {rnn_ppl_madison_model_madison_document:5.1f}                {rnn_ppl_hamilton_model_madison_document:5.1f}\\n\"\n",
    "       f\"Hamilton       {rnn_ppl_madison_model_hamilton_document:5.1f}                {rnn_ppl_hamilton_model_hamilton_document:5.1f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author      Madison Model        Hamilton Model\n",
      "Madison         96.0                118.6\n",
      "Hamilton       100.9                 85.7\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "id": "89c4be5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "89c4be5c"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** Which type of model is better? The RNN language models or the feedforward language models? What are the possible reasons?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_ffnn_vs_rnn\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb04c35",
   "metadata": {
    "id": "4bb04c35"
   },
   "source": [
    "Considering the perplexity metric, we get that the RNN language model is better than the FFLM as it assigns higher probabilities to the given documents.\n",
    "One possible reason for this is the fact that the RNN takes context of any length, where the hidden layer represents the entire context of the word, unlike the FF which takes a limited constant size context (n-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259ea26",
   "metadata": {
    "id": "0259ea26"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# Authorship attribution using language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1128d07d",
   "metadata": {
    "id": "1128d07d"
   },
   "source": [
    "In lab 1-3, you saw how to use a Naive Bayes model to determine authorship:\n",
    "\n",
    "\\begin{align*}\n",
    "\\argmax{i} \\Prob(c_i \\given \\vect{x})\n",
    "&= \\argmax{i} \\frac{\\Prob(\\vect{x} \\given c_i) \\cdot \\Prob(c_i)}{\\Prob(\\vect{x})} \\\\\n",
    "&= \\argmax{i} \\Prob(\\vect{x} \\given c_i) \\cdot \\Prob(c_i)\n",
    "\\end{align*}\n",
    "\n",
    "In this lab, the language models trained on Madison documents can be used to calculate $\\Pr(\\vect{x} \\given \\text{Madison})$, and the language models trained on Hamilton documents can be used to calculate $\\Pr(\\vect{x} \\given \\text{Hamilton})$. Therefore, they can also be used for authorship attribution.\n",
    "\n",
    "Recall that for numerical stability issues, we operate in log space (with base 2). With a little abuse of notation, let's denote the _log posterior_ as\n",
    "\n",
    "$$\n",
    "\\log \\Prob(\\vect{x} \\given c_i) + \\log \\Prob(c_i),\n",
    "$$\n",
    "where the priors $\\Prob(c_i)$ from lab 1-3 are given below."
   ]
  },
  {
   "cell_type": "code",
   "id": "0c757c11",
   "metadata": {
    "id": "0c757c11",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:51.968843Z",
     "start_time": "2024-12-19T16:59:51.966066Z"
    }
   },
   "source": [
    "prior_madison = 15 / (15+51)\n",
    "prior_hamilton = 51 / (15+51)"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "f2cece69",
   "metadata": {
    "id": "f2cece69"
   },
   "source": [
    "Let's consider a document from the test set."
   ]
  },
  {
   "cell_type": "code",
   "id": "127936b5",
   "metadata": {
    "id": "127936b5",
    "ExecuteTime": {
     "end_time": "2024-12-19T16:59:51.993104Z",
     "start_time": "2024-12-19T16:59:51.990123Z"
    }
   },
   "source": [
    "document = testing[0]['tokens']"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "202d6efc",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "202d6efc"
   },
   "source": [
    "Use the feedforward neural language models to calculate the log posteriors for `document`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: ffnn_author\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "b8526816",
   "metadata": {
    "id": "b8526816",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:00.785470Z",
     "start_time": "2024-12-19T16:59:52.007258Z"
    }
   },
   "source": [
    "#TODO - calculate the log posteriors for Madisonn and Hamilton using feedforward LMs\n",
    "log_posterior_madison_ffnn = -neglogprob(document, ffnn_lm_madison) + math.log2(prior_madison)\n",
    "log_posterior_hamilton_ffnn = -neglogprob(document, ffnn_lm_hamilton) + math.log2(prior_hamilton)\n",
    "#TODO - determine authorship\n",
    "author_ffnn = 'Madison' if log_posterior_madison_ffnn > log_posterior_hamilton_ffnn else 'Hamilton'"
   ],
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "id": "90e5262f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "90e5262f",
    "outputId": "276660de-557d-4777-9632-af6164ad5e2e",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:00.806166Z",
     "start_time": "2024-12-19T17:00:00.801953Z"
    }
   },
   "source": [
    "grader.check(\"ffnn_author\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "ecdd6c4b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecdd6c4b",
    "outputId": "4f5a0be8-117f-4682-d610-54957755bd60",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:00.839512Z",
     "start_time": "2024-12-19T17:00:00.834691Z"
    }
   },
   "source": [
    "print (author_ffnn)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamilton\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "id": "984ef5ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "984ef5ee"
   },
   "source": [
    "Use the RNN neural language models to calculate the log posteriors for `document`.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: rnn_author\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "71f22253",
   "metadata": {
    "id": "71f22253",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:10.176675Z",
     "start_time": "2024-12-19T17:00:00.874691Z"
    }
   },
   "source": [
    "#TODO - calculate the log posteriors for Madison and Hamilton using RNN LMs\n",
    "log_posterior_madison_rnn = -neglogprob(document, rnn_lm_madison) + math.log2(prior_madison)\n",
    "log_posterior_hamilton_rnn = -neglogprob(document, rnn_lm_hamilton) + math.log2(prior_hamilton)\n",
    "#TODO - determine authorship\n",
    "author_rnn = 'Madison' if log_posterior_madison_rnn > log_posterior_hamilton_rnn else 'Hamilton'"
   ],
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "e6a36f45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "e6a36f45",
    "outputId": "3155cbc3-e036-43c6-d5ff-9c240c51738a",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:10.207082Z",
     "start_time": "2024-12-19T17:00:10.200115Z"
    }
   },
   "source": [
    "grader.check(\"rnn_author\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "id": "7ade7ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ade7ab7",
    "outputId": "431c1525-7702-4a94-e971-cb642d8962cd",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:00:10.247371Z",
     "start_time": "2024-12-19T17:00:10.241492Z"
    }
   },
   "source": [
    "print (author_rnn)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Madison\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "604c5bfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "604c5bfa"
   },
   "source": [
    "Now, we can use these models to determine authorship on the entire test set. Define the `ffnn_classify` and `rnn_classify` functions, which take a sequence of `tokens` and return either `'Hamilton'` or `'Madison'` depending on which of the two has a higher probability of authoring the text.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: authorship\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "id": "58cbab94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250,
     "referenced_widgets": [
      "18e074654e2046359d924a0bf3c2fd66",
      "6a619643a85f41ccb55953b2c6d46c81",
      "d2b60b16a8f9465f89b2b09b292d1899",
      "1a5ed63fad8e4d77a8a350de68eb3b0b",
      "cc349348069f4bc899dc8dc1d40d17e0",
      "15c329db9f57449081d81ce236016113",
      "92408ced77e044c5a912570371e0d802",
      "87a5ac661d194ed793eb1d8c7f1f0ff7",
      "7139ccaee5934d0a876b7edc6177683c",
      "05728584355a48ee904d45b6f7471be7",
      "c91fd9a264f54e2c8d6fdff005fe16c0"
     ]
    },
    "id": "58cbab94",
    "outputId": "e13c94a3-93ba-4241-a0e2-286a32c5faae",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:03:28.572121Z",
     "start_time": "2024-12-19T17:00:10.281327Z"
    }
   },
   "source": [
    "def ffnn_classify(tokens):\n",
    "    \"\"\"Returns the predicted author according to the FFNN model.\n",
    "    Arguments:\n",
    "      tokens: a list of tokens.\n",
    "    Returns: 'Hamilton' or 'Madison'.\"\"\"\n",
    "    #TODO - implement this method\n",
    "    log_posterior_madison_ffnn = -neglogprob(tokens, ffnn_lm_madison) + math.log2(prior_madison)\n",
    "    log_posterior_hamilton_ffnn = -neglogprob(tokens, ffnn_lm_hamilton) + math.log2(prior_hamilton)\n",
    "    #TODO - determine authorship\n",
    "    author_ffnn = 'Madison' if log_posterior_madison_ffnn > log_posterior_hamilton_ffnn else 'Hamilton'\n",
    "    return author_ffnn\n",
    "\n",
    "def rnn_classify(tokens):\n",
    "    \"\"\"Returns the predicted author according to the RNN model.\n",
    "    Arguments:\n",
    "      tokens: a list of tokens.\n",
    "    Returns: 'Hamilton' or 'Madison'.\"\"\"\n",
    "    #TODO - implement this method\n",
    "    log_posterior_madison_rnn = -neglogprob(tokens, rnn_lm_madison) + math.log2(prior_madison)\n",
    "    log_posterior_hamilton_rnn = -neglogprob(tokens, rnn_lm_hamilton) + math.log2(prior_hamilton)\n",
    "    #TODO - determine authorship\n",
    "    author_rnn = 'Madison' if log_posterior_madison_rnn > log_posterior_hamilton_rnn else 'Hamilton'\n",
    "    return author_rnn\n",
    "\n",
    "for ex in tqdm(testing):\n",
    "    print(f\"{ex['number']:2} {ffnn_classify(ex['tokens']):8} {rnn_classify(ex['tokens']):8}\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d41b5964cd142c7ae48ff42b4530c73"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 Hamilton Madison \n",
      "50 Madison  Madison \n",
      "51 Madison  Madison \n",
      "52 Madison  Madison \n",
      "53 Madison  Madison \n",
      "54 Madison  Madison \n",
      "55 Madison  Madison \n",
      "56 Madison  Madison \n",
      "57 Madison  Madison \n",
      "62 Madison  Madison \n",
      "63 Madison  Madison \n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "id": "b61df793",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 46
    },
    "id": "b61df793",
    "outputId": "8cb7ad0c-f193-4976-c1f6-80c6e2849d34",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:03:46.637960Z",
     "start_time": "2024-12-19T17:03:28.613992Z"
    }
   },
   "source": [
    "grader.check(\"authorship\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    All tests passed!\n",
       "    "
      ],
      "text/html": [
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "id": "12c1fd6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "12c1fd6b"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "**Question:** What would happen if the dataset is imbalanced, that is, if we have much more training data for one author than the other?\n",
    "\n",
    ">Hint: With sufficient data, the model usually gets lower perplexity than with an insufficient amount of data.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_imbalanced\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cef2c0",
   "metadata": {
    "id": "63cef2c0"
   },
   "source": [
    "When training a model with sufficient data (more sentences and more contexts), the model is likely to assign higher probabilities to documents written by the author it was trained on, as it has more confidence in predicting his/her documents. This means that if we train different models, one for each author, while using imbalanced data, we are likely to get more predictions of the author that is more frequent in the training data, as its corresponding model has lower perplexity for it.\n",
    "Additionaly, in the specific method we are using, the imbalances on the training data also affects the prior probabilities which affect the overall posterior probability, which amplifies this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fff33",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "c55fff33"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "# Lab debrief\n",
    "\n",
    "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following, but you're not restricted to these:\n",
    "\n",
    "* Was the lab too long or too short?\n",
    "* Were the readings appropriate for the lab?\n",
    "* Was it clear (at least after you completed the lab) what the points of the exercises were?\n",
    "* Are there additions or changes you think would make the lab better?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: open_response_debrief\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d822e17",
   "metadata": {
    "id": "7d822e17"
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c7fef",
   "metadata": {
    "id": "c57c7fef"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "# End of Lab 2-3 {-}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31268c8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "b31268c8"
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "ef6b4eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "ef6b4eb8",
    "outputId": "595fd116-2e05-44a7-955c-3a69852230a8",
    "ExecuteTime": {
     "end_time": "2024-12-19T17:04:13.546534Z",
     "start_time": "2024-12-19T17:03:46.649738Z"
    }
   },
   "source": [
    "grader.check_all()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "authorship:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ffnn_author:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ffnn_neglogprob:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ffnn_ppl:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "ffnn_sample:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "rnn_author:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "rnn_ppl:\n",
       "\n",
       "    All tests passed!\n",
       "    \n",
       "\n",
       "rnn_sample:\n",
       "\n",
       "    All tests passed!\n",
       "    \n"
      ],
      "text/html": [
       "<p><strong>authorship:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ffnn_author:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ffnn_neglogprob:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ffnn_ppl:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>ffnn_sample:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>rnn_author:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>rnn_ppl:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n",
       "<p><strong>rnn_sample:</strong></p>\n",
       "\n",
       "    \n",
       "    \n",
       "        <p>All tests passed!</p>\n",
       "    \n",
       "    \n",
       "\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 41
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "title": "Course 236299 Lab 2-3: Language modeling with neural networks",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "18e074654e2046359d924a0bf3c2fd66": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6a619643a85f41ccb55953b2c6d46c81",
       "IPY_MODEL_d2b60b16a8f9465f89b2b09b292d1899",
       "IPY_MODEL_1a5ed63fad8e4d77a8a350de68eb3b0b"
      ],
      "layout": "IPY_MODEL_cc349348069f4bc899dc8dc1d40d17e0"
     }
    },
    "6a619643a85f41ccb55953b2c6d46c81": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c329db9f57449081d81ce236016113",
      "placeholder": "​",
      "style": "IPY_MODEL_92408ced77e044c5a912570371e0d802",
      "value": "100%"
     }
    },
    "d2b60b16a8f9465f89b2b09b292d1899": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87a5ac661d194ed793eb1d8c7f1f0ff7",
      "max": 11,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7139ccaee5934d0a876b7edc6177683c",
      "value": 11
     }
    },
    "1a5ed63fad8e4d77a8a350de68eb3b0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05728584355a48ee904d45b6f7471be7",
      "placeholder": "​",
      "style": "IPY_MODEL_c91fd9a264f54e2c8d6fdff005fe16c0",
      "value": " 11/11 [11:19&lt;00:00, 61.25s/it]"
     }
    },
    "cc349348069f4bc899dc8dc1d40d17e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15c329db9f57449081d81ce236016113": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "92408ced77e044c5a912570371e0d802": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "87a5ac661d194ed793eb1d8c7f1f0ff7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7139ccaee5934d0a876b7edc6177683c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "05728584355a48ee904d45b6f7471be7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c91fd9a264f54e2c8d6fdff005fe16c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
