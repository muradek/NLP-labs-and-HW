{"cells":[{"cell_type":"code","execution_count":1,"id":"853aae26","metadata":{"deletable":false,"editable":false,"execution":{"iopub.execute_input":"2024-11-17T20:04:37.253783Z","iopub.status.busy":"2024-11-17T20:04:37.253454Z","iopub.status.idle":"2024-11-17T20:04:38.524767Z","shell.execute_reply":"2024-11-17T20:04:38.523757Z"},"jupyter":{"outputs_hidden":true,"source_hidden":true},"colab":{"base_uri":"https://localhost:8080/"},"id":"853aae26","executionInfo":{"status":"ok","timestamp":1732210330469,"user_tz":-120,"elapsed":4399,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"2903736f-2a20-4aa7-bfbf-63d82557316a"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"]}],"source":["# Please do not change this cell because some hidden tests might depend on it.\n","import os\n","\n","# Otter grader does not handle ! commands well, so we define and use our\n","# own function to execute shell commands.\n","def shell(commands, warn=True):\n","    \"\"\"Executes the string `commands` as a sequence of shell commands.\n","\n","       Prints the result to stdout and returns the exit status.\n","       Provides a printed warning on non-zero exit status unless `warn`\n","       flag is unset.\n","    \"\"\"\n","    file = os.popen(commands)\n","    print (file.read().rstrip('\\n'))\n","    exit_status = file.close()\n","    if warn and exit_status != None:\n","        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n","    return exit_status\n","\n","shell(\"\"\"\n","ls requirements.txt >/dev/null 2>&1\n","if [ ! $? = 0 ]; then\n"," rm -rf .tmp\n"," git clone https://github.com/cs236299-2024-winter/lab1-1.git .tmp\n"," mv .tmp/tests ./\n"," mv .tmp/requirements.txt ./\n"," rm -rf .tmp\n","fi\n","pip install -q -r requirements.txt\n","\"\"\")"]},{"cell_type":"code","execution_count":2,"id":"822d6c6e","metadata":{"deletable":false,"editable":false,"id":"822d6c6e","executionInfo":{"status":"ok","timestamp":1732210331900,"user_tz":-120,"elapsed":1436,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["# Initialize Otter\n","import otter\n","grader = otter.Notebook()"]},{"cell_type":"markdown","id":"a70ca4c4","metadata":{"tags":["remove_for_latex"],"id":"a70ca4c4"},"source":["# Course 236299\n","## Lab 1-1 â€“ Types, tokens, and representing text"]},{"cell_type":"code","execution_count":3,"id":"74c4d93a","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:38.528423Z","iopub.status.busy":"2024-11-17T20:04:38.527851Z","iopub.status.idle":"2024-11-17T20:04:40.300579Z","shell.execute_reply":"2024-11-17T20:04:40.299774Z"},"id":"74c4d93a","executionInfo":{"status":"ok","timestamp":1732210349505,"user_tz":-120,"elapsed":17609,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["import math\n","import re\n","import sys\n","\n","import torch\n","import nltk"]},{"cell_type":"code","execution_count":4,"id":"50b92761","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.304164Z","iopub.status.busy":"2024-11-17T20:04:40.303642Z","iopub.status.idle":"2024-11-17T20:04:40.676393Z","shell.execute_reply":"2024-11-17T20:04:40.675423Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"50b92761","executionInfo":{"status":"ok","timestamp":1732210350613,"user_tz":-120,"elapsed":1112,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"ee2bcae9-03bf-49a0-8a4a-7af3caf17fa0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":4}],"source":["nltk.download('punkt', quiet=True) # this module is used to tokenize the text"]},{"cell_type":"markdown","id":"4f337f41","metadata":{"id":"4f337f41"},"source":["Where we're headed: Nearest neighbor text classification works by classifying a novel text with the same class as that of the training text that is closest according to some distance metric. These metrics are calculated based on representations of the texts. In this lab, we'll introduce some different representations and you'll use nearest neighbor classification to predict the speaker of sentences selected from a children's book.\n","    \n","The objectives of this lab are to:\n","\n","* Clarify terminology around words and texts,\n","* Manipulate different representations of words and texts,\n","* Apply these representations to calculate text similarity, and\n","* Classify documents by a simple nearest neighbor model.\n","   \n","Recall that in this and all labs, we will have you carry out several exercises in notebook cells. The cells you are to do are marked '`#TODO`'. They will typically have a `...` where your code or answer should go. Where specified, you needn't write code to calculate the answer, but instead, simply work out the answer yourself and enter it."]},{"cell_type":"markdown","id":"3fe29a70","metadata":{"id":"3fe29a70"},"source":["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n","\n","* `math.acos`\n","* `math.pi`\n","* `re.match`\n","* `set`\n","* `sorted`\n","* `str.join`\n","* `str.lower`\n","* `torch.amax`\n","* `torch.dot`\n","* `torch.linalg.norm`\n","* `torch.maximum`\n","* `torch.minimum`\n","* `torch.stack`\n","* `torch.sum`\n","* `torch.Tensor.type`\n","* `torch.where`\n","* `torch.zeros`\n","* `torch.zeros_like`\n"]},{"cell_type":"markdown","id":"f8c7327a","metadata":{"id":"f8c7327a"},"source":["# Counting words\n","\n","<img src=\"https://github.com/nlp-236299/data/blob/master/Seuss/seuss%20-%201966%20-%20green%20eggs%20and%20ham.gif?raw=true\" width=150 align=right />\n","\n","Here are five sentences from Dr. Seuss's [_Green Eggs and Ham_](https://en.wikipedia.org/wiki/Green_Eggs_and_Ham):\n","\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","\n","Let's make this text available in the variable `text`."]},{"cell_type":"code","execution_count":5,"id":"5fc6f0a9","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.679974Z","iopub.status.busy":"2024-11-17T20:04:40.679414Z","iopub.status.idle":"2024-11-17T20:04:40.683081Z","shell.execute_reply":"2024-11-17T20:04:40.682273Z"},"id":"5fc6f0a9","executionInfo":{"status":"ok","timestamp":1732210350613,"user_tz":-120,"elapsed":31,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["text = \"\"\"\n","    Would you like them here or there?\n","    I would not like them here or there.\n","    I would not like them anywhere.\n","    I do not like green eggs and ham.\n","    I do not like them, Sam-I-am.\n","    \"\"\""]},{"cell_type":"markdown","id":"9a497459","metadata":{"deletable":false,"editable":false,"id":"9a497459"},"source":["A Python string like this is, of course, a sequence of characters. But we think of this text as a sequence of sentences each composed of a sequence of words. How many words are there in this text? That is a fraught question, for several reasons, including\n","\n","* The type-token distinction\n","* Tokenization issues\n","* Normalization\n","\n","## Types versus tokens\n","\n","In determining the number of words in `text`, are we talking about word _types_ or word _tokens_. (For instance, there are five _tokens_ of the word _type_ 'like' in the example `text`.)\n","\n","How many word tokens are there in total in this text? (Just count them manually.) Assign the number to the variable `token_count` in the next cell.\n","<!--\n","BEGIN QUESTION\n","name: token_count\n","-->"]},{"cell_type":"code","execution_count":6,"id":"bed7bf0e","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.686244Z","iopub.status.busy":"2024-11-17T20:04:40.685656Z","iopub.status.idle":"2024-11-17T20:04:40.689236Z","shell.execute_reply":"2024-11-17T20:04:40.688418Z"},"id":"bed7bf0e","executionInfo":{"status":"ok","timestamp":1732210350613,"user_tz":-120,"elapsed":30,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - define `token_count` to be the number of tokens in `text`\n","token_count = 35"]},{"cell_type":"code","execution_count":7,"id":"4339174e","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"4339174e","executionInfo":{"status":"ok","timestamp":1732210350614,"user_tz":-120,"elapsed":31,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"be27c12c-d686-4d47-9a78-a561207d30b8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":7}],"source":["grader.check(\"token_count\")"]},{"cell_type":"markdown","id":"d4f61130","metadata":{"deletable":false,"editable":false,"id":"d4f61130"},"source":["How many word types are there? (Again, you can just count manually.)\n","<!--\n","BEGIN QUESTION\n","name: type_count\n","-->"]},{"cell_type":"code","execution_count":8,"id":"8a72ea17","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.707974Z","iopub.status.busy":"2024-11-17T20:04:40.707767Z","iopub.status.idle":"2024-11-17T20:04:40.710960Z","shell.execute_reply":"2024-11-17T20:04:40.710147Z"},"id":"8a72ea17","executionInfo":{"status":"ok","timestamp":1732210350614,"user_tz":-120,"elapsed":29,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - define `type_count` to be the number of types in `text`\n","type_count = 21"]},{"cell_type":"code","execution_count":9,"id":"54395200","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"54395200","executionInfo":{"status":"ok","timestamp":1732210350614,"user_tz":-120,"elapsed":29,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"63dd3aae-e59b-4687-b95b-e77728141ec0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":9}],"source":["grader.check(\"type_count\")"]},{"cell_type":"markdown","id":"57f75fee","metadata":{"deletable":false,"editable":false,"id":"57f75fee"},"source":["<!-- BEGIN QUESTION -->\n","\n","**Question:** The set of types of a language is referred to as its _vocabulary_. Are there more types or tokens as you calculated above? Could it be otherwise?\n","<!--\n","BEGIN QUESTION\n","name: type_vs_token_count\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"28a5d1f3","metadata":{"id":"28a5d1f3"},"source":["Tokens are instances of word types, which means that there is at least one token per each type, hence the number of tokens is always equal or greater than the number of type."]},{"cell_type":"markdown","id":"9ac81807","metadata":{"id":"9ac81807"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","## Tokenization"]},{"cell_type":"markdown","id":"f6dfd169","metadata":{"id":"f6dfd169"},"source":["Did you count 'there?' as one token or two? This raises the issue of _tokenization_ of text, how to decide where the token boundaries occur. For instance, here's a simple way to split a string â€“ to _tokenize_ it â€“ in Python by splitting at whitespace."]},{"cell_type":"code","execution_count":10,"id":"aaa42f76","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.736383Z","iopub.status.busy":"2024-11-17T20:04:40.736155Z","iopub.status.idle":"2024-11-17T20:04:40.739724Z","shell.execute_reply":"2024-11-17T20:04:40.738870Z"},"id":"aaa42f76","executionInfo":{"status":"ok","timestamp":1732210350614,"user_tz":-120,"elapsed":27,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["def whitespace_tokenize(str):\n","    return str.split()"]},{"cell_type":"markdown","id":"0ba40880","metadata":{"deletable":false,"editable":false,"id":"0ba40880"},"source":["Try it out on the `text` defined above.\n","<!--\n","BEGIN QUESTION\n","name: tokens_whitespace\n","-->"]},{"cell_type":"code","execution_count":11,"id":"3a637ff7","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.742659Z","iopub.status.busy":"2024-11-17T20:04:40.742449Z","iopub.status.idle":"2024-11-17T20:04:40.745857Z","shell.execute_reply":"2024-11-17T20:04:40.745290Z"},"id":"3a637ff7","executionInfo":{"status":"ok","timestamp":1732210350614,"user_tz":-120,"elapsed":26,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - define `tokens` to be the tokens as defined by the `whitespace_tokenize` function\n","tokens = whitespace_tokenize(text)"]},{"cell_type":"code","execution_count":12,"id":"dc210441","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"dc210441","executionInfo":{"status":"ok","timestamp":1732210350615,"user_tz":-120,"elapsed":27,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"04cf72cd-5659-4bc1-d90c-9f243be1f417"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":12}],"source":["grader.check(\"tokens_whitespace\")"]},{"cell_type":"markdown","id":"1d726058","metadata":{"deletable":false,"editable":false,"id":"1d726058"},"source":["Using this tokenization method, count the number of tokens in the text, this time using Python to do the work.\n","<!--\n","BEGIN QUESTION\n","name: token_count_whitespace\n","-->"]},{"cell_type":"code","execution_count":13,"id":"96006969","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.763138Z","iopub.status.busy":"2024-11-17T20:04:40.762921Z","iopub.status.idle":"2024-11-17T20:04:40.766273Z","shell.execute_reply":"2024-11-17T20:04:40.765414Z"},"id":"96006969","executionInfo":{"status":"ok","timestamp":1732210350615,"user_tz":-120,"elapsed":24,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - place your token count here\n","token_count_2 = len(tokens)"]},{"cell_type":"code","execution_count":14,"id":"5ed1d524","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"5ed1d524","executionInfo":{"status":"ok","timestamp":1732210350615,"user_tz":-120,"elapsed":23,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"987fcb3c-823a-4727-b8df-d533d5243e39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":14}],"source":["grader.check(\"token_count_whitespace\")"]},{"cell_type":"markdown","id":"b4aa65a3","metadata":{"id":"b4aa65a3"},"source":["Arguably, we _should_ split off punctuation as separate tokens, but even there, some care must be taken. We don't want to split 'don't' into three tokens or 'Sam-I-am' into five. (There's a good argument to be made however that the string 'don't' should be construed as two tokens, namely, 'do' and 'n't', but that's beyond the scope of today's discussion.)\n","\n","Here, we provide an alternative tokenizer that splits tokens at whitespace and splits off punctuation at the beginning and end of non-whitespace regions as separate tokens as well. It makes use of [the Python `re` module](https://docs.python.org/3/library/re.html) for regular expressions to specify the splitting process. Look over the code and make sure you understand what's going on. You might find [this online tool](https://regexr.com/) useful."]},{"cell_type":"code","execution_count":15,"id":"4c1c3739","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.783783Z","iopub.status.busy":"2024-11-17T20:04:40.783589Z","iopub.status.idle":"2024-11-17T20:04:40.786965Z","shell.execute_reply":"2024-11-17T20:04:40.786344Z"},"id":"4c1c3739","executionInfo":{"status":"ok","timestamp":1732210350615,"user_tz":-120,"elapsed":22,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["def punc_tokenize(str):\n","    return [tok for tok in re.split('(\\W*?)\\s+', str) if tok != '']"]},{"cell_type":"markdown","id":"b4e327e9","metadata":{"deletable":false,"editable":false,"id":"b4e327e9"},"source":["Now how many tokens are there in the text if tokenized in this way?\n","<!--\n","BEGIN QUESTION\n","name: token_count_punc\n","-->"]},{"cell_type":"code","execution_count":16,"id":"f035efcd","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.789710Z","iopub.status.busy":"2024-11-17T20:04:40.789518Z","iopub.status.idle":"2024-11-17T20:04:40.792901Z","shell.execute_reply":"2024-11-17T20:04:40.792061Z"},"id":"f035efcd","executionInfo":{"status":"ok","timestamp":1732210350615,"user_tz":-120,"elapsed":22,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","token_count_3 = len(punc_tokenize(text))"]},{"cell_type":"code","execution_count":17,"id":"ed0b1856","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"ed0b1856","executionInfo":{"status":"ok","timestamp":1732210350616,"user_tz":-120,"elapsed":23,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"3542b9c3-01d5-4183-f94c-c204d4aad04f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":17}],"source":["grader.check(\"token_count_punc\")"]},{"cell_type":"markdown","id":"bc5a9174","metadata":{"deletable":false,"editable":false,"id":"bc5a9174"},"source":["## Normalization\n","\n","This tokenization method counts 'Would' and 'would' (capitalized and uncapitalized) as separate types. Is that a good idea? This raises the issue of text _normalization_.\n","\n","Define a function `normalize_token` that normalizes tokens by making them lowercase if at most the first character is uppercase. (Hints [here](https://docs.python.org/3/library/stdtypes.html#str.lower) and [here](https://docs.python.org/3/library/re.html#re.match). These are also listed in the hint cell at the top of the lab, so we'll mostly stop providing these hints from here on.)\n","<!--\n","BEGIN QUESTION\n","name: normalize_token\n","-->"]},{"cell_type":"code","execution_count":18,"id":"b248590a","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.810677Z","iopub.status.busy":"2024-11-17T20:04:40.810444Z","iopub.status.idle":"2024-11-17T20:04:40.814153Z","shell.execute_reply":"2024-11-17T20:04:40.813515Z"},"id":"b248590a","executionInfo":{"status":"ok","timestamp":1732210350616,"user_tz":-120,"elapsed":22,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - implement normalize_token, which returns the normalized word for a single word `str`\n","def normalize_token(str):\n","    if re.match(r'^[A-Z]', str):\n","        return str.lower()\n","    else:\n","        return str"]},{"cell_type":"code","execution_count":19,"id":"4e95f110","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"4e95f110","executionInfo":{"status":"ok","timestamp":1732210350616,"user_tz":-120,"elapsed":22,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"c89ff39b-d5af-4053-9df9-053f8ecf114b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":19}],"source":["grader.check(\"normalize_token\")"]},{"cell_type":"markdown","id":"c94bd934","metadata":{"deletable":false,"editable":false,"id":"c94bd934"},"source":["Now define `text_tokenized` to be the sequence of normalized tokens as tokenized by `punc_tokenize`\n","<!--\n","BEGIN QUESTION\n","name: text_tokenized\n","-->"]},{"cell_type":"code","execution_count":20,"id":"259ece95","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.831692Z","iopub.status.busy":"2024-11-17T20:04:40.831500Z","iopub.status.idle":"2024-11-17T20:04:40.834685Z","shell.execute_reply":"2024-11-17T20:04:40.834030Z"},"id":"259ece95","executionInfo":{"status":"ok","timestamp":1732210350616,"user_tz":-120,"elapsed":21,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","text_tokenized = [normalize_token(token) for token in punc_tokenize(text)]"]},{"cell_type":"code","execution_count":21,"id":"120e26f2","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"120e26f2","executionInfo":{"status":"ok","timestamp":1732210350616,"user_tz":-120,"elapsed":21,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"1d2b3b00-c929-4392-c74a-3dfb2a84e332"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":21}],"source":["grader.check(\"text_tokenized\")"]},{"cell_type":"markdown","id":"cb8e4620","metadata":{"deletable":false,"editable":false,"id":"cb8e4620"},"source":["How many types are there when tokenized and normalized in this way?\n","<!--\n","BEGIN QUESTION\n","name: type_count_norm_punc\n","-->"]},{"cell_type":"code","execution_count":22,"id":"7923065f","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.853213Z","iopub.status.busy":"2024-11-17T20:04:40.852521Z","iopub.status.idle":"2024-11-17T20:04:40.856535Z","shell.execute_reply":"2024-11-17T20:04:40.855825Z"},"id":"7923065f","executionInfo":{"status":"ok","timestamp":1732210350892,"user_tz":-120,"elapsed":296,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","type_count_norm_punc = len(set(text_tokenized))"]},{"cell_type":"code","execution_count":23,"id":"d220aad1","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"d220aad1","executionInfo":{"status":"ok","timestamp":1732210350892,"user_tz":-120,"elapsed":29,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"8b3879b4-e753-4400-e778-1b682c8ee80b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":23}],"source":["grader.check(\"type_count_norm_punc\")"]},{"cell_type":"markdown","id":"7588be5f","metadata":{"id":"7588be5f"},"source":["## Using prebuilt tokenizers\n","\n","Tokenization and normalization are so commonly needed that many packages provide pre-built tokenizers of various sorts. We'll use one from the [Natural Language Tool Kit (NLTK)](http://nltk.org). The package has already been imported above under the name `nltk`."]},{"cell_type":"markdown","id":"a3fde4eb","metadata":{"deletable":false,"editable":false,"id":"a3fde4eb"},"source":["Define two tokenizers, versions of `whitespace_tokenize` and a normalized version of `punc_tokenize` above, using [the `nltk.tokenize.WhitespaceTokenizer`](https://www.nltk.org/api/nltk.tokenize.regexp.html#nltk.tokenize.regexp.WhitespaceTokenizer) and [`nltk.tokenize.word_tokenize`](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize) respectively.\n","Note that `nltk.tokenize.word_tokenize` only tokenizes the string, so we normalize the string by lowering the string's characters.\n","<!--\n","BEGIN QUESTION\n","name: nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\n","-->"]},{"cell_type":"code","execution_count":24,"id":"e9ffd035","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.874991Z","iopub.status.busy":"2024-11-17T20:04:40.874303Z","iopub.status.idle":"2024-11-17T20:04:40.878572Z","shell.execute_reply":"2024-11-17T20:04:40.878048Z"},"id":"e9ffd035","executionInfo":{"status":"ok","timestamp":1732210350892,"user_tz":-120,"elapsed":27,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["from nltk.tokenize import word_tokenize\n","from nltk.tokenize import WhitespaceTokenizer\n","\n","def nltk_whitespace_tokenize(str):\n","    return nltk.tokenize.WhitespaceTokenizer().tokenize(str)\n","\n","def nltk_normpunc_tokenize(str):\n","  return word_tokenize(str.lower())"]},{"cell_type":"code","execution_count":25,"id":"3616cb21","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"3616cb21","executionInfo":{"status":"ok","timestamp":1732210350892,"user_tz":-120,"elapsed":26,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"dd56fd9d-3781-491d-e472-892516abeada"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":25}],"source":["grader.check(\"nltk_whitespace_tokenize_and_nltk_normpunc_tokenize\")"]},{"cell_type":"markdown","id":"fba455b0","metadata":{"id":"fba455b0"},"source":["We'll print out the last few tokens of the `text` tokenized by the whitespace tokenizer and the NLTK tokenizer to see the differences."]},{"cell_type":"code","execution_count":26,"id":"0d30df65","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.904105Z","iopub.status.busy":"2024-11-17T20:04:40.903672Z","iopub.status.idle":"2024-11-17T20:04:40.908598Z","shell.execute_reply":"2024-11-17T20:04:40.907692Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"0d30df65","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":26,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"9bcfa42c-cc05-42f1-9127-e25200dd18f5"},"outputs":[{"output_type":"stream","name":"stdout","text":["['green', 'eggs', 'and', 'ham.', 'I', 'do', 'not', 'like', 'them,', 'Sam-I-am.']\n","['ham', '.', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am', '.']\n"]}],"source":["print(nltk_whitespace_tokenize(text)[-10:])\n","print(nltk_normpunc_tokenize(text)[-10:])"]},{"cell_type":"markdown","id":"e020c72e","metadata":{"id":"e020c72e"},"source":["> _Meta-comment:_ Because it's important that you get practice both with implementing the ideas in the course from first principles and also with using prebuilt software that provides similar functionality, we'll often have you engage in this seemingly redundant process of first implementing a small example and then applying a prebuilt method to do much the same thing. The effort may be duplicative, but it is not wasted."]},{"cell_type":"markdown","id":"5c68361b","metadata":{"deletable":false,"editable":false,"id":"5c68361b"},"source":["In the next section, it will be helpful to have the tokenized text available. Define `nltk_text_tokenized` to be the sequence of normalized tokens of the sample `text` as tokenized by the tokenizer `nltk_normpunc_tokenize` that you've just written.\n","<!--\n","BEGIN QUESTION\n","name: nltk_text_tokenized\n","-->"]},{"cell_type":"code","execution_count":27,"id":"2da5ff32","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.911505Z","iopub.status.busy":"2024-11-17T20:04:40.911290Z","iopub.status.idle":"2024-11-17T20:04:40.915000Z","shell.execute_reply":"2024-11-17T20:04:40.914364Z"},"id":"2da5ff32","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":22,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","nltk_text_tokenized = nltk_normpunc_tokenize(text)"]},{"cell_type":"code","execution_count":28,"id":"c00d93b4","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"c00d93b4","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":21,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"06057eee-58c9-451c-d0c9-e48f81eb7a09"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":28}],"source":["grader.check(\"nltk_text_tokenized\")"]},{"cell_type":"markdown","id":"09b40526","metadata":{"id":"09b40526"},"source":["> _Meta-comment:_ Chooing which tokenization method to use is an important (and unsolved!) question even in the age of large language models (LLMs). For more information, see [\"Tokenizer Choice For LLM Training: Negligible or Crucial?\"](https://arxiv.org/pdf/2310.08754)."]},{"cell_type":"markdown","id":"82fd746c","metadata":{"id":"82fd746c"},"source":["# Representing words\n","\n","In this section, we'll explore some simple representations for tokens, as a step on the way to representing texts â€“ sentences or documents:\n","\n","## String encoding\n","We've already seen string encoding above, representing a token of a word type by a string specific to that type: a token 'green' represented by an instance of the Python string `'green'`, for instance, or 'Sam-I-am' represented by `'Sam-I-am'`. So let's move on.\n","\n","## 1-hot encoding\n","Given a vocabulary for a language, we can associate each type with an integer, say by its index in a vector. We've already imported the `torch` module; we'll use `torch` tensors for the index vector. For the Seuss text, we can use the following list named `vocabulary` to represent the ordered vocabulary:"]},{"cell_type":"code","execution_count":29,"id":"fc9f4155","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.933307Z","iopub.status.busy":"2024-11-17T20:04:40.933096Z","iopub.status.idle":"2024-11-17T20:04:40.938717Z","shell.execute_reply":"2024-11-17T20:04:40.937763Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"fc9f4155","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":19,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"23d0e2fb-de2a-4778-d585-f01a1d63efb0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[',',\n"," '.',\n"," '?',\n"," 'and',\n"," 'anywhere',\n"," 'do',\n"," 'eggs',\n"," 'green',\n"," 'ham',\n"," 'here',\n"," 'i',\n"," 'like',\n"," 'not',\n"," 'or',\n"," 'sam-i-am',\n"," 'them',\n"," 'there',\n"," 'would',\n"," 'you']"]},"metadata":{},"execution_count":29}],"source":["vocabulary = sorted(set(nltk_text_tokenized))\n","vocabulary"]},{"cell_type":"markdown","id":"3632cbe7","metadata":{"id":"3632cbe7"},"source":["### A digression on `torch` tensors\n","\n","Recall that `torch` tensors allow for vectorized computations: many operations on them work [*componentwise*](https://en.wikipedia.org/wiki/Pointwise#Componentwise_operations), that is, separately for each component of the tensor, rather than on the tensor all at once. Compare the following two operations, first on lists, then on `torch` tensors."]},{"cell_type":"code","execution_count":30,"id":"5cc78d86","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.941783Z","iopub.status.busy":"2024-11-17T20:04:40.941352Z","iopub.status.idle":"2024-11-17T20:04:40.946325Z","shell.execute_reply":"2024-11-17T20:04:40.945746Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"5cc78d86","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":16,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"1e719fbb-17a5-4975-b589-83a81479f290"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":30}],"source":["[1, 2] == 1"]},{"cell_type":"code","execution_count":31,"id":"fcd07c50","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.949144Z","iopub.status.busy":"2024-11-17T20:04:40.948934Z","iopub.status.idle":"2024-11-17T20:04:40.955295Z","shell.execute_reply":"2024-11-17T20:04:40.954721Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"fcd07c50","executionInfo":{"status":"ok","timestamp":1732210350893,"user_tz":-120,"elapsed":12,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"9a7001e4-fcd2-4e74-dd53-cc684c6c9bf6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ True, False])"]},"metadata":{},"execution_count":31}],"source":["torch.tensor([1, 2]) == 1"]},{"cell_type":"markdown","id":"a33f2c4f","metadata":{"id":"a33f2c4f"},"source":["This behavior of tensors is quite powerful, allowing for simply specifying complex operations and for efficient, even parallelizable, computation of them. You'll want ot take advantage of these characteristics of tensors where possible, here and in future assignments.\n","\n","But back to the 1-hot representation."]},{"cell_type":"markdown","id":"971eacc9","metadata":{"id":"971eacc9"},"source":["In the _1-hot representation_ of words, a token is then represented by a bit vector (again given as a `torch` tensor), with a 1 at the index of the token's type. (For consistency with some later `torch` functions, we'll take the elements to be floats rather than ints. The `.type` method is useful for converting the type, and it even conveniently broadcasts componentwise over the tensors.) For instance, the 1-hot representation of the comma token ',' would be"]},{"cell_type":"code","execution_count":32,"id":"e687630f","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.958229Z","iopub.status.busy":"2024-11-17T20:04:40.958018Z","iopub.status.idle":"2024-11-17T20:04:40.964338Z","shell.execute_reply":"2024-11-17T20:04:40.963786Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"e687630f","executionInfo":{"status":"ok","timestamp":1732210351437,"user_tz":-120,"elapsed":554,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"ee3243d3-2d77-48f1-f7a7-5db72010923a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0.])"]},"metadata":{},"execution_count":32}],"source":["torch.tensor([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]).type(torch.float32)"]},{"cell_type":"markdown","id":"d6251c90","metadata":{"id":"d6251c90"},"source":["Conversion back and forth between these various representations is useful. Define functions `str_to_onehot` and `onehot_to_str` that convert between the string and one-hot representations using a vocabulary array to define the  conversion.\n","\n","Ideally, in your implementation, you'll want to take advantage of the componentwise nature of many tensor operations discussed above."]},{"cell_type":"code","execution_count":33,"id":"4d02ba04","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.967216Z","iopub.status.busy":"2024-11-17T20:04:40.967008Z","iopub.status.idle":"2024-11-17T20:04:40.971325Z","shell.execute_reply":"2024-11-17T20:04:40.970690Z"},"id":"4d02ba04","executionInfo":{"status":"ok","timestamp":1732210351437,"user_tz":-120,"elapsed":39,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def str_to_onehot(wordtype, vocab):\n","    \"\"\"Returns the 1-hot representation of `wordtype` with vocabulary\n","    `vocab`.\n","    The returned value should be a `torch.tensor` with data type `float`.\n","    \"\"\"\n","    idx = vocab.index(wordtype)\n","    onehot = torch.zeros(len(vocab)).type(torch.float32)\n","    onehot[idx] = 1\n","    return onehot\n","\n","def onehot_to_str(onehot, vocab):\n","    \"\"\"Returns the string representation of `onehot`, a one-hot\n","    representation of a word type, with vocabulary `vocab`.\n","    \"\"\"\n","    return vocab[torch.argmax(onehot)]"]},{"cell_type":"markdown","id":"0cf47213","metadata":{"deletable":false,"editable":false,"id":"0cf47213"},"source":["Now use `str_to_onehot` to define the variable `anywhere_1hot` to be the 1-hot representation for a token of the type 'anywhere'.\n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot\n","-->"]},{"cell_type":"code","execution_count":34,"id":"a0d59d3a","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:40.974165Z","iopub.status.busy":"2024-11-17T20:04:40.973958Z","iopub.status.idle":"2024-11-17T20:04:40.977081Z","shell.execute_reply":"2024-11-17T20:04:40.976441Z"},"id":"a0d59d3a","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":39,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","anywhere_1hot = str_to_onehot(\"anywhere\", vocabulary)"]},{"cell_type":"code","execution_count":35,"id":"faaf31b1","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"faaf31b1","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":39,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"4e1ded03-6533-496a-88e6-f19e2dcdaffa"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":35}],"source":["grader.check(\"anywhere_1hot\")"]},{"cell_type":"markdown","id":"e7be1f12","metadata":{"deletable":false,"editable":false,"id":"e7be1f12"},"source":["You can verify that the conversion worked correctly by inverting it using `onehot_to_str`, which we've done in the following unit test.\n","<!--\n","BEGIN QUESTION\n","name: anywhere_1hot_reverse\n","-->"]},{"cell_type":"code","execution_count":36,"id":"8a1c6097","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"8a1c6097","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":38,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"e8022a88-54c5-44ea-ed01-19d0479afcc0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":36}],"source":["grader.check(\"anywhere_1hot_reverse\")"]},{"cell_type":"markdown","id":"826ea430","metadata":{"id":"826ea430"},"source":["# Representing texts\n","\n","## The set-of-words representation\n","\n","We can represent a whole *text* (a sequence of words) by manipulating the vector representations of the words within the text. For instance, we can take the componentwise maximum of the vectors. We refer to this as the _set-of-words_ representation.\n","\n","Here we've defined a function `set_of_words` that returns the set of words representation for a token sequence."]},{"cell_type":"code","execution_count":37,"id":"189f2a94","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.014039Z","iopub.status.busy":"2024-11-17T20:04:41.013800Z","iopub.status.idle":"2024-11-17T20:04:41.017559Z","shell.execute_reply":"2024-11-17T20:04:41.016911Z"},"id":"189f2a94","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":37,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["def set_of_words(tokens, vocabulary):\n","    \"\"\"Returns the set-of-words representation as a tensor of floats for the\n","    sequence of `tokens` using the `vocabulary` to specify the conversion.\n","    \"\"\"\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.amax(onehots, 0).type(torch.float32)"]},{"cell_type":"markdown","id":"fc50f829","metadata":{"id":"fc50f829"},"source":["The set-of-words representation for a text is a vector that has a `1` for each word type that occurs in the text. The vector represents the [characteristic function](https://en.wikipedia.org/wiki/Characteristic_function) for the subset of vocabulary words that appear in the text; hence the term 'set of words'."]},{"cell_type":"markdown","id":"24def7ba","metadata":{"deletable":false,"editable":false,"id":"24def7ba"},"source":["What is the set-of-words representation for the example text 'I would not, would not, here or there.'?\n","\n","Define the variable `example_sow` to be the set of words representation for the example text 'I would not, would not, here or there.' Use the `nltk_normpunc_tokenize` tokenizer.\n","\n","<!--\n","BEGIN QUESTION\n","name: example_sow\n","-->"]},{"cell_type":"code","execution_count":38,"id":"a2dc8074","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.020466Z","iopub.status.busy":"2024-11-17T20:04:41.020245Z","iopub.status.idle":"2024-11-17T20:04:41.024114Z","shell.execute_reply":"2024-11-17T20:04:41.023568Z"},"id":"a2dc8074","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":36,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","example_sow = set_of_words(nltk_normpunc_tokenize(\"I would not, would not, here or there.\"), vocabulary)"]},{"cell_type":"code","execution_count":39,"id":"5fecf372","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"5fecf372","executionInfo":{"status":"ok","timestamp":1732210351438,"user_tz":-120,"elapsed":36,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"469a5732-8c3d-4677-a3fa-f64a974ada96"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":39}],"source":["grader.check(\"example_sow\")"]},{"cell_type":"markdown","id":"5bf2ed59","metadata":{"id":"5bf2ed59"},"source":["## The bag of words representation\n","\n","If instead, we take the componentwise _sum_ of the vectors instead of the maximum, the text representation provides the _frequency_ of each word type in the text. We refer to this representation as the _[bag](https://en.wikipedia.org/wiki/Multiset) of words_ representation.\n","\n","Define a function `bag_of_words`, analogous to `set_of_words` above, that returns the bag-of-words representation for a token sequence."]},{"cell_type":"code","execution_count":40,"id":"810e3c4c","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.042326Z","iopub.status.busy":"2024-11-17T20:04:41.042116Z","iopub.status.idle":"2024-11-17T20:04:41.046012Z","shell.execute_reply":"2024-11-17T20:04:41.045121Z"},"id":"810e3c4c","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":36,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def bag_of_words(tokens, vocabulary):\n","    onehots = torch.stack([str_to_onehot(token, vocabulary) for token in tokens])\n","    return torch.sum(onehots, 0).type(torch.float32)"]},{"cell_type":"markdown","id":"d3fb6f86","metadata":{"deletable":false,"editable":false,"id":"d3fb6f86"},"source":["What is the bag of words representation for the example text 'I would not, would not, here or there.'?\n","<!--\n","BEGIN QUESTION\n","name: example_bow\n","-->"]},{"cell_type":"code","execution_count":41,"id":"e9ec58df","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.048753Z","iopub.status.busy":"2024-11-17T20:04:41.048540Z","iopub.status.idle":"2024-11-17T20:04:41.052635Z","shell.execute_reply":"2024-11-17T20:04:41.051929Z"},"id":"e9ec58df","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":35,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO - define the variable to be the bag of words representation for the example text\n","# 'I would not, would not, here or there.'\n","# Use the `nltk_normpunc_tokenize` tokenizer\n","example_bow = bag_of_words(nltk_normpunc_tokenize(\"I would not, would not, here or there.\"), vocabulary)"]},{"cell_type":"code","execution_count":42,"id":"5162a2ca","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"5162a2ca","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":35,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"a843c0b9-3a21-4f00-830b-07ee910e3990"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":42}],"source":["grader.check(\"example_bow\")"]},{"cell_type":"markdown","id":"38033da6","metadata":{"id":"38033da6"},"source":["# Document similarity metrics\n","\n","Consider the following text classification problem: Each sentence in _Green Eggs and Ham_ is spoken by one of two characters, Sam-I-am and Guy-am-I. We want to be able to classify new sentences as (most likely) being uttered by one of the two.\n","\n","A simple method for text classification is the _nearest neighbor_ method. We select the class for the new sentence that is the same as the class of the \"nearest\" (most similar) sentence for which we already know the class. (You'll experiment much more with this text classification method in the next lab.)\n","\n","To perform nearest neighbor classification, we need a method for measuring the (metaphorical) distance between two texts based on their representations. We'll explore a few methods here:\n","\n","* Hamming distance\n","* Jaccard distance\n","* Euclidean distance\n","* cosine distance\n","\n","You'll implement code for all of these distance metrics. Try to implement the functions using `torch` tensor functions only, without explicit iteration over the elements in the vector.\n","\n","We'll take a look at the distances among the following sentences:\n","\n","1. Would you like them here or there?\n","2. I would not like them here or there.\n","3. Do you like green eggs and ham?\n","4. I do not like them Sam-I-am.\n","\n","We'll start with the set of words representations of these sentences:"]},{"cell_type":"code","execution_count":43,"id":"07aa1fbc","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.073070Z","iopub.status.busy":"2024-11-17T20:04:41.072854Z","iopub.status.idle":"2024-11-17T20:04:41.078553Z","shell.execute_reply":"2024-11-17T20:04:41.077851Z"},"id":"07aa1fbc","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":34,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["examples = \"\"\"Would you like them here or there?\n","              I would not like them here or there.\n","              Do you like green eggs and ham?\n","              I do not like them Sam-I-am.\"\"\" \\\n","           .split(\"\\n\")\n","sows = [set_of_words(nltk_normpunc_tokenize(sentence), vocabulary)\n","            for sentence in examples]"]},{"cell_type":"code","execution_count":44,"id":"0f20d5ad","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.081234Z","iopub.status.busy":"2024-11-17T20:04:41.081017Z","iopub.status.idle":"2024-11-17T20:04:41.088289Z","shell.execute_reply":"2024-11-17T20:04:41.087549Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"0f20d5ad","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":33,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"228978dd-421e-49ca-a856-64a564f1d9cf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1.,\n","         1.]),\n"," tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n","         0.]),\n"," tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n","         1.]),\n"," tensor([0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0.,\n","         0.])]"]},"metadata":{},"execution_count":44}],"source":["sows"]},{"cell_type":"markdown","id":"e177a26b","metadata":{"deletable":false,"editable":false,"id":"e177a26b"},"source":["## Hamming distance\n","\n","The Hamming distance between two vectors is the number of positions at which they differ. Define a function `hamming_distance` that computes the Hamming distance between two vectors. The returned value should be a (tensorized) integer.\n","<!--\n","BEGIN QUESTION\n","name: hamming_distance\n","-->"]},{"cell_type":"code","execution_count":45,"id":"53f91367","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.091548Z","iopub.status.busy":"2024-11-17T20:04:41.090926Z","iopub.status.idle":"2024-11-17T20:04:41.095048Z","shell.execute_reply":"2024-11-17T20:04:41.094323Z"},"id":"53f91367","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":31,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def hamming_distance(v1, v2):\n","    return torch.sum(v1 != v2)\n","\n","# v1 = torch.tensor([1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","# v2 = torch.tensor([0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","# print(hamming_distance(v1, v2))"]},{"cell_type":"code","execution_count":46,"id":"7fcd8366","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"7fcd8366","executionInfo":{"status":"ok","timestamp":1732210351439,"user_tz":-120,"elapsed":31,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"9c2a1f2d-8db3-4d3e-e84a-1b45a528b2a2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":46}],"source":["grader.check(\"hamming_distance\")"]},{"cell_type":"markdown","id":"cc958ad6","metadata":{"id":"cc958ad6"},"source":["Now we can generate the Hamming distances among all of the sample sentences in a little table. Do the values make sense? What does the diagonal tell you?"]},{"cell_type":"code","execution_count":47,"id":"d78e6793","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.115290Z","iopub.status.busy":"2024-11-17T20:04:41.114780Z","iopub.status.idle":"2024-11-17T20:04:41.119990Z","shell.execute_reply":"2024-11-17T20:04:41.119436Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"d78e6793","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":31,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"f349e0b9-2f59-48e3-d5ce-4a2a17e7f265"},"outputs":[{"output_type":"stream","name":"stdout","text":["   0    5   10   11 \n","   5    0   15    6 \n","  10   15    0   11 \n","  11    6   11    0 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{hamming_distance(sows[i], sows[j]):4} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"0930ae3f","metadata":{"deletable":false,"editable":false,"id":"0930ae3f"},"source":["## Jaccard distance\n","\n","The Jaccard distance between two sets (and remember that these bit strings basically represent sets) is one minus the number of elements in their intersection divided by the number of elements in their union.\n","\n","$$ D_{jaccard}(v_1, v_2) = 1 - \\frac{| v_1 \\cap v_2 |}{| v_1 \\cup v_2 |} $$\n","\n","Define a function `jaccard_distance` to compute the Jaccard distance between two set-of-words representations. The returned value should be a (tensorized) float.\n","<!--\n","BEGIN QUESTION\n","name: jaccard_distance\n","-->"]},{"cell_type":"code","execution_count":48,"id":"972e98fd","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.123203Z","iopub.status.busy":"2024-11-17T20:04:41.122489Z","iopub.status.idle":"2024-11-17T20:04:41.126758Z","shell.execute_reply":"2024-11-17T20:04:41.126228Z"},"id":"972e98fd","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":29,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def jaccard_distance(v1, v2):\n","    v1_int = v1.type(torch.int)\n","    v2_int = v2.type(torch.int)\n","    intersection = torch.sum(v1_int&v2_int)\n","    union = torch.sum(v1_int|v2_int)\n","    return 1 - (intersection / union)"]},{"cell_type":"code","execution_count":49,"id":"94dd5cae","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"94dd5cae","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":29,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"7dacdcec-a08f-4f7b-adef-5adaf38681a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":49}],"source":["grader.check(\"jaccard_distance\")"]},{"cell_type":"markdown","id":"d784efc7","metadata":{"id":"d784efc7"},"source":["Again, here's a table of the Jaccard distances among the sample sentences."]},{"cell_type":"code","execution_count":50,"id":"24a0ab81","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.149287Z","iopub.status.busy":"2024-11-17T20:04:41.148645Z","iopub.status.idle":"2024-11-17T20:04:41.154028Z","shell.execute_reply":"2024-11-17T20:04:41.153435Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"24a0ab81","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":28,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"6ebced13-f2a7-4ebc-d69e-b272392d4264"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.455 0.769 0.846 \n","0.455 0.000 0.938 0.545 \n","0.769 0.938 0.000 0.846 \n","0.846 0.545 0.846 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{jaccard_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"2c4e2e62","metadata":{"deletable":false,"editable":false,"id":"2c4e2e62"},"source":["## Euclidean distance\n","\n","The Euclidean distance between two vectors is the norm of the vector between them, that is,\n","\n","$$ D_{euclidean}(\\mathbf{x}, \\mathbf{y}) = |\\mathbf{x} - \\mathbf{y}| $$\n","\n","where $|\\mathbf{z}|$, the norm of a vector $\\mathbf{z}$, is calculated as\n","\n","$$ |\\mathbf{z}| = \\sqrt{\\sum_{i=1}^N \\mathbf{z}_i^2} $$\n","\n","Fortunately, `torch` provides the function [`torch.linalg.norm`](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html#torch.linalg.norm) to compute the norm, and the vector between two vectors can be computed by componentwise subtraction.\n","\n","Define a function `euclidean_distance` to compute the Euclidean distance between two vectors. (For the time being, try to implement it directly without using `torch.linalg.norm`.)\n","<!--\n","BEGIN QUESTION\n","name: euclidean_distance\n","-->"]},{"cell_type":"code","execution_count":51,"id":"5f082b14","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.156935Z","iopub.status.busy":"2024-11-17T20:04:41.156484Z","iopub.status.idle":"2024-11-17T20:04:41.160941Z","shell.execute_reply":"2024-11-17T20:04:41.160235Z"},"id":"5f082b14","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":26,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def euclidean_distance(v1, v2):\n","    sub = v1 - v2\n","    return torch.sqrt(torch.sum(sub**2))"]},{"cell_type":"code","execution_count":52,"id":"172b904e","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"172b904e","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":26,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"6d8cff04-af5e-410a-c98e-e398b7053641"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":52}],"source":["grader.check(\"euclidean_distance\")"]},{"cell_type":"markdown","id":"a00dcae1","metadata":{"id":"a00dcae1"},"source":["Again, here's a table of the Euclidean distances among the sample sentences."]},{"cell_type":"code","execution_count":53,"id":"3249d767","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.180535Z","iopub.status.busy":"2024-11-17T20:04:41.180100Z","iopub.status.idle":"2024-11-17T20:04:41.185070Z","shell.execute_reply":"2024-11-17T20:04:41.184516Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"3249d767","executionInfo":{"status":"ok","timestamp":1732210351440,"user_tz":-120,"elapsed":25,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"d8b0b0d7-f1bb-4a55-83ad-c514d55e4453"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 2.236 3.162 3.317 \n","2.236 0.000 3.873 2.449 \n","3.162 3.873 0.000 3.317 \n","3.317 2.449 3.317 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{euclidean_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"c26abc76","metadata":{"id":"c26abc76"},"source":["## Cosine distance\n","\n","The _cosine similarity_ of two vectors of length $N$ is the cosine of the angle that they form, which is computed as the dot product of the two vectors divided by their norms.\n","\n","$$ cos(\\mathbf{x}, \\mathbf{y}) =\n","      \\frac{\\sum_{i=1}^N \\mathbf{x}_i \\cdot \\mathbf{y}_i}{|\\mathbf{x}| \\cdot |\\mathbf{y}|} $$\n","\n","This isn't a distance metric, but a similarity metric. For vectors of non-negative numbers, it ranges from 0 to 1, where 0 is maximally different and 1 is maximally similar. To turn it into a distance metric, then, we take the inverse cosine (to convert the cosine to an angle between $\\pi$ and 0) and divide by $\\pi$.\n","\n","$$ D_{cosine}(\\mathbf{x}, \\mathbf{y}) = \\frac{cos^{-1}(cos(\\mathbf{x}, \\mathbf{y}))}{\\pi} $$\n","\n","Since we're using `torch`, some of these functions are already provided. See hints [here](https://pytorch.org/docs/stable/generated/torch.dot.html), [here](https://pytorch.org/docs/stable/generated/torch.linalg.norm.html), and [here](https://docs.python.org/3/library/math.html#math.acos).\n","\n","(To avoid some math domain errors, we recommend that you use the function `safe_acos` that we've provided to compute the inverse cosine function instead of using `math.acos` directly.)"]},{"cell_type":"code","execution_count":54,"id":"bed2a5ee","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.188129Z","iopub.status.busy":"2024-11-17T20:04:41.187693Z","iopub.status.idle":"2024-11-17T20:04:41.191605Z","shell.execute_reply":"2024-11-17T20:04:41.191078Z"},"id":"bed2a5ee","executionInfo":{"status":"ok","timestamp":1732210351441,"user_tz":-120,"elapsed":25,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["def safe_acos(x):\n","    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it\n","       does not raise an exception for values of `x` out of range,\n","       but rather clips `x` at -1..1, thereby avoiding math domain\n","       errors in the case of numerical errors.\"\"\"\n","    return math.acos(math.copysign(min(1.0, abs(x)), x))"]},{"cell_type":"code","execution_count":55,"id":"4ac812dc","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.194465Z","iopub.status.busy":"2024-11-17T20:04:41.194255Z","iopub.status.idle":"2024-11-17T20:04:41.198279Z","shell.execute_reply":"2024-11-17T20:04:41.197384Z"},"id":"4ac812dc","executionInfo":{"status":"ok","timestamp":1732210351441,"user_tz":-120,"elapsed":24,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}}},"outputs":[],"source":["#TODO\n","def cosine_distance(v1, v2):\n","    \"\"\"Returns the cosine distance between two vectors\"\"\"\n","    dot_product = torch.dot(v1, v2)\n","    norm_v1 = torch.linalg.norm(v1)\n","    norm_v2 = torch.linalg.norm(v2)\n","    return safe_acos(dot_product / (norm_v1 * norm_v2)) / math.pi"]},{"cell_type":"code","execution_count":56,"id":"af1cd303","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":46},"id":"af1cd303","executionInfo":{"status":"ok","timestamp":1732210351706,"user_tz":-120,"elapsed":289,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"6f662588-beb5-4142-fd4d-55942b470492"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\n","    All tests passed!\n","    "],"text/html":["\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    "]},"metadata":{},"execution_count":56}],"source":["grader.check(\"cosine_distance\")"]},{"cell_type":"code","execution_count":57,"id":"1a3574a0","metadata":{"execution":{"iopub.execute_input":"2024-11-17T20:04:41.220009Z","iopub.status.busy":"2024-11-17T20:04:41.219805Z","iopub.status.idle":"2024-11-17T20:04:41.225539Z","shell.execute_reply":"2024-11-17T20:04:41.224989Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"1a3574a0","executionInfo":{"status":"ok","timestamp":1732210351707,"user_tz":-120,"elapsed":9,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"3d7d7241-350b-4317-ac82-2b08c43c05b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.000 0.250 0.378 0.414 \n","0.250 0.000 0.462 0.283 \n","0.378 0.462 0.000 0.414 \n","0.414 0.283 0.414 0.000 \n"]}],"source":["for i in range(4):\n","    for j in range(4):\n","        print(f\"{cosine_distance(sows[i], sows[j]):5.3f} \", end='')\n","    print()"]},{"cell_type":"markdown","id":"94184322","metadata":{"id":"94184322"},"source":["In the next lab, you'll use some of these distance metrics to automatically classify text using nearest neighbor classification."]},{"cell_type":"markdown","id":"ea17c4f2","metadata":{"deletable":false,"editable":false,"id":"ea17c4f2"},"source":["<!-- BEGIN QUESTION -->\n","\n","# Lab debrief\n","\n","**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following, but you're not restricted to these:\n","\n","* Was the lab too long or too short?\n","* Were the readings appropriate for the lab?\n","* Was it clear (at least after you completed the lab) what the points of the exercises were?\n","* Are there additions or changes you think would make the lab better?\n","\n","<!--\n","BEGIN QUESTION\n","name: open_response_debrief\n","manual: true\n","-->"]},{"cell_type":"markdown","id":"a6dcd047","metadata":{"id":"a6dcd047"},"source":["_Type your answer here, replacing this text._"]},{"cell_type":"markdown","id":"0a1fcbef","metadata":{"id":"0a1fcbef"},"source":["<!-- END QUESTION -->\n","\n","\n","\n","# Submission Instructions\n","\n","This lab should be submitted to Gradescope at <https://www.gradescope.com/courses/903849>, under \"lab 1-1 - Code\", which will be made available some time before the due date.\n","\n","Make sure that you have passed all public tests by running `grader.check_all()` below before submitting. Note that there are hidden tests on Gradescope, the results of which will be revealed after the submission deadline.\n","\n","We also request that you **submit a PDF of the freshly run notebook**. For that, the best method is to press \"restart kernel and run all cells\", allowing time for all cells to be run to completion. Then, export the notebook as a PDF, and submit it to Gradescope, under the assignment \"lab1-1 - PDF\"."]},{"cell_type":"markdown","id":"1b1347fc","metadata":{"id":"1b1347fc"},"source":["# End of lab 1-1"]},{"cell_type":"markdown","id":"847f7033","metadata":{"deletable":false,"editable":false,"id":"847f7033"},"source":["---\n","\n","To double-check your work, the cell below will rerun all of the autograder tests."]},{"cell_type":"code","execution_count":58,"id":"bb53a5cb","metadata":{"deletable":false,"editable":false,"colab":{"base_uri":"https://localhost:8080/","height":864},"id":"bb53a5cb","executionInfo":{"status":"ok","timestamp":1732210351707,"user_tz":-120,"elapsed":7,"user":{"displayName":"Murad Ektilat","userId":"16186005461946402621"}},"outputId":"4463c3fe-cbff-4452-8c69-e418d170b273"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["anywhere_1hot:\n","\n","    All tests passed!\n","    \n","\n","anywhere_1hot_reverse:\n","\n","    All tests passed!\n","    \n","\n","cosine_distance:\n","\n","    All tests passed!\n","    \n","\n","euclidean_distance:\n","\n","    All tests passed!\n","    \n","\n","example_bow:\n","\n","    All tests passed!\n","    \n","\n","example_sow:\n","\n","    All tests passed!\n","    \n","\n","hamming_distance:\n","\n","    All tests passed!\n","    \n","\n","jaccard_distance:\n","\n","    All tests passed!\n","    \n","\n","nltk_text_tokenized:\n","\n","    All tests passed!\n","    \n","\n","nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:\n","\n","    All tests passed!\n","    \n","\n","normalize_token:\n","\n","    All tests passed!\n","    \n","\n","text_tokenized:\n","\n","    All tests passed!\n","    \n","\n","token_count:\n","\n","    All tests passed!\n","    \n","\n","token_count_punc:\n","\n","    All tests passed!\n","    \n","\n","token_count_whitespace:\n","\n","    All tests passed!\n","    \n","\n","tokens_whitespace:\n","\n","    All tests passed!\n","    \n","\n","type_count:\n","\n","    All tests passed!\n","    \n","\n","type_count_norm_punc:\n","\n","    All tests passed!\n","    \n"],"text/html":["<p><strong>anywhere_1hot:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>anywhere_1hot_reverse:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>cosine_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>euclidean_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_bow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>example_sow:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>hamming_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>jaccard_distance:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_text_tokenized:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>nltk_whitespace_tokenize_and_nltk_normpunc_tokenize:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>normalize_token:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>text_tokenized:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>token_count_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>tokens_whitespace:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n","<p><strong>type_count_norm_punc:</strong></p>\n","\n","    \n","    \n","        <p>All tests passed!</p>\n","    \n","    \n","\n"]},"metadata":{},"execution_count":58}],"source":["grader.check_all()"]}],"metadata":{"celltoolbar":"Tags","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"title":"Course 236299 Lab 1-1: Types, tokens, and representing text","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}